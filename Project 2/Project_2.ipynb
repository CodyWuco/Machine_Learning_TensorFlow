{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Project 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8de79b"
      },
      "source": [
        "CSC 180 Intelligent Systems (Fall 2021)"
      ],
      "id": "5e8de79b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hxpMmSHSFIQ"
      },
      "source": [
        "## Mount Google Drive\n",
        "Use these functions to mount the drive hosting the data you wish to use."
      ],
      "id": "1hxpMmSHSFIQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUk4uDAs7WWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d630c5-8120-4a6b-eda2-136f94b7f278"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CUk4uDAs7WWI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXxYlP2M_0eK"
      },
      "source": [
        "## Defined functions"
      ],
      "id": "YXxYlP2M_0eK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3872c70a"
      },
      "source": [
        "from collections.abc import Sequence\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "\n",
        "# Encode a numeric column as zscores\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the median\n",
        "def missing_median(df, name):\n",
        "    med = df[name].median()\n",
        "    df[name] = df[name].fillna(med)\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the default\n",
        "def missing_default(df, name, default_value):\n",
        "    df[name] = df[name].fillna(default_value)\n",
        "\n",
        "\n",
        "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # find out the type of the target column. \n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
        "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
        "    if target_type in (np.int64, np.int32):\n",
        "        # Classification\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
        "    else:\n",
        "        # Regression\n",
        "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
        "\n",
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "\n",
        "# Regression chart.\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    b = plt.plot(t['y'].tolist(),label='expected')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Remove all rows where the specified column is +/- sd standard deviations\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)\n",
        "\n",
        "\n",
        "# Encode a column to a range between normalized_low and normalized_high.\n",
        "def encode_numeric_range(df, name, normalized_low=0, normalized_high=1,\n",
        "                         data_low=None, data_high=None):\n",
        "    if data_low is None:\n",
        "        data_low = min(df[name])\n",
        "        data_high = max(df[name])\n",
        "\n",
        "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
        "               * (normalized_high - normalized_low) + normalized_low\n",
        "        \n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(names))\n",
        "    plt.xticks(tick_marks, names, rotation=45)\n",
        "    plt.yticks(tick_marks, names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "id": "3872c70a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czkrxNB5UrsL"
      },
      "source": [
        "## Import data from Drive\n",
        "Importing th network intrutions data set and labeling it."
      ],
      "id": "czkrxNB5UrsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6f4aff6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "171009f6-1ec8-4730-b575-7e16d8218ff4"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the data\n",
        "df_intrution = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/project 2/network_intrusion_data.csv')\n",
        "\n",
        "# Labeling each column\n",
        "df_intrution.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "                        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "                        'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
        "                        'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', \n",
        "                        'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'outcome' ] \n",
        "df_intrution"
      ],
      "id": "6f4aff6d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>239</td>\n",
              "      <td>486</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>235</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>219</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>217</td>\n",
              "      <td>2032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>217</td>\n",
              "      <td>2032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494015</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>310</td>\n",
              "      <td>1881</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>86</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494016</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>282</td>\n",
              "      <td>2286</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494017</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>203</td>\n",
              "      <td>1200</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>16</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494018</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>291</td>\n",
              "      <td>1200</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>26</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494019</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>219</td>\n",
              "      <td>1234</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>6</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>494020 rows Ã— 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
              "0              0           tcp  ...                      0.0  normal.\n",
              "1              0           tcp  ...                      0.0  normal.\n",
              "2              0           tcp  ...                      0.0  normal.\n",
              "3              0           tcp  ...                      0.0  normal.\n",
              "4              0           tcp  ...                      0.0  normal.\n",
              "...          ...           ...  ...                      ...      ...\n",
              "494015         0           tcp  ...                      0.0  normal.\n",
              "494016         0           tcp  ...                      0.0  normal.\n",
              "494017         0           tcp  ...                      0.0  normal.\n",
              "494018         0           tcp  ...                      0.0  normal.\n",
              "494019         0           tcp  ...                      0.0  normal.\n",
              "\n",
              "[494020 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8VmDR_HUle7"
      },
      "source": [
        "## Remove Duplicate lines\n",
        "Removing duplicate lines to avoid overfitting."
      ],
      "id": "v8VmDR_HUle7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cQez0M5D345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e98fd1f5-67b9-4a52-8910-5ee7d84a36fe"
      },
      "source": [
        "df_intrution = df_intrution.drop_duplicates()\n",
        "df_intrution = df_intrution.reset_index(drop=True) # Resets index to avoid missing numbers\n",
        "df_intrution"
      ],
      "id": "1cQez0M5D345",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>239</td>\n",
              "      <td>486</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>235</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>219</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>217</td>\n",
              "      <td>2032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>217</td>\n",
              "      <td>2032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145580</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>310</td>\n",
              "      <td>1881</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>86</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145581</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>282</td>\n",
              "      <td>2286</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145582</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>203</td>\n",
              "      <td>1200</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>16</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145583</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>291</td>\n",
              "      <td>1200</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>26</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145584</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>219</td>\n",
              "      <td>1234</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>6</td>\n",
              "      <td>255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145585 rows Ã— 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
              "0              0           tcp  ...                      0.0  normal.\n",
              "1              0           tcp  ...                      0.0  normal.\n",
              "2              0           tcp  ...                      0.0  normal.\n",
              "3              0           tcp  ...                      0.0  normal.\n",
              "4              0           tcp  ...                      0.0  normal.\n",
              "...          ...           ...  ...                      ...      ...\n",
              "145580         0           tcp  ...                      0.0  normal.\n",
              "145581         0           tcp  ...                      0.0  normal.\n",
              "145582         0           tcp  ...                      0.0  normal.\n",
              "145583         0           tcp  ...                      0.0  normal.\n",
              "145584         0           tcp  ...                      0.0  normal.\n",
              "\n",
              "[145585 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVIy019VFRPy"
      },
      "source": [
        "## Normalize numeric values\n",
        "This function needs to be corrected. it should be normalizing, but something seems to be going wrong.\n",
        "\n",
        "Descrete values are either 1 or 0, so normalizing won't change them."
      ],
      "id": "tVIy019VFRPy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "SxFAidc7BgWv",
        "outputId": "e82e5c98-1227-4f50-b719-dfc024719e61"
      },
      "source": [
        "encode_numeric_range(df_intrution, 'duration')\n",
        "encode_numeric_range(df_intrution, 'src_bytes')\n",
        "encode_numeric_range(df_intrution, 'dst_bytes')\n",
        "encode_numeric_range(df_intrution, 'land')\n",
        "encode_numeric_range(df_intrution, 'wrong_fragment')\n",
        "encode_numeric_range(df_intrution, 'urgent')\n",
        "encode_numeric_range(df_intrution, 'hot')\n",
        "encode_numeric_range(df_intrution, 'num_failed_logins')\n",
        "encode_numeric_range(df_intrution, 'logged_in')\n",
        "encode_numeric_range(df_intrution, 'num_compromised')\n",
        "encode_numeric_range(df_intrution, 'root_shell')\n",
        "encode_numeric_range(df_intrution, 'su_attempted')\n",
        "encode_numeric_range(df_intrution, 'num_root')\n",
        "encode_numeric_range(df_intrution, 'num_file_creations')\n",
        "encode_numeric_range(df_intrution, 'num_shells')\n",
        "encode_numeric_range(df_intrution, 'num_access_files')\n",
        "encode_numeric_range(df_intrution, 'num_outbound_cmds')\n",
        "encode_numeric_range(df_intrution, 'is_host_login')\n",
        "encode_numeric_range(df_intrution, 'is_guest_login')\n",
        "encode_numeric_range(df_intrution, 'count')\n",
        "encode_numeric_range(df_intrution, 'srv_count')\n",
        "encode_numeric_range(df_intrution, 'serror_rate')\n",
        "encode_numeric_range(df_intrution, 'srv_serror_rate')\n",
        "encode_numeric_range(df_intrution, 'rerror_rate')\n",
        "encode_numeric_range(df_intrution, 'srv_rerror_rate')\n",
        "encode_numeric_range(df_intrution, 'same_srv_rate')\n",
        "encode_numeric_range(df_intrution, 'diff_srv_rate')\n",
        "encode_numeric_range(df_intrution, 'srv_diff_host_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_count')\n",
        "encode_numeric_range(df_intrution, 'dst_host_srv_count')\n",
        "encode_numeric_range(df_intrution, 'dst_host_same_srv_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_diff_srv_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_same_src_port_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_srv_diff_host_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_serror_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_srv_serror_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_rerror_rate')\n",
        "encode_numeric_range(df_intrution, 'dst_host_srv_rerror_rate')\n",
        "df_intrution"
      ],
      "id": "SxFAidc7BgWv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.446905e-07</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.389216e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145580</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>4.470881e-07</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007828</td>\n",
              "      <td>0.009785</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.337255</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145581</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>4.067060e-07</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145582</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>2.927706e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.035225</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145583</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>4.196859e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.023483</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.101961</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145584</th>\n",
              "      <td>0.0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145585 rows Ã— 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
              "0            0.0           tcp  ...                      0.0  normal.\n",
              "1            0.0           tcp  ...                      0.0  normal.\n",
              "2            0.0           tcp  ...                      0.0  normal.\n",
              "3            0.0           tcp  ...                      0.0  normal.\n",
              "4            0.0           tcp  ...                      0.0  normal.\n",
              "...          ...           ...  ...                      ...      ...\n",
              "145580       0.0           tcp  ...                      0.0  normal.\n",
              "145581       0.0           tcp  ...                      0.0  normal.\n",
              "145582       0.0           tcp  ...                      0.0  normal.\n",
              "145583       0.0           tcp  ...                      0.0  normal.\n",
              "145584       0.0           tcp  ...                      0.0  normal.\n",
              "\n",
              "[145585 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "ciard6MrNJtS",
        "outputId": "66acfd73-2955-41d8-f76a-e5edcb53e7f7"
      },
      "source": [
        "encode_text_dummy(df_intrution, \"protocol_type\")\n",
        "encode_text_dummy(df_intrution, \"service\")\n",
        "encode_text_dummy(df_intrution, \"flag\")\n",
        "df_intrution"
      ],
      "id": "ciard6MrNJtS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "      <th>protocol_type-icmp</th>\n",
              "      <th>...</th>\n",
              "      <th>service-nnsp</th>\n",
              "      <th>service-nntp</th>\n",
              "      <th>service-ntp_u</th>\n",
              "      <th>service-other</th>\n",
              "      <th>service-pm_dump</th>\n",
              "      <th>service-pop_2</th>\n",
              "      <th>service-pop_3</th>\n",
              "      <th>service-printer</th>\n",
              "      <th>service-private</th>\n",
              "      <th>service-red_i</th>\n",
              "      <th>service-remote_job</th>\n",
              "      <th>service-rje</th>\n",
              "      <th>service-shell</th>\n",
              "      <th>service-smtp</th>\n",
              "      <th>service-sql_net</th>\n",
              "      <th>service-ssh</th>\n",
              "      <th>service-sunrpc</th>\n",
              "      <th>service-supdup</th>\n",
              "      <th>service-systat</th>\n",
              "      <th>service-telnet</th>\n",
              "      <th>service-tftp_u</th>\n",
              "      <th>service-tim_i</th>\n",
              "      <th>service-time</th>\n",
              "      <th>service-urh_i</th>\n",
              "      <th>service-urp_i</th>\n",
              "      <th>service-uucp</th>\n",
              "      <th>service-uucp_path</th>\n",
              "      <th>service-vmnet</th>\n",
              "      <th>service-whois</th>\n",
              "      <th>flag-OTH</th>\n",
              "      <th>flag-REJ</th>\n",
              "      <th>flag-RSTO</th>\n",
              "      <th>flag-RSTOS0</th>\n",
              "      <th>flag-RSTR</th>\n",
              "      <th>flag-S0</th>\n",
              "      <th>flag-S1</th>\n",
              "      <th>flag-S2</th>\n",
              "      <th>flag-S3</th>\n",
              "      <th>flag-SF</th>\n",
              "      <th>flag-SH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.446905e-07</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.389216e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145580</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.470881e-07</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007828</td>\n",
              "      <td>0.009785</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.337255</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145581</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.067060e-07</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145582</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.927706e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.035225</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145583</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.196859e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.023483</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.101961</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145584</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145585 rows Ã— 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        duration     src_bytes  dst_bytes  ...  flag-S3  flag-SF  flag-SH\n",
              "0            0.0  3.446905e-07   0.000094  ...        0        1        0\n",
              "1            0.0  3.389216e-07   0.000259  ...        0        1        0\n",
              "2            0.0  3.158461e-07   0.000259  ...        0        1        0\n",
              "3            0.0  3.129617e-07   0.000394  ...        0        1        0\n",
              "4            0.0  3.129617e-07   0.000394  ...        0        1        0\n",
              "...          ...           ...        ...  ...      ...      ...      ...\n",
              "145580       0.0  4.470881e-07   0.000365  ...        0        1        0\n",
              "145581       0.0  4.067060e-07   0.000443  ...        0        1        0\n",
              "145582       0.0  2.927706e-07   0.000233  ...        0        1        0\n",
              "145583       0.0  4.196859e-07   0.000233  ...        0        1        0\n",
              "145584       0.0  3.158461e-07   0.000239  ...        0        1        0\n",
              "\n",
              "[145585 rows x 119 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp1nZaXEc0wz"
      },
      "source": [
        "## Remove columns with missing Values"
      ],
      "id": "vp1nZaXEc0wz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "gKsIGI38cyth",
        "outputId": "fbef5392-6be0-422e-9044-98e8040717c2"
      },
      "source": [
        "df_intrution = df_intrution.dropna(axis='columns')\n",
        "df_intrution"
      ],
      "id": "gKsIGI38cyth",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "      <th>protocol_type-icmp</th>\n",
              "      <th>protocol_type-tcp</th>\n",
              "      <th>protocol_type-udp</th>\n",
              "      <th>...</th>\n",
              "      <th>service-nnsp</th>\n",
              "      <th>service-nntp</th>\n",
              "      <th>service-ntp_u</th>\n",
              "      <th>service-other</th>\n",
              "      <th>service-pm_dump</th>\n",
              "      <th>service-pop_2</th>\n",
              "      <th>service-pop_3</th>\n",
              "      <th>service-printer</th>\n",
              "      <th>service-private</th>\n",
              "      <th>service-red_i</th>\n",
              "      <th>service-remote_job</th>\n",
              "      <th>service-rje</th>\n",
              "      <th>service-shell</th>\n",
              "      <th>service-smtp</th>\n",
              "      <th>service-sql_net</th>\n",
              "      <th>service-ssh</th>\n",
              "      <th>service-sunrpc</th>\n",
              "      <th>service-supdup</th>\n",
              "      <th>service-systat</th>\n",
              "      <th>service-telnet</th>\n",
              "      <th>service-tftp_u</th>\n",
              "      <th>service-tim_i</th>\n",
              "      <th>service-time</th>\n",
              "      <th>service-urh_i</th>\n",
              "      <th>service-urp_i</th>\n",
              "      <th>service-uucp</th>\n",
              "      <th>service-uucp_path</th>\n",
              "      <th>service-vmnet</th>\n",
              "      <th>service-whois</th>\n",
              "      <th>flag-OTH</th>\n",
              "      <th>flag-REJ</th>\n",
              "      <th>flag-RSTO</th>\n",
              "      <th>flag-RSTOS0</th>\n",
              "      <th>flag-RSTR</th>\n",
              "      <th>flag-S0</th>\n",
              "      <th>flag-S1</th>\n",
              "      <th>flag-S2</th>\n",
              "      <th>flag-S3</th>\n",
              "      <th>flag-SF</th>\n",
              "      <th>flag-SH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.446905e-07</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.389216e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>0.113725</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.129617e-07</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>0.231373</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145580</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.470881e-07</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007828</td>\n",
              "      <td>0.009785</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.337255</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145581</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.067060e-07</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145582</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.927706e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.035225</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145583</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.196859e-07</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.023483</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.101961</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145584</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.158461e-07</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145585 rows Ã— 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        duration     src_bytes  dst_bytes  ...  flag-S3  flag-SF  flag-SH\n",
              "0            0.0  3.446905e-07   0.000094  ...        0        1        0\n",
              "1            0.0  3.389216e-07   0.000259  ...        0        1        0\n",
              "2            0.0  3.158461e-07   0.000259  ...        0        1        0\n",
              "3            0.0  3.129617e-07   0.000394  ...        0        1        0\n",
              "4            0.0  3.129617e-07   0.000394  ...        0        1        0\n",
              "...          ...           ...        ...  ...      ...      ...      ...\n",
              "145580       0.0  4.470881e-07   0.000365  ...        0        1        0\n",
              "145581       0.0  4.067060e-07   0.000443  ...        0        1        0\n",
              "145582       0.0  2.927706e-07   0.000233  ...        0        1        0\n",
              "145583       0.0  4.196859e-07   0.000233  ...        0        1        0\n",
              "145584       0.0  3.158461e-07   0.000239  ...        0        1        0\n",
              "\n",
              "[145585 rows x 117 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd3TVQ1ahAJ3"
      },
      "source": [
        "## Checking outcome types"
      ],
      "id": "Vd3TVQ1ahAJ3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsG5khNcfUlg",
        "outputId": "63283ad6-c24c-4430-c1c6-e533329193a1"
      },
      "source": [
        "df_outcome = df_intrution.drop_duplicates(subset='outcome')\n",
        "df_outcome = df_outcome.reset_index(drop=True) # Resets index to avoid missing numbers\n",
        "df_outcome['outcome']"
      ],
      "id": "rsG5khNcfUlg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0              normal.\n",
              "1     buffer_overflow.\n",
              "2          loadmodule.\n",
              "3                perl.\n",
              "4             neptune.\n",
              "5               smurf.\n",
              "6        guess_passwd.\n",
              "7                 pod.\n",
              "8            teardrop.\n",
              "9           portsweep.\n",
              "10            ipsweep.\n",
              "11               land.\n",
              "12          ftp_write.\n",
              "13               back.\n",
              "14               imap.\n",
              "15              satan.\n",
              "16                phf.\n",
              "17               nmap.\n",
              "18           multihop.\n",
              "19        warezmaster.\n",
              "20        warezclient.\n",
              "21                spy.\n",
              "22            rootkit.\n",
              "Name: outcome, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QqiuHqk8jmt"
      },
      "source": [
        "## Defining Encode funtion for outcome column\n",
        "Turning all normal values to 0 and every other value to 1"
      ],
      "id": "3QqiuHqk8jmt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2uTWfwoqvc"
      },
      "source": [
        "# create a function that encodes outcome column of dataset\n",
        "def network_outcome_encoder(x):\n",
        "    if x == \"normal.\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "id": "iA2uTWfwoqvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke42NB2V84wu"
      },
      "source": [
        "## Creating encoded outcome array."
      ],
      "id": "ke42NB2V84wu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4Kz9RsX05x3",
        "outputId": "de16860c-7f02-419a-c254-6b304388fae5"
      },
      "source": [
        "df_outcome_encoded = df_intrution['outcome'].apply(network_outcome_encoder)\n",
        "df_outcome_encoded\n",
        "#df_intrution['outcome'] = df_intrution['outcome'].map(network_outcome_encoder)"
      ],
      "id": "S4Kz9RsX05x3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "145580    0\n",
              "145581    0\n",
              "145582    0\n",
              "145583    0\n",
              "145584    0\n",
              "Name: outcome, Length: 145585, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUMNXwa39A0j"
      },
      "source": [
        "## Swapping outcome column for encoded outcome array."
      ],
      "id": "RUMNXwa39A0j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_VvJeHk0Z_9",
        "outputId": "c791ecae-1969-4bce-f173-ca69e02cacd8"
      },
      "source": [
        "df_intrution['outcome'] = df_outcome_encoded\n",
        "df_intrution['outcome']"
      ],
      "id": "C_VvJeHk0Z_9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "145580    0\n",
              "145581    0\n",
              "145582    0\n",
              "145583    0\n",
              "145584    0\n",
              "Name: outcome, Length: 145585, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSPVnUDJagxe",
        "outputId": "0e0e59cd-6c27-4775-e7e9-4eef48ae5286"
      },
      "source": [
        "df_intrution.shape"
      ],
      "id": "hSPVnUDJagxe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145585, 117)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFwUeF2-9Tfs"
      },
      "source": [
        "## Checking dataset outcome column\n",
        "Checking to see if swap was a success."
      ],
      "id": "HFwUeF2-9Tfs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Lz58m2kr1r",
        "outputId": "b61bb3a4-2a11-469e-e49e-868b5f482756"
      },
      "source": [
        "df_outcome = df_intrution.drop_duplicates(subset='outcome')\n",
        "df_outcome = df_outcome.reset_index(drop=True) # Resets index to avoid missing numbers\n",
        "df_outcome['outcome']"
      ],
      "id": "e2Lz58m2kr1r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    1\n",
              "Name: outcome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exi-8i7E-tkX"
      },
      "source": [
        "# **stop here for now**"
      ],
      "id": "exi-8i7E-tkX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4CWGtb3YiIK"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure, show\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "id": "k4CWGtb3YiIK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpm5zmdNbcNi",
        "outputId": "8b8fc8ce-c364-40b7-e8b5-552f9273558e"
      },
      "source": [
        "# Encode to a 2D matrix for training\n",
        "x = df_intrution.drop('outcome', axis='columns').to_numpy()\n",
        "y = df_intrution['outcome'].to_numpy()\n",
        "\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print()\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "id": "Jpm5zmdNbcNi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train: (116468, 116)\n",
            "Shape of y_train: (116468,)\n",
            "\n",
            "Shape of x_test: (29117, 116)\n",
            "Shape of y_test: (29117,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDdxegqle4yW",
        "outputId": "4c855c41-eb6e-49ad-d1f0-67822d135cb4"
      },
      "source": [
        "# define input image dimensions\n",
        "img_rows, img_cols = 1, 116\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "id": "eDdxegqle4yW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116468, 1, 116, 1)\n",
            "(29117, 1, 116, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFKMvrYAgQGo"
      },
      "source": [
        "num_classes = 2\n",
        "\n",
        "# Converts a class vector (integers) to binary class matrix.   One-hot encoding!  Use with categorical_crossentropy.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "id": "yFKMvrYAgQGo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "PoYRHxuwilPg",
        "outputId": "6dca19d0-78e6-4b4f-d9c0-17effed0faf1"
      },
      "source": [
        "pd.DataFrame(y_train)"
      ],
      "id": "PoYRHxuwilPg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116463</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116464</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116465</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116466</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116467</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>116468 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0    1\n",
              "0       1.0  0.0\n",
              "1       0.0  1.0\n",
              "2       1.0  0.0\n",
              "3       1.0  0.0\n",
              "4       1.0  0.0\n",
              "...     ...  ...\n",
              "116463  0.0  1.0\n",
              "116464  0.0  1.0\n",
              "116465  0.0  1.0\n",
              "116466  0.0  1.0\n",
              "116467  1.0  0.0\n",
              "\n",
              "[116468 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "X7iKTk5Fih7b",
        "outputId": "6f3b0672-bbc0-4e07-d2a6-83f41ed468b7"
      },
      "source": [
        "pd.DataFrame(y_test)"
      ],
      "id": "X7iKTk5Fih7b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29112</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29113</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29114</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29115</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29116</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29117 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1\n",
              "0      1.0  0.0\n",
              "1      0.0  1.0\n",
              "2      1.0  0.0\n",
              "3      0.0  1.0\n",
              "4      1.0  0.0\n",
              "...    ...  ...\n",
              "29112  0.0  1.0\n",
              "29113  0.0  1.0\n",
              "29114  0.0  1.0\n",
              "29115  0.0  1.0\n",
              "29116  0.0  1.0\n",
              "\n",
              "[29117 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyd2Ah1DjAZO",
        "outputId": "458bbb81-021b-4936-c5ec-e11739fa3c57"
      },
      "source": [
        "y_train.shape, y_test.shape"
      ],
      "id": "Nyd2Ah1DjAZO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((116468, 2), (29117, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuCzz0mSmjG2"
      },
      "source": [
        "## Convelusional Neural Network"
      ],
      "id": "EuCzz0mSmjG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN5-ttGJ9Tei"
      },
      "source": [
        "## Activation = Relu, Optimizer = adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "Accuracy: 0.9985575437029914\n",
        "\n",
        "Averaged F1: 0.9985574573441822\n",
        "\n",
        "Time: 1m 38s\n"
      ],
      "id": "kN5-ttGJ9Tei"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1301977d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d972b38-6c67-4e30-f938-4bee473ed783"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "1301977d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.0271 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02713, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.0097 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02713 to 0.00969, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.0081 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00969 to 0.00805, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.0079 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00805 to 0.00786, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0068 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00786 to 0.00675, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00675 to 0.00611, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0067 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00611\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0055 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00611 to 0.00549, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0061 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00549\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0054 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00549 to 0.00541, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00541\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00541\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00541 to 0.00472, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00013: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0328 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00472\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0185 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00472\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.0093 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00472\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.0071 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00472\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00472\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.0071 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00472\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00472\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0057 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00472\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00472\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00472\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00472\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0050 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00472\n",
            "Epoch 00012: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0544 - accuracy: 0.9830 - val_loss: 0.0282 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00472\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.0118 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00472\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0073 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00472\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0085 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00472\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0061 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00472\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00472\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00472\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00472\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00472\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00472\n",
            "Epoch 00010: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0543 - accuracy: 0.9835 - val_loss: 0.0291 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00472\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.0137 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00472\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0092 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00472\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00472\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00472\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0073 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00472\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.0065 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00472\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00472\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00472\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00472\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0048 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00472\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0058 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00472\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00472\n",
            "Epoch 00013: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0537 - accuracy: 0.9837 - val_loss: 0.0268 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00472\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.0115 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00472\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.0077 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00472\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0089 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00472\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00472\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0065 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00472\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.0063 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00472\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0065 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00472\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00472\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00472\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00472\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0054 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00472\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00472\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00472 to 0.00469, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00014: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17572    17]\n",
            " [   25 11503]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985575437029914\n",
            "Averaged F1: 0.9985574573441822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VXfLHFv-V6K"
      },
      "source": [
        "## Activation = Sigmoid, Optimizer = adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "\n",
        "Accuracy: 0.9981797575299654\n",
        "\n",
        "Averaged F1: 0.9981796621925855\n",
        "\n",
        "Time: 5m 47s"
      ],
      "id": "2VXfLHFv-V6K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qQmlCRH6__av",
        "outputId": "ac73f383-3039-456a-cc7f-b465910ffbf6"
      },
      "source": [
        "\n",
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='sigmoid', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='sigmoid', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='sigmoid'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "qQmlCRH6__av",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.4635 - accuracy: 0.7680 - val_loss: 0.1083 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10831, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1213 - accuracy: 0.9632 - val_loss: 0.0844 - val_accuracy: 0.9739\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10831 to 0.08436, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0990 - accuracy: 0.9709 - val_loss: 0.0709 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.08436 to 0.07090, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0864 - accuracy: 0.9752 - val_loss: 0.0667 - val_accuracy: 0.9825\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.07090 to 0.06670, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0776 - accuracy: 0.9780 - val_loss: 0.0569 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.06670 to 0.05688, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0682 - accuracy: 0.9805 - val_loss: 0.0527 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.05688 to 0.05267, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0624 - accuracy: 0.9821 - val_loss: 0.0472 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.05267 to 0.04718, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0574 - accuracy: 0.9835 - val_loss: 0.0474 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.04718\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0540 - accuracy: 0.9845 - val_loss: 0.0460 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.04718 to 0.04603, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0516 - accuracy: 0.9855 - val_loss: 0.0411 - val_accuracy: 0.9901\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.04603 to 0.04107, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.0412 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.04107\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9867 - val_loss: 0.0398 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.04107 to 0.03982, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0446 - accuracy: 0.9871 - val_loss: 0.0355 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.03982 to 0.03552, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0422 - accuracy: 0.9877 - val_loss: 0.0370 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03552\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0405 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.03552 to 0.03172, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0394 - accuracy: 0.9883 - val_loss: 0.0362 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03172\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0373 - accuracy: 0.9886 - val_loss: 0.0324 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03172\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0364 - accuracy: 0.9893 - val_loss: 0.0299 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.03172 to 0.02991, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0349 - accuracy: 0.9898 - val_loss: 0.0325 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02991\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0339 - accuracy: 0.9902 - val_loss: 0.0267 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.02991 to 0.02670, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0325 - accuracy: 0.9902 - val_loss: 0.0289 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02670\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0321 - accuracy: 0.9905 - val_loss: 0.0252 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.02670 to 0.02524, saving model to dnn/best_weights.hdf5\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0315 - accuracy: 0.9906 - val_loss: 0.0289 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02524\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0310 - accuracy: 0.9909 - val_loss: 0.0248 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.02524 to 0.02478, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0308 - accuracy: 0.9907 - val_loss: 0.0254 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02478\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0298 - accuracy: 0.9909 - val_loss: 0.0235 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.02478 to 0.02352, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0284 - accuracy: 0.9913 - val_loss: 0.0255 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02352\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0286 - accuracy: 0.9910 - val_loss: 0.0247 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02352\n",
            "Epoch 29/200\n",
            "389/389 - 2s - loss: 0.0272 - accuracy: 0.9919 - val_loss: 0.0251 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.02352\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.0233 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.02352 to 0.02334, saving model to dnn/best_weights.hdf5\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0270 - accuracy: 0.9916 - val_loss: 0.0222 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.02334 to 0.02219, saving model to dnn/best_weights.hdf5\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0263 - accuracy: 0.9918 - val_loss: 0.0232 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02219\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0255 - accuracy: 0.9920 - val_loss: 0.0193 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.02219 to 0.01931, saving model to dnn/best_weights.hdf5\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0262 - accuracy: 0.9920 - val_loss: 0.0207 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01931\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0190 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.01931 to 0.01902, saving model to dnn/best_weights.hdf5\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.0210 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01902\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.0172 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.01902 to 0.01722, saving model to dnn/best_weights.hdf5\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0235 - accuracy: 0.9929 - val_loss: 0.0159 - val_accuracy: 0.9948\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.01722 to 0.01590, saving model to dnn/best_weights.hdf5\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01590\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0226 - accuracy: 0.9931 - val_loss: 0.0167 - val_accuracy: 0.9945\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01590\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.0157 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.01590 to 0.01573, saving model to dnn/best_weights.hdf5\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0213 - accuracy: 0.9930 - val_loss: 0.0171 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01573\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0208 - accuracy: 0.9937 - val_loss: 0.0143 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.01573 to 0.01433, saving model to dnn/best_weights.hdf5\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0130 - val_accuracy: 0.9954\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.01433 to 0.01297, saving model to dnn/best_weights.hdf5\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0132 - val_accuracy: 0.9958\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01297\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0190 - accuracy: 0.9943 - val_loss: 0.0142 - val_accuracy: 0.9953\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01297\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0183 - accuracy: 0.9943 - val_loss: 0.0104 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.01297 to 0.01044, saving model to dnn/best_weights.hdf5\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.0111 - val_accuracy: 0.9959\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01044\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.0104 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.01044 to 0.01038, saving model to dnn/best_weights.hdf5\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0115 - val_accuracy: 0.9959\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01038\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.0078 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.01038 to 0.00781, saving model to dnn/best_weights.hdf5\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0101 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00781\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0085 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00781\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0102 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00781\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.0070 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00781 to 0.00703, saving model to dnn/best_weights.hdf5\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0085 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00703\n",
            "Epoch 00056: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.4817 - accuracy: 0.7556 - val_loss: 0.1189 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00703\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1219 - accuracy: 0.9632 - val_loss: 0.0808 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00703\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0972 - accuracy: 0.9714 - val_loss: 0.0674 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00703\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0843 - accuracy: 0.9761 - val_loss: 0.0603 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00703\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0738 - accuracy: 0.9795 - val_loss: 0.0573 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00703\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0657 - accuracy: 0.9812 - val_loss: 0.0530 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00703\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0608 - accuracy: 0.9826 - val_loss: 0.0505 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00703\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0572 - accuracy: 0.9836 - val_loss: 0.0488 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00703\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0551 - accuracy: 0.9847 - val_loss: 0.0425 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00703\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0499 - accuracy: 0.9856 - val_loss: 0.0417 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00703\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0473 - accuracy: 0.9862 - val_loss: 0.0370 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00703\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0454 - accuracy: 0.9870 - val_loss: 0.0388 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00703\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.0372 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00703\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.0365 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00703\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0402 - accuracy: 0.9883 - val_loss: 0.0306 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00703\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.0328 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00703\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0376 - accuracy: 0.9885 - val_loss: 0.0328 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00703\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0366 - accuracy: 0.9892 - val_loss: 0.0279 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00703\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0350 - accuracy: 0.9897 - val_loss: 0.0281 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00703\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.0306 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00703\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0330 - accuracy: 0.9901 - val_loss: 0.0311 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00703\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0322 - accuracy: 0.9904 - val_loss: 0.0278 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00703\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0313 - accuracy: 0.9905 - val_loss: 0.0230 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00703\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0312 - accuracy: 0.9905 - val_loss: 0.0265 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00703\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0297 - accuracy: 0.9909 - val_loss: 0.0241 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00703\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0285 - accuracy: 0.9917 - val_loss: 0.0247 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00703\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0295 - accuracy: 0.9910 - val_loss: 0.0247 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00703\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0277 - accuracy: 0.9916 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00703\n",
            "Epoch 00028: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.5584 - accuracy: 0.7261 - val_loss: 0.1203 - val_accuracy: 0.9561\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00703\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1242 - accuracy: 0.9625 - val_loss: 0.0860 - val_accuracy: 0.9743\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00703\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0988 - accuracy: 0.9709 - val_loss: 0.0699 - val_accuracy: 0.9821\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00703\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0845 - accuracy: 0.9762 - val_loss: 0.0603 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00703\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0751 - accuracy: 0.9788 - val_loss: 0.0574 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00703\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0675 - accuracy: 0.9811 - val_loss: 0.0528 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00703\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0629 - accuracy: 0.9821 - val_loss: 0.0500 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00703\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0597 - accuracy: 0.9830 - val_loss: 0.0486 - val_accuracy: 0.9861\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00703\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0566 - accuracy: 0.9835 - val_loss: 0.0434 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00703\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0530 - accuracy: 0.9846 - val_loss: 0.0411 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00703\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0513 - accuracy: 0.9851 - val_loss: 0.0392 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00703\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0486 - accuracy: 0.9859 - val_loss: 0.0380 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00703\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.0348 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00703\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0443 - accuracy: 0.9869 - val_loss: 0.0338 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00703\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0420 - accuracy: 0.9876 - val_loss: 0.0365 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00703\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0406 - accuracy: 0.9877 - val_loss: 0.0320 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00703\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0400 - accuracy: 0.9879 - val_loss: 0.0335 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00703\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0381 - accuracy: 0.9885 - val_loss: 0.0296 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00703\n",
            "Epoch 19/200\n",
            "389/389 - 2s - loss: 0.0365 - accuracy: 0.9888 - val_loss: 0.0284 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00703\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0274 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00703\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0334 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00703\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.0260 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00703\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0325 - accuracy: 0.9900 - val_loss: 0.0258 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00703\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0320 - accuracy: 0.9901 - val_loss: 0.0243 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00703\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0310 - accuracy: 0.9903 - val_loss: 0.0273 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00703\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.0225 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00703\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0297 - accuracy: 0.9905 - val_loss: 0.0226 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00703\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0222 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00703\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0274 - accuracy: 0.9916 - val_loss: 0.0246 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00703\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0273 - accuracy: 0.9916 - val_loss: 0.0225 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00703\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0187 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00703\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.0174 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00703\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0254 - accuracy: 0.9922 - val_loss: 0.0186 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00703\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0250 - accuracy: 0.9920 - val_loss: 0.0202 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00703\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0242 - accuracy: 0.9924 - val_loss: 0.0161 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00703\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.0167 - val_accuracy: 0.9946\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00703\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0223 - accuracy: 0.9928 - val_loss: 0.0174 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00703\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.0155 - val_accuracy: 0.9945\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00703\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0209 - accuracy: 0.9933 - val_loss: 0.0136 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00703\n",
            "Epoch 40/200\n",
            "389/389 - 2s - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.0117 - val_accuracy: 0.9953\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00703\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.0141 - val_accuracy: 0.9950\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00703\n",
            "Epoch 42/200\n",
            "389/389 - 2s - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.0102 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00703\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0094 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00703\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.0098 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00703\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0172 - accuracy: 0.9943 - val_loss: 0.0104 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00703\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0089 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00703\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0090 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00703\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.0080 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00703\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0092 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00703\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.0079 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00703\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0074 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00703\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0074 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00703\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.0072 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00703\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.0075 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00703\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00703 to 0.00661, saving model to dnn/best_weights.hdf5\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.0088 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00661\n",
            "Epoch 00056: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.5428 - accuracy: 0.7323 - val_loss: 0.1121 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00661\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1199 - accuracy: 0.9636 - val_loss: 0.0825 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00661\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0964 - accuracy: 0.9724 - val_loss: 0.0680 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00661\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0875 - accuracy: 0.9748 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00661\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0766 - accuracy: 0.9783 - val_loss: 0.0568 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00661\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0705 - accuracy: 0.9797 - val_loss: 0.0575 - val_accuracy: 0.9858\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00661\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0644 - accuracy: 0.9818 - val_loss: 0.0547 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00661\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0600 - accuracy: 0.9826 - val_loss: 0.0469 - val_accuracy: 0.9874\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00661\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0570 - accuracy: 0.9837 - val_loss: 0.0473 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00661\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0539 - accuracy: 0.9849 - val_loss: 0.0470 - val_accuracy: 0.9864\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00661\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0519 - accuracy: 0.9851 - val_loss: 0.0459 - val_accuracy: 0.9865\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00661\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0487 - accuracy: 0.9862 - val_loss: 0.0394 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00661\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0473 - accuracy: 0.9865 - val_loss: 0.0396 - val_accuracy: 0.9900\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00661\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0452 - accuracy: 0.9869 - val_loss: 0.0357 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00661\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0436 - accuracy: 0.9875 - val_loss: 0.0393 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00661\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0415 - accuracy: 0.9879 - val_loss: 0.0354 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00661\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0393 - accuracy: 0.9888 - val_loss: 0.0336 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00661\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0399 - accuracy: 0.9885 - val_loss: 0.0368 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00661\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.0332 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00661\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.0307 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00661\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0354 - accuracy: 0.9899 - val_loss: 0.0305 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00661\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0352 - accuracy: 0.9898 - val_loss: 0.0324 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00661\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0339 - accuracy: 0.9902 - val_loss: 0.0298 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00661\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0332 - accuracy: 0.9905 - val_loss: 0.0265 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00661\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0329 - accuracy: 0.9906 - val_loss: 0.0253 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00661\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0318 - accuracy: 0.9908 - val_loss: 0.0258 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00661\n",
            "Epoch 27/200\n",
            "389/389 - 2s - loss: 0.0309 - accuracy: 0.9910 - val_loss: 0.0259 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00661\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0308 - accuracy: 0.9909 - val_loss: 0.0281 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00661\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.0253 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00661\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0293 - accuracy: 0.9914 - val_loss: 0.0271 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00661\n",
            "Epoch 00030: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.5179 - accuracy: 0.7413 - val_loss: 0.1153 - val_accuracy: 0.9571\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00661\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1207 - accuracy: 0.9633 - val_loss: 0.0922 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00661\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0971 - accuracy: 0.9713 - val_loss: 0.0672 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00661\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0821 - accuracy: 0.9767 - val_loss: 0.0602 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00661\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0742 - accuracy: 0.9789 - val_loss: 0.0540 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00661\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0672 - accuracy: 0.9811 - val_loss: 0.0505 - val_accuracy: 0.9847\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00661\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0608 - accuracy: 0.9826 - val_loss: 0.0511 - val_accuracy: 0.9868\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00661\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0572 - accuracy: 0.9836 - val_loss: 0.0450 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00661\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.0418 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00661\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0505 - accuracy: 0.9855 - val_loss: 0.0440 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00661\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0377 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00661\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0470 - accuracy: 0.9866 - val_loss: 0.0430 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00661\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0445 - accuracy: 0.9872 - val_loss: 0.0391 - val_accuracy: 0.9905\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00661\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0430 - accuracy: 0.9874 - val_loss: 0.0351 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00661\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0411 - accuracy: 0.9880 - val_loss: 0.0364 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00661\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0392 - accuracy: 0.9885 - val_loss: 0.0304 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00661\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.0336 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00661\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0364 - accuracy: 0.9893 - val_loss: 0.0285 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00661\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0352 - accuracy: 0.9893 - val_loss: 0.0330 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00661\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0352 - accuracy: 0.9893 - val_loss: 0.0271 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00661\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0337 - accuracy: 0.9898 - val_loss: 0.0262 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00661\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.0243 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00661\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0316 - accuracy: 0.9903 - val_loss: 0.0247 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00661\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0252 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00661\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0305 - accuracy: 0.9906 - val_loss: 0.0228 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00661\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0297 - accuracy: 0.9908 - val_loss: 0.0279 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00661\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0214 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00661\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0287 - accuracy: 0.9912 - val_loss: 0.0257 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00661\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0229 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00661\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.0211 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00661\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0227 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00661\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0258 - accuracy: 0.9920 - val_loss: 0.0186 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00661\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0251 - accuracy: 0.9920 - val_loss: 0.0216 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00661\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.0225 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00661\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0240 - accuracy: 0.9921 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00661\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0233 - accuracy: 0.9926 - val_loss: 0.0174 - val_accuracy: 0.9940\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00661\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00661\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0224 - accuracy: 0.9928 - val_loss: 0.0143 - val_accuracy: 0.9950\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00661\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.0169 - val_accuracy: 0.9941\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00661\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.0179 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00661\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.0170 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00661\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0130 - val_accuracy: 0.9954\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00661\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0128 - val_accuracy: 0.9958\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00661\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.0111 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00661\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0171 - accuracy: 0.9944 - val_loss: 0.0107 - val_accuracy: 0.9963\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00661\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0127 - val_accuracy: 0.9954\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00661\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0156 - accuracy: 0.9951 - val_loss: 0.0085 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00661\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0157 - accuracy: 0.9952 - val_loss: 0.0099 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00661\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0109 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00661\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0088 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00661\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0072 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00661\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.0072 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00661\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.0095 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00661\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0064 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00661 to 0.00638, saving model to dnn/best_weights.hdf5\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00638 to 0.00588, saving model to dnn/best_weights.hdf5\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0071 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00588\n",
            "Epoch 57/200\n",
            "389/389 - 1s - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0071 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00588\n",
            "Epoch 58/200\n",
            "389/389 - 1s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00588\n",
            "Epoch 59/200\n",
            "389/389 - 1s - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00588\n",
            "Epoch 60/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00588\n",
            "Epoch 00060: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17566    23]\n",
            " [   30 11498]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEmCAYAAAAnRIjxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf6ElEQVR4nO3debgdVZ3u8e+bRCaZCSIGkKgRjdxGMQLKlUZoIaC3gz4OIGoa0x0HRNvhqtjejo3i1dYWoRVtlAiIMogDURHIRbmIjwwRESGA5IJIwhgS5jHw3j9qHdkezrD3zq7UOfu8H556TtWqVVVrn5Bf1l61BtkmIiJ6b1LTBYiI6FcJsBERNUmAjYioSQJsRERNEmAjImqSABsRUZME2AlE0oaSfiLpXknfX4v7HCrp/F6WrSmSXi3p+qbLEf1J6Qc79kh6G/Bh4EXA/cCVwNG2L17L+74DOAJ4le01a13QMU6SgRm2lzVdlpiYUoMdYyR9GPgK8DlgG2AH4HhgTg9u/1zgjxMhuLZD0pSmyxB9zna2MbIBmwEPAG8eIc/6VAH41rJ9BVi/nNsbWA58BLgTuA04rJz7N+Ax4PHyjHnAp4FTW+69I2BgSjn+B+BGqlr0TcChLekXt1z3KuBy4N7y81Ut5y4EPgP8utznfGDqMJ9toPwfayn/QcCBwB+BVcAnW/LvBvwGuKfk/SqwXjl3UfksD5bP+9aW+38cuB34zkBaueb55Rm7luPnAHcBezf9/0a28bmlBju2vBLYAPjRCHn+BdgDeCmwC1WQ+VTL+WdTBeppVEH0a5K2sL2AqlZ8hu2NbZ84UkEkPRM4DjjA9iZUQfTKIfJtCfys5N0K+DLwM0lbtWR7G3AY8CxgPeCjIzz62VS/g2nAvwLfBN4OvBx4NfC/JE0veZ8APgRMpfrd7Qu8D8D2XiXPLuXzntFy/y2pavPzWx9s+/9RBd9TJW0EfBs42faFI5Q3YlgJsGPLVsBKj/wV/lDgKNt32r6Lqmb6jpbzj5fzj9s+h6r2tlOX5XkS2FnShrZvs33NEHleB9xg+zu219g+DbgO+B8teb5t+4+2HwbOpPrHYTiPU7U3Pw6cThU8j7V9f3n+Uqp/WLD9W9uXlOf+Cfgv4G/b+EwLbD9ayvNXbH8TWAZcCmxL9Q9aRFcSYMeWu4Gpo7QNPge4ueX45pL2l3sMCtAPARt3WhDbD1J9rX4PcJukn0l6URvlGSjTtJbj2zsoz922nyj7AwHwjpbzDw9cL+mFkn4q6XZJ91HV0KeOcG+Au2w/MkqebwI7A/9p+9FR8kYMKwF2bPkN8ChVu+NwbqX6ejtgh5LWjQeBjVqOn9160vZ5tl9LVZO7jirwjFaegTKt6LJMnfg6Vblm2N4U+CSgUa4ZsduMpI2p2rVPBD5dmkAiupIAO4bYvpeq3fFrkg6StJGkZ0g6QNK/l2ynAZ+StLWkqSX/qV0+8kpgL0k7SNoMOHLghKRtJM0pbbGPUjU1PDnEPc4BXijpbZKmSHorMBP4aZdl6sQmwH3AA6V2/d5B5+8AntfhPY8Fltj+R6q25W+sdSljwkqAHWNs/wdVH9hPUb3BvgV4P/DjkuWzwBLgKuAPwBUlrZtnLQbOKPf6LX8dFCeVctxK9Wb9b3l6AMP23cDrqXou3E3VA+D1tld2U6YOfZTqBdr9VLXrMwad/zRwsqR7JL1ltJtJmgPM5qnP+WFgV0mH9qzEMaFkoEFERE1Sg42IqEkCbERETRJgIyJqkgAbEVGTMTXZhaZsaK23SdPFiB552Yt3aLoI0SM33/wnVq5cOVof445M3vS59pqnDaYblh++6zzbs3tZhrqNrQC73iasv9OovWlinPj1pV9tugjRI3vuPqvn9/Sahzv6+/7IlV8bbZTemDOmAmxETCQC9XcrZQJsRDRDgHra6jDmJMBGRHNSg42IqINg0uSmC1GrBNiIaE6aCCIiaiDSRBARUQ+lBhsRUZvUYCMiapIabEREHTLQICKiHhloEBFRo9RgIyLqIJicgQYREb2XfrARETVKG2xERB36vxdBf3+6iBjbpPa3UW+lhZLulHT1oPQjJF0n6RpJ/96SfqSkZZKul7R/S/rskrZM0ida0qdLurSknyFpvdHKlAAbEc3RpPa30Z0E/NWSMpJeA8wBdrH9EuBLJX0mcDDwknLN8ZImS5oMfA04AJgJHFLyAnwBOMb2C4DVwLzRCpQAGxHN6KT22kYN1vZFwKpBye8FPm/70ZLnzpI+Bzjd9qO2bwKWAbuVbZntG20/BpwOzJEkYB/grHL9ycBBo5UpATYimtNZDXaqpCUt2/w2nvBC4NXlq/3/lfSKkj4NuKUl3/KSNlz6VsA9ttcMSh9RXnJFRHM660Ww0nanqy9OAbYE9gBeAZwp6Xkd3qNrCbAR0ZB10otgOfBD2wYuk/QkMBVYAWzfkm+7ksYw6XcDm0uaUmqxrfmHlSaCiGiGqJaMaXfrzo+B1wBIeiGwHrASWAQcLGl9SdOBGcBlwOXAjNJjYD2qF2GLSoD+JfCmct+5wNmjPTw12IhoSG9rsJJOA/amaqtdDiwAFgILS9etx4C5JVheI+lMYCmwBjjc9hPlPu8HzgMmAwttX1Me8XHgdEmfBX4HnDhamRJgI6I5PRzJZfuQYU69fZj8RwNHD5F+DnDOEOk3UvUyaFsCbEQ0p89HciXARkRzMhdBREQN1P9zESTARkRzUoONiKiHEmAjInqvWpIrATYiovckNCkBNiKiFqnBRkTUJAE2IqImCbAREXVQ2fpYAmxENEIoNdiIiLokwEZE1CQBNiKiJgmwERF1yEuuiIh6CDFpUn/PptXfny4ixjRJbW9t3GuhpDvL8jCDz31EkiVNLceSdJykZZKukrRrS965km4o29yW9JdL+kO55ji1UagE2IhojjrYRncSMPtpj5C2B/YD/tySfADVQoczgPnA10veLanW8tqdanmYBZK2KNd8Hfinluue9qzBEmAjohnqbQ3W9kXAqiFOHQN8DHBL2hzgFFcuoVqSe1tgf2Cx7VW2VwOLgdnl3Ka2LymLJp4CHDRamdIGGxGNqbsXgaQ5wArbvx/0rGnALS3Hy0vaSOnLh0gfUQJsRDSmwwA7VdKSluMTbJ8wwr03Aj5J1TzQiATYiGhEF0NlV9qe1UH+5wPTgYHa63bAFZJ2A1YA27fk3a6krQD2HpR+YUnfboj8I0obbEQ0p7cvuf6K7T/YfpbtHW3vSPW1flfbtwOLgHeW3gR7APfavg04D9hP0hbl5dZ+wHnl3H2S9ii9B94JnD1aGVKDjYhmqLdtsJJOo6p9TpW0HFhg+8Rhsp8DHAgsAx4CDgOwvUrSZ4DLS76jbA+8OHsfVU+FDYGfl21ECbAR0ZheBljbh4xyfseWfQOHD5NvIbBwiPQlwM6dlCkBNiIakzW5IiJq0u+TvdT6kkvSbEnXl6Fln6jzWRExvnQyyGC8BuLaarCSJgNfA15L9fbuckmLbC+t65kRMb6M18DZrjprsLsBy2zfaPsx4HSq4WkREUBvh8qORXUG2OGGnP0VSfMlLZG0xGserrE4ETHm1NgPdixo/CVXGep2AsCkjZ7lUbJHRB8ZrzXTdtUZYIcbihYR0fOBBmNRnU0ElwMzJE2XtB5wMNXwtIiI6pu/2t/Go9pqsLbXSHo/1djeycBC29fU9byIGG/EpAw06J7tc6jG/EZEPE2/NxE0/pIrIiaocfzVv10JsBHRCEGaCCIi6pIabERETdIGGxFRh7TBRkTUo+oH298RNgE2IhoyfidxaVcWPYyIxvRyJJekhZLulHR1S9oXJV0n6SpJP5K0ecu5I8tc1ddL2r8lfch5rMuo1EtL+hllhOqIEmAjohmqumm1u7XhJGD2oLTFwM62/wb4I3AkgKSZVMP3X1KuOV7S5JZ5rA8AZgKHlLwAXwCOsf0CYDUwb7QCJcBGRCMG2mB7NR+s7YuAVYPSzre9phxeQjXpFFRzU59u+1HbN1GtLrsbw8xjXZbq3gc4q1x/MnDQaGVKgI2IxnTYRDB1YO7oss3v8HHv4qmltoebr3q49K2Ae1qC9ZDzWw+Wl1wR0ZgOX3KttD2ry+f8C7AG+G4313crATYiGrMuOhFI+gfg9cC+tgcm9R9pvuqh0u8GNpc0pdRi25rfOk0EEdEM1b8ml6TZwMeAv7f9UMupRcDBktaXNB2YAVzGMPNYl8D8S+BN5fq5wNmjPT8BNiIa0esJtyWdBvwG2EnScknzgK8CmwCLJV0p6RsAZW7qM4GlwLnA4bafKLXTgXmsrwXObJnH+uPAhyUto2qTPXG0MqWJICIa0tuBBrYPGSJ52CBo+2jg6CHSh5zH2vaNVL0M2pYAGxGN6fOBXAmwEdEQZT7YiIhaZLKXiIgaJcBGRNSkz+NrAmxENCc12IiIOmRFg4iIemgCTLidABsRjenz+JoAGxHNmdTnETYBNiIa0+fxNQE2IpohweSM5IqIqEdeckVE1KTP4+vwAVbSfwIe7rztD9RSooiYEETVVaufjVSDXbLOShERE1KfN8EOH2Btn9x6LGmjQUsuRER0by2WghkvRl0yRtIrJS0FrivHu0g6vvaSRUTf6/GSMQsl3Snp6pa0LSUtlnRD+blFSZek4yQtk3SVpF1brplb8t8gaW5L+ssl/aFcc5za+NehnTW5vgLsT7WqIrZ/D+zVxnUREcMS1UCDdrc2nATMHpT2CeAC2zOAC8oxwAFUCx3OAOYDX4cqIAMLgN2plodZMBCUS55/arlu8LOepq1FD23fMijpiXaui4gYSS9rsLYvAlYNSp4DDDR3ngwc1JJ+iiuXUC3JvS1VZXKx7VW2VwOLgdnl3Ka2LykrzJ7Scq9htdNN6xZJrwIs6RnAB6lWW4yIWCsdtsFOldT68v0E2yeMcs02tm8r+7cD25T9aUBrxXF5SRspffkQ6SNqJ8C+Bzi23OxWquVsD2/juoiIYXUxkmul7VndPs+2JQ3b9bQOowZY2yuBQ9dBWSJiglkHfQjukLSt7dvK1/w7S/oKYPuWfNuVtBXA3oPSLyzp2w2Rf0Tt9CJ4nqSfSLqrvKE7W9LzRrsuImI0Kl212tm6tAgY6AkwFzi7Jf2dpTfBHsC9pSnhPGA/SVuUl1v7AeeVc/dJ2qP0Hnhny72G1U4TwfeArwFvKMcHA6dRvWWLiOhK1Yugh/eTTqOqfU6VtJyqN8DngTMlzQNuBt5Ssp8DHAgsAx4CDgOwvUrSZ4DLS76jbA+8OHsfVU+FDYGfl21E7QTYjWx/p+X4VEn/s43rIiKG1+OBBrYPGebUvkPkNcO8S7K9EFg4RPoSYOdOyjTSXARblt2fS/oEcDrV3ARvpYr+ERFrpc8Hco1Yg/0tVUAd+BW8u+WcgSPrKlRETAz9PlR2pLkIpq/LgkTExNLrNtixqK35YCXtDMwENhhIs31KXYWKiIlhwtZgB0haQPVmbiZV2+sBwMVUQ8UiIroiweQ+D7DtzEXwJqq3cLfbPgzYBdis1lJFxITQy7kIxqJ2mggetv2kpDWSNqUaCbH9aBdFRIxmwjcRAEskbQ58k6pnwQPAb2otVURMCH0eX9uai+B9Zfcbks6lmrLrqnqLFRH9TrQ9z+u4NdJAg11HOmf7inqKFBETwjhuW23XSDXY/xjhnIF9elwWXvbiHfj1pV/t9W2jIfsec1HTRYgeuf6OB2q574Rtg7X9mnVZkIiYeNpaUmUca2ugQUREr4kJXIONiKhbhspGRNSgiyVjxp12VjSQpLdL+tdyvIOk3eovWkT0u0lqfxuP2mljPh54JTAwme39VCscRESslQyVhd1t7yrpdwC2V0tar+ZyRUSfq6YrHKeRs03t1GAflzSZqu8rkrYGnqy1VBExIUzqYBuNpA9JukbS1ZJOk7SBpOmSLpW0TNIZA5VDSeuX42Xl/I4t9zmypF8vaf+1/XyjOQ74EfAsSUdTTVX4ubV5aEQE9K6JQNI04APALNs7A5OpFmj9AnCM7RcAq4F55ZJ5wOqSfkzJh6SZ5bqXALOB40sFsyujBljb3wU+Bvxv4DbgINvf7/aBERFQ9YGd1MHWhinAhpKmABtRxat9gLPK+ZOBg8r+nHJMOb9vWY57DnC67Udt30S16mzXL/XbmXB7B6plbX/Smmb7z90+NCICOn55NVXSkpbjE2yfAGB7haQvAX8GHgbOp5r97x7ba0r+5cC0sj8NuKVcu0bSvcBWJf2Slme0XtOxdl5y/YynFj/cAJgOXE9VhY6I6FqH3a9W2p411AlJW1DVPqcD9wDfp/qK36h2piv8b63HZZat9w2TPSKiLaKnAw3+DrjJ9l0Akn4I7AlsLmlKqcVuB6wo+VdQLRywvDQpbAbc3ZI+oPWajnU810KZpnD3bh8YEQFAB4MM2ojDfwb2kLRRaUvdF1gK/JJq2SuAucDZZX9ROaac/4Vtl/SDSy+D6cAM4LJuP2I7bbAfbjmcBOwK3NrtAyMiBoje1GBtXyrpLOAKYA3wO+AEqibO0yV9tqSdWC45EfiOpGXAKqqeA9i+RtKZVMF5DXC47Se6LVc7bbCbtOyvKQX+QbcPjIiAgYEGvbuf7QXAgkHJNzJELwDbjwBvHuY+RwNH96JMIwbY0v9rE9sf7cXDIiJajdc5Bto10pIxU0r3hT3XZYEiYuKYyPPBXkbV3nqlpEVU3R4eHDhp+4c1ly0i+livmwjGonbaYDeg6r6wD0/1hzWQABsR3RvHs2S1a6QA+6zSg+BqngqsA1xrqSJiQuj32bRGCrCTgY1hyH4UCbARsVYmehPBbbaPWmcliYgJRkyewDXY/v7kEdGoalXZpktRr5EC7L7rrBQRMfGM47W22jVsgLW9al0WJCImnon8kisiojYTvYkgIqJWqcFGRNSkz+NrAmxENEN0MSH1OJMAGxHN0MSe7CUiolb9HV4TYCOiIYK+H8nV700gETGGSe1vo99Lm0s6S9J1kq6V9EpJW0paLOmG8nOLkleSjpO0TNJVZTHXgfvMLflvkDR3+CeOLgE2IhoipPa3NhwLnGv7RcAuwLXAJ4ALbM8ALijHAAdQLWg4A5gPfB1A0pZUy87sTrXUzIKBoNyNBNiIaMRAL4J2txHvJW0G7EVZ1ND2Y7bvAeYAJ5dsJwMHlf05wCmuXEK1vPe2wP7AYturbK8GFgOzu/2MCbAR0ZgOa7BTJS1p2ea33Go6cBfwbUm/k/QtSc8EtrF9W8lzO7BN2Z8G3NJy/fKSNlx6V/KSKyIa0+ErrpW2Zw1zbgrVEldHlCW8j+Wp5gAAbFvSOp3LOjXYiGiGOq7BjmQ5sNz2peX4LKqAe0f56k/5eWc5vwLYvuX67UracOldSYCNiEb0sg3W9u3ALZJ2Kkn7AkuBRcBAT4C5wNllfxHwztKbYA/g3tKUcB6wn6Qtysut/UpaV9JEEBGN6fFIriOA70paD7gROIwqNp8paR5wM/CWkvcc4EBgGfBQyYvtVZI+A1xe8h21NlO3JsBGRGN6OeG27SuBodpon7Z4gG0Dhw9zn4XAwl6UKQE2IhpRNRH090iuBNiIaEyfj5RNgI2IpgilBhsRUY/UYCMiapA22IiIurQ5S9Z4lgAbEY1JgI2IqEleckVE1ED0dqDBWJQAGxGNmdTnbQQJsBHRmDQRRETUYCI0EdQ2XaGkhZLulHR1Xc+IiPFMHf03HtU5H+xJrMVaNhHR5zpYUXa8NtXWFmBtXwR0PY9iRPQ/dbCNR423wZaFy+YDbL/DDg2XJiLWlaoNdryGzvY0vmSM7RNsz7I9a+upWzddnIhYh/q9Btt4gI2ICazHEVbS5LJs90/L8XRJl0paJumMspwMktYvx8vK+R1b7nFkSb9e0v5r8/ESYCOiMZOktrc2fRC4tuX4C8Axtl8ArAbmlfR5wOqSfkzJh6SZwMHAS6he0h8vaXLXn6/bC0cj6TTgN8BOkpaXRcciIv6ilxVYSdsBrwO+VY4F7EO1hDfAycBBZX9OOaac37fknwOcbvtR2zdRLYq4W7efr7aXXLYPqeveEdEnetu4+hXgY8Am5Xgr4B7ba8rxcmBa2Z8G3AJge42ke0v+acAlLfdsvaZjaSKIiEZUNdOOBhpMlbSkZZv/l3tJrwfutP3bpj7PUBrvphURE1TnAwhW2h5qWW6APYG/l3QgsAGwKXAssLmkKaUWux2wouRfAWwPLJc0BdgMuLslfUDrNR1LDTYiGtOrNljbR9rezvaOVC+pfmH7UOCXwJtKtrnA2WV/UTmmnP+FbZf0g0svg+nADOCybj9farAR0Zz6O7h+HDhd0meB3wEnlvQTge9IWkY14vRgANvXSDoTWAqsAQ63/US3D0+AjYiG1DOJi+0LgQvL/o0M0QvA9iPAm4e5/mjg6F6UJQE2IhrT5yNlE2AjohnjeQhsuxJgI6Ix6vMqbAJsRDSmz+NrAmxENKfP42sCbEQ0ZAI0wibARkRjxutaW+1KgI2IRoi0wUZE1KbP42sCbEQ0qM8jbAJsRDQmbbARETWZ1N/xNQE2IhqUABsR0XsDKxr0swTYiGhG5ysajDsJsBHRmD6PrwmwEdGgPo+wCbAR0ZB6VjQYS7LoYUQ0Rmp/G/k+2l7SLyUtlXSNpA+W9C0lLZZ0Q/m5RUmXpOMkLZN0laRdW+41t+S/QdLc4Z7ZjgTYiGhEJyvKtlHPXQN8xPZMYA/gcEkzgU8AF9ieAVxQjgEOoFoxdgYwH/g6VAEZWADsTrWW14KBoNyNBNiIaE6PIqzt22xfUfbvB64FpgFzgJNLtpOBg8r+HOAUVy4BNpe0LbA/sNj2KturgcXA7G4/XtpgI6IxkzrrpzVV0pKW4xNsnzA4k6QdgZcBlwLb2L6tnLod2KbsTwNuablseUkbLr0rCbAR0ZgOX3GttD1rxPtJGwM/AP7Z9n2ta37ZtiR3UcyupYkgIprRwQuudiq6kp5BFVy/a/uHJfmO8tWf8vPOkr4C2L7l8u1K2nDpXUmAjYgG9aYRVlVV9UTgWttfbjm1CBjoCTAXOLsl/Z2lN8EewL2lKeE8YD9JW5SXW/uVtK6kiSAiGtHjFQ32BN4B/EHSlSXtk8DngTMlzQNuBt5Szp0DHAgsAx4CDgOwvUrSZ4DLS76jbK/qtlAJsBHRmF7FV9sXj3C7fYfIb+DwYe61EFjYi3IlwEZEYzLZS0RETfp9qGwCbEQ0p7/jawJsRDSnz+NrAmxENEPqeCTXuJMAGxHN6e/4mgAbEc3p8/iaABsRzenzFoIE2IhoSv+vaJAAGxGN6PFQ2TEpk71ERNQkNdiIaEy/12ATYCOiMWmDjYioQTXQoOlS1CsBNiKakwAbEVGPNBFERNQkL7kiImrS5/E1ATYiGtTnETYBNiIa0+9tsKrW/hobJN1FtfJjv5sKrGy6ENETE+XP8rm2t+7lDSWdS/X7a9dK27N7WYa6jakAO1FIWmJ7VtPliLWXP8sYSeYiiIioSQJsRERNEmCbcULTBYieyZ9lDCttsBERNUkNNiKiJgmwERE1SYCNiKhJAuw6IGknSa+U9AxJk5suT6y9/DlGO/KSq2aS3gh8DlhRtiXASbbva7Rg0RVJL7T9x7I/2fYTTZcpxq7UYGsk6RnAW4F5tvcFzga2Bz4uadNGCxcdk/R64EpJ3wOw/URqsjGSBNj6bQrMKPs/An4KPAN4m9Tvs2H2D0nPBN4P/DPwmKRTIUE2RpYAWyPbjwNfBt4o6dW2nwQuBq4E/nujhYuO2H4QeBfwPeCjwAatQbbJssXYlQBbv18B5wPvkLSX7Sdsfw94DrBLs0WLTti+1fYDtlcC7wY2HAiyknaV9KJmSxhjTeaDrZntRyR9FzBwZPlL+CiwDXBbo4WLrtm+W9K7gS9Kug6YDLym4WLFGJMAuw7YXi3pm8BSqprPI8Dbbd/RbMlibdheKekq4ADgtbaXN12mGFvSTWsdKy9EXNpjYxyTtAVwJvAR21c1XZ4YexJgI9aCpA1sP9J0OWJsSoCNiKhJehFERNQkATYioiYJsBERNUmAjYioSQJsn5D0hKQrJV0t6fuSNlqLe50k6U1l/1uSZo6Qd29Jr+riGX+SNLXd9EF5HujwWZ+W9NFOyxixthJg+8fDtl9qe2fgMeA9rScldTWoxPY/2l46Qpa9gY4DbMREkADbn34FvKDULn8laRGwVNJkSV+UdLmkq8pQT1T5qqTrJf0f4FkDN5J0oaRZZX+2pCsk/V7SBZJ2pArkHyq151dL2lrSD8ozLpe0Z7l2K0nnS7pG0reAUWcSk/RjSb8t18wfdO6Ykn6BpK1L2vMlnVuu+VXmBoimZahsnyk11QOAc0vSrsDOtm8qQepe26+QtD7wa0nnAy8DdgJmUs2RsBRYOOi+WwPfBPYq99rS9ipJ3wAesP2lku97wDG2L5a0A3Ae8GJgAXCx7aMkvQ6Y18bHeVd5xobA5ZJ+YPtu4JnAEtsfkvSv5d7vp1pC+z22b5C0O3A8sE8Xv8aInkiA7R8bSrqy7P8KOJHqq/tltm8q6fsBfzPQvgpsRjVX7V7AaWXavVsl/WKI++8BXDRwL9urhinH3wEzW6a63VTSxuUZbyzX/kzS6jY+0wckvaHsb1/KejfwJHBGST8V+GF5xquA77c8e/02nhFRmwTY/vGw7Ze2JpRA82BrEnCE7fMG5Tuwh+WYBOwxePhop3OLS9qbKli/0vZDki4ENhgmu8tz7xn8O4hoUtpgJ5bzgPeWpWyQ9MIyU/9FwFtLG+22DD3t3iXAXpKml2u3LOn3A5u05DsfOGLgQNJAwLsIeFtJOwDYYpSybgasLsH1RVQ16AGTgIFa+Nuomh7uA26S9ObyDEnKfLvRqATYieVbVO2rV0i6Gvgvqm8xPwJuKOdOAX4z+ELbdwHzqb6O/56nvqL/BHjDwEsu4APArPISbSlP9Wb4N6oAfQ1VU8GfRynrucAUSdcCn6cK8AMeBHYrn2Ef4KiSfigwr5TvGmBOG7+TiNpkspeIiJqkBhsRUZME2IiImiTARkTUJAE2IqImCbARETVJgI2IqEkCbERETf4/IBPim7Bkp8YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9981797575299654\n",
            "Averaged F1: 0.9981796621925855\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZyKV4w-VDH"
      },
      "source": [
        "## Activation = Tanh, Optimizer = adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accurary: 0.9985918879005392\n",
        "\n",
        "Average F1: 0.9985919404666145\n",
        "\n",
        "Time: 1m 34s\n"
      ],
      "id": "-zZyKV4w-VDH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RK6lH316AIAJ",
        "outputId": "020f88b7-0496-4106-c3c5-36e921f17fc0"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='tanh', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='tanh', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='tanh'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "RK6lH316AIAJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0555 - accuracy: 0.9831 - val_loss: 0.0343 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03425, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0393 - accuracy: 0.9869 - val_loss: 0.0242 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03425 to 0.02415, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0328 - accuracy: 0.9891 - val_loss: 0.0266 - val_accuracy: 0.9897\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02415\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0277 - accuracy: 0.9910 - val_loss: 0.0160 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02415 to 0.01602, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.0136 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01602 to 0.01358, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0115 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01358 to 0.01152, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.0110 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01152 to 0.01095, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0123 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01095\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.0095 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01095 to 0.00954, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.0089 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00954 to 0.00887, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.0083 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00887 to 0.00832, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00832 to 0.00754, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0065 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00754 to 0.00651, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0082 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00651\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00651 to 0.00637, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0060 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00637 to 0.00597, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0058 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00597 to 0.00582, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "389/389 - 2s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00582 to 0.00498, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0062 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00498\n",
            "Epoch 20/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0050 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.00498 to 0.00497, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00497\n",
            "Epoch 22/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00497\n",
            "Epoch 23/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00497\n",
            "Epoch 00023: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0556 - accuracy: 0.9821 - val_loss: 0.0306 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00497\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.0257 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00497\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.0146 - val_accuracy: 0.9946\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00497\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0122 - val_accuracy: 0.9953\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00497\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.0117 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00497\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.0099 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00497\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.0095 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00497\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0146 - accuracy: 0.9957 - val_loss: 0.0086 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00497\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0071 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00497\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0069 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00497\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00497\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00497\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0060 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00497\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0060 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00497\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00497\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00497\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00497\n",
            "Epoch 00017: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0565 - accuracy: 0.9825 - val_loss: 0.0394 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00497\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0400 - accuracy: 0.9868 - val_loss: 0.0216 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00497\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0317 - accuracy: 0.9891 - val_loss: 0.0166 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00497\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.0130 - val_accuracy: 0.9960\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00497\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0244 - accuracy: 0.9922 - val_loss: 0.0123 - val_accuracy: 0.9960\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00497\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.0102 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00497\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.0095 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00497\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0190 - accuracy: 0.9942 - val_loss: 0.0087 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00497\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.0116 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00497\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0081 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00497\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0079 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00497\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0082 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00497\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00497\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00497\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00497\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0054 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00497\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00497\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0054 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00497\n",
            "Epoch 00018: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0330 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00497\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0373 - accuracy: 0.9876 - val_loss: 0.0193 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00497\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.0147 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00497\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.0117 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00497\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.0114 - val_accuracy: 0.9963\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00497\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.0108 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00497\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0109 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00497\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0103 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00497\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.0078 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00497\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0076 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00497\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00497\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0064 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00497\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00497\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0068 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00497\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0063 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00497\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00497\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00497\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00497 to 0.00493, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00493\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00493\n",
            "Epoch 21/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0054 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00493\n",
            "Epoch 00021: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0562 - accuracy: 0.9828 - val_loss: 0.0337 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00493\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0391 - accuracy: 0.9869 - val_loss: 0.0257 - val_accuracy: 0.9890\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00493\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.0215 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00493\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0266 - accuracy: 0.9913 - val_loss: 0.0152 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00493\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.0148 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00493\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0121 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00493\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.0106 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00493\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0117 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00493\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.0096 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00493\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0146 - accuracy: 0.9956 - val_loss: 0.0085 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00493\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0073 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00493\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.0065 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00493\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.0073 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00493\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0071 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00493\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00493\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00493\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00493\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00493\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00493\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00493\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00493\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0054 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00493\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00493\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00493\n",
            "Epoch 00024: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17566    23]\n",
            " [   18 11510]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985918879005392\n",
            "Averaged F1: 0.9985919404666145\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H9hE8qi7aUt"
      },
      "source": [
        "## Activation = Relu, Optimizer = SGD\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "8m 15s\n",
        "\n",
        "Accuracy: 0.9892159219699832\n",
        "\n",
        "Averaged F1: 0.9892098788985679"
      ],
      "id": "6H9hE8qi7aUt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xuoh_F3VAO5Z",
        "outputId": "a80ce22d-0c14-43d6-ce7a-8ffc2273f213"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "xuoh_F3VAO5Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.2697 - accuracy: 0.9480 - val_loss: 0.1143 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.11432, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1114 - accuracy: 0.9695 - val_loss: 0.0937 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.11432 to 0.09370, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0959 - accuracy: 0.9749 - val_loss: 0.0821 - val_accuracy: 0.9793\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.09370 to 0.08214, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0864 - accuracy: 0.9776 - val_loss: 0.0744 - val_accuracy: 0.9823\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.08214 to 0.07439, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0804 - accuracy: 0.9793 - val_loss: 0.0695 - val_accuracy: 0.9825\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.07439 to 0.06954, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0757 - accuracy: 0.9800 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.06954 to 0.06544, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0727 - accuracy: 0.9803 - val_loss: 0.0628 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06544 to 0.06284, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0703 - accuracy: 0.9807 - val_loss: 0.0601 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.06284 to 0.06015, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0675 - accuracy: 0.9814 - val_loss: 0.0580 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.06015 to 0.05805, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0663 - accuracy: 0.9816 - val_loss: 0.0565 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.05805 to 0.05652, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0648 - accuracy: 0.9819 - val_loss: 0.0552 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.05652 to 0.05516, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0629 - accuracy: 0.9823 - val_loss: 0.0540 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.05516 to 0.05398, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0621 - accuracy: 0.9826 - val_loss: 0.0529 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.05398 to 0.05288, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0609 - accuracy: 0.9826 - val_loss: 0.0517 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.05288 to 0.05170, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0603 - accuracy: 0.9828 - val_loss: 0.0511 - val_accuracy: 0.9847\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.05170 to 0.05105, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0595 - accuracy: 0.9830 - val_loss: 0.0501 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.05105 to 0.05014, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0587 - accuracy: 0.9834 - val_loss: 0.0494 - val_accuracy: 0.9853\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.05014 to 0.04940, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0577 - accuracy: 0.9837 - val_loss: 0.0489 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.04940 to 0.04893, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0572 - accuracy: 0.9838 - val_loss: 0.0484 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.04893 to 0.04841, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.0476 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.04841 to 0.04760, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0556 - accuracy: 0.9839 - val_loss: 0.0471 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.04760 to 0.04705, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0549 - accuracy: 0.9842 - val_loss: 0.0465 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.04705 to 0.04650, saving model to dnn/best_weights.hdf5\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0550 - accuracy: 0.9842 - val_loss: 0.0462 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.04650 to 0.04623, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0547 - accuracy: 0.9842 - val_loss: 0.0455 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.04623 to 0.04555, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0538 - accuracy: 0.9847 - val_loss: 0.0450 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.04555 to 0.04505, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0532 - accuracy: 0.9848 - val_loss: 0.0446 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.04505 to 0.04457, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0528 - accuracy: 0.9846 - val_loss: 0.0441 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.04457 to 0.04414, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0523 - accuracy: 0.9847 - val_loss: 0.0437 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.04414 to 0.04367, saving model to dnn/best_weights.hdf5\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0516 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.04367 to 0.04323, saving model to dnn/best_weights.hdf5\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0508 - accuracy: 0.9851 - val_loss: 0.0428 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.04323 to 0.04283, saving model to dnn/best_weights.hdf5\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0508 - accuracy: 0.9853 - val_loss: 0.0424 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.04283 to 0.04243, saving model to dnn/best_weights.hdf5\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0506 - accuracy: 0.9851 - val_loss: 0.0421 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.04243 to 0.04209, saving model to dnn/best_weights.hdf5\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0501 - accuracy: 0.9853 - val_loss: 0.0416 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.04209 to 0.04164, saving model to dnn/best_weights.hdf5\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0499 - accuracy: 0.9855 - val_loss: 0.0413 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.04164 to 0.04130, saving model to dnn/best_weights.hdf5\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0497 - accuracy: 0.9853 - val_loss: 0.0410 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.04130 to 0.04098, saving model to dnn/best_weights.hdf5\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0487 - accuracy: 0.9857 - val_loss: 0.0406 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.04098 to 0.04060, saving model to dnn/best_weights.hdf5\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.0401 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.04060 to 0.04014, saving model to dnn/best_weights.hdf5\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0477 - accuracy: 0.9858 - val_loss: 0.0398 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.04014 to 0.03979, saving model to dnn/best_weights.hdf5\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0482 - accuracy: 0.9855 - val_loss: 0.0395 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.03979 to 0.03945, saving model to dnn/best_weights.hdf5\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0474 - accuracy: 0.9858 - val_loss: 0.0391 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.03945 to 0.03912, saving model to dnn/best_weights.hdf5\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0473 - accuracy: 0.9861 - val_loss: 0.0387 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.03912 to 0.03873, saving model to dnn/best_weights.hdf5\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9862 - val_loss: 0.0383 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.03873 to 0.03830, saving model to dnn/best_weights.hdf5\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.0382 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.03830 to 0.03818, saving model to dnn/best_weights.hdf5\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0458 - accuracy: 0.9861 - val_loss: 0.0383 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03818\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.0377 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.03818 to 0.03769, saving model to dnn/best_weights.hdf5\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0453 - accuracy: 0.9863 - val_loss: 0.0376 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.03769 to 0.03760, saving model to dnn/best_weights.hdf5\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0454 - accuracy: 0.9861 - val_loss: 0.0370 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.03760 to 0.03705, saving model to dnn/best_weights.hdf5\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0452 - accuracy: 0.9863 - val_loss: 0.0364 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.03705 to 0.03637, saving model to dnn/best_weights.hdf5\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0445 - accuracy: 0.9864 - val_loss: 0.0369 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03637\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0440 - accuracy: 0.9866 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.03637 to 0.03589, saving model to dnn/best_weights.hdf5\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0437 - accuracy: 0.9865 - val_loss: 0.0361 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03589\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.0357 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.03589 to 0.03572, saving model to dnn/best_weights.hdf5\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0434 - accuracy: 0.9865 - val_loss: 0.0354 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.03572 to 0.03542, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00053: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.2520 - accuracy: 0.9416 - val_loss: 0.1128 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03542\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1107 - accuracy: 0.9722 - val_loss: 0.0925 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03542\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0952 - accuracy: 0.9761 - val_loss: 0.0806 - val_accuracy: 0.9798\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03542\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0853 - accuracy: 0.9781 - val_loss: 0.0728 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03542\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0784 - accuracy: 0.9798 - val_loss: 0.0679 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03542\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0744 - accuracy: 0.9804 - val_loss: 0.0642 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03542\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0710 - accuracy: 0.9809 - val_loss: 0.0616 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03542\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0688 - accuracy: 0.9813 - val_loss: 0.0590 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03542\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0668 - accuracy: 0.9816 - val_loss: 0.0577 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03542\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0650 - accuracy: 0.9816 - val_loss: 0.0558 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03542\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0632 - accuracy: 0.9821 - val_loss: 0.0544 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03542\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0623 - accuracy: 0.9825 - val_loss: 0.0533 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03542\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0614 - accuracy: 0.9826 - val_loss: 0.0519 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.03542\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0607 - accuracy: 0.9826 - val_loss: 0.0510 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03542\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0597 - accuracy: 0.9831 - val_loss: 0.0503 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03542\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0585 - accuracy: 0.9834 - val_loss: 0.0499 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03542\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0581 - accuracy: 0.9837 - val_loss: 0.0488 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03542\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0566 - accuracy: 0.9839 - val_loss: 0.0483 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03542\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0562 - accuracy: 0.9840 - val_loss: 0.0474 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03542\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0555 - accuracy: 0.9840 - val_loss: 0.0467 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03542\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0551 - accuracy: 0.9841 - val_loss: 0.0462 - val_accuracy: 0.9865\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03542\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0543 - accuracy: 0.9844 - val_loss: 0.0456 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03542\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0534 - accuracy: 0.9849 - val_loss: 0.0451 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03542\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0534 - accuracy: 0.9848 - val_loss: 0.0446 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03542\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0528 - accuracy: 0.9847 - val_loss: 0.0440 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03542\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0517 - accuracy: 0.9852 - val_loss: 0.0436 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03542\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0514 - accuracy: 0.9851 - val_loss: 0.0430 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03542\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0510 - accuracy: 0.9851 - val_loss: 0.0429 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03542\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0509 - accuracy: 0.9853 - val_loss: 0.0421 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03542\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0499 - accuracy: 0.9855 - val_loss: 0.0417 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03542\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0496 - accuracy: 0.9854 - val_loss: 0.0413 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03542\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0491 - accuracy: 0.9857 - val_loss: 0.0409 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03542\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0487 - accuracy: 0.9853 - val_loss: 0.0404 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03542\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0483 - accuracy: 0.9860 - val_loss: 0.0406 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.03542\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0481 - accuracy: 0.9856 - val_loss: 0.0397 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.03542\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0475 - accuracy: 0.9855 - val_loss: 0.0398 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03542\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0474 - accuracy: 0.9857 - val_loss: 0.0390 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03542\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0465 - accuracy: 0.9859 - val_loss: 0.0386 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03542\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9860 - val_loss: 0.0381 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03542\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0458 - accuracy: 0.9860 - val_loss: 0.0379 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.03542\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0459 - accuracy: 0.9860 - val_loss: 0.0375 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03542\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0455 - accuracy: 0.9862 - val_loss: 0.0371 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03542\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.0368 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03542\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0443 - accuracy: 0.9863 - val_loss: 0.0366 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03542\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.0363 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03542\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.0361 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03542\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0440 - accuracy: 0.9860 - val_loss: 0.0357 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03542\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0431 - accuracy: 0.9866 - val_loss: 0.0354 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.03542 to 0.03542, saving model to dnn/best_weights.hdf5\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.0352 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.03542 to 0.03519, saving model to dnn/best_weights.hdf5\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0428 - accuracy: 0.9865 - val_loss: 0.0348 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.03519 to 0.03482, saving model to dnn/best_weights.hdf5\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0429 - accuracy: 0.9860 - val_loss: 0.0345 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.03482 to 0.03451, saving model to dnn/best_weights.hdf5\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0420 - accuracy: 0.9867 - val_loss: 0.0344 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.03451 to 0.03440, saving model to dnn/best_weights.hdf5\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.0343 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.03440 to 0.03432, saving model to dnn/best_weights.hdf5\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0417 - accuracy: 0.9871 - val_loss: 0.0343 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.03432 to 0.03431, saving model to dnn/best_weights.hdf5\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0416 - accuracy: 0.9870 - val_loss: 0.0336 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.03431 to 0.03361, saving model to dnn/best_weights.hdf5\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0413 - accuracy: 0.9867 - val_loss: 0.0333 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.03361 to 0.03329, saving model to dnn/best_weights.hdf5\n",
            "Epoch 57/200\n",
            "389/389 - 1s - loss: 0.0402 - accuracy: 0.9872 - val_loss: 0.0333 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.03329 to 0.03328, saving model to dnn/best_weights.hdf5\n",
            "Epoch 58/200\n",
            "389/389 - 1s - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.0332 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.03328 to 0.03316, saving model to dnn/best_weights.hdf5\n",
            "Epoch 59/200\n",
            "389/389 - 1s - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.0327 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.03316 to 0.03270, saving model to dnn/best_weights.hdf5\n",
            "Epoch 60/200\n",
            "389/389 - 1s - loss: 0.0398 - accuracy: 0.9873 - val_loss: 0.0324 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.03270 to 0.03242, saving model to dnn/best_weights.hdf5\n",
            "Epoch 61/200\n",
            "389/389 - 1s - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.0321 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.03242 to 0.03215, saving model to dnn/best_weights.hdf5\n",
            "Epoch 62/200\n",
            "389/389 - 1s - loss: 0.0400 - accuracy: 0.9870 - val_loss: 0.0321 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.03215 to 0.03212, saving model to dnn/best_weights.hdf5\n",
            "Epoch 63/200\n",
            "389/389 - 1s - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0318 - val_accuracy: 0.9892\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.03212 to 0.03184, saving model to dnn/best_weights.hdf5\n",
            "Epoch 64/200\n",
            "389/389 - 1s - loss: 0.0394 - accuracy: 0.9871 - val_loss: 0.0329 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03184\n",
            "Epoch 65/200\n",
            "389/389 - 1s - loss: 0.0390 - accuracy: 0.9876 - val_loss: 0.0321 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03184\n",
            "Epoch 66/200\n",
            "389/389 - 1s - loss: 0.0388 - accuracy: 0.9876 - val_loss: 0.0314 - val_accuracy: 0.9892\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.03184 to 0.03136, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00066: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.2435 - accuracy: 0.9534 - val_loss: 0.1113 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03136\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1096 - accuracy: 0.9711 - val_loss: 0.0928 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03136\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0952 - accuracy: 0.9754 - val_loss: 0.0821 - val_accuracy: 0.9788\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03136\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0859 - accuracy: 0.9775 - val_loss: 0.0748 - val_accuracy: 0.9802\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03136\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0798 - accuracy: 0.9786 - val_loss: 0.0699 - val_accuracy: 0.9819\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03136\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0756 - accuracy: 0.9796 - val_loss: 0.0665 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03136\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0730 - accuracy: 0.9797 - val_loss: 0.0636 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03136\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0706 - accuracy: 0.9807 - val_loss: 0.0616 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03136\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0686 - accuracy: 0.9810 - val_loss: 0.0598 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03136\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0674 - accuracy: 0.9809 - val_loss: 0.0584 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03136\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0660 - accuracy: 0.9815 - val_loss: 0.0568 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03136\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0646 - accuracy: 0.9816 - val_loss: 0.0558 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03136\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0632 - accuracy: 0.9823 - val_loss: 0.0551 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.03136\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0626 - accuracy: 0.9821 - val_loss: 0.0539 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03136\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0618 - accuracy: 0.9827 - val_loss: 0.0532 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03136\n",
            "Epoch 16/200\n",
            "389/389 - 2s - loss: 0.0610 - accuracy: 0.9828 - val_loss: 0.0522 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03136\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0607 - accuracy: 0.9828 - val_loss: 0.0519 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03136\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0600 - accuracy: 0.9827 - val_loss: 0.0510 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03136\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0598 - accuracy: 0.9831 - val_loss: 0.0505 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03136\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0587 - accuracy: 0.9835 - val_loss: 0.0499 - val_accuracy: 0.9855\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03136\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0584 - accuracy: 0.9837 - val_loss: 0.0495 - val_accuracy: 0.9855\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03136\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0573 - accuracy: 0.9840 - val_loss: 0.0493 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03136\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0571 - accuracy: 0.9839 - val_loss: 0.0486 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03136\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0565 - accuracy: 0.9840 - val_loss: 0.0480 - val_accuracy: 0.9874\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03136\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0566 - accuracy: 0.9840 - val_loss: 0.0476 - val_accuracy: 0.9874\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03136\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0562 - accuracy: 0.9842 - val_loss: 0.0474 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03136\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0469 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03136\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.0464 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03136\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0548 - accuracy: 0.9846 - val_loss: 0.0460 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03136\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0545 - accuracy: 0.9846 - val_loss: 0.0457 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03136\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0534 - accuracy: 0.9850 - val_loss: 0.0455 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03136\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0532 - accuracy: 0.9850 - val_loss: 0.0451 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03136\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0532 - accuracy: 0.9852 - val_loss: 0.0446 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03136\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0524 - accuracy: 0.9854 - val_loss: 0.0444 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.03136\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0524 - accuracy: 0.9853 - val_loss: 0.0438 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.03136\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0522 - accuracy: 0.9849 - val_loss: 0.0436 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03136\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0520 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03136\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0513 - accuracy: 0.9855 - val_loss: 0.0430 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03136\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0513 - accuracy: 0.9855 - val_loss: 0.0428 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03136\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0503 - accuracy: 0.9858 - val_loss: 0.0421 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.03136\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0508 - accuracy: 0.9854 - val_loss: 0.0417 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03136\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0503 - accuracy: 0.9855 - val_loss: 0.0419 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03136\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0498 - accuracy: 0.9858 - val_loss: 0.0412 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03136\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0490 - accuracy: 0.9862 - val_loss: 0.0410 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03136\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0492 - accuracy: 0.9862 - val_loss: 0.0405 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03136\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0489 - accuracy: 0.9857 - val_loss: 0.0409 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03136\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0487 - accuracy: 0.9862 - val_loss: 0.0399 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03136\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0482 - accuracy: 0.9862 - val_loss: 0.0396 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.03136\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0479 - accuracy: 0.9861 - val_loss: 0.0398 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03136\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0475 - accuracy: 0.9863 - val_loss: 0.0391 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.03136\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0473 - accuracy: 0.9865 - val_loss: 0.0386 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03136\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0469 - accuracy: 0.9864 - val_loss: 0.0386 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03136\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9865 - val_loss: 0.0382 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03136\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9865 - val_loss: 0.0378 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03136\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0458 - accuracy: 0.9869 - val_loss: 0.0380 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03136\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0460 - accuracy: 0.9866 - val_loss: 0.0373 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03136\n",
            "Epoch 57/200\n",
            "389/389 - 1s - loss: 0.0457 - accuracy: 0.9865 - val_loss: 0.0372 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03136\n",
            "Epoch 58/200\n",
            "389/389 - 1s - loss: 0.0454 - accuracy: 0.9864 - val_loss: 0.0369 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03136\n",
            "Epoch 59/200\n",
            "389/389 - 1s - loss: 0.0448 - accuracy: 0.9868 - val_loss: 0.0365 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03136\n",
            "Epoch 60/200\n",
            "389/389 - 1s - loss: 0.0442 - accuracy: 0.9869 - val_loss: 0.0363 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03136\n",
            "Epoch 61/200\n",
            "389/389 - 1s - loss: 0.0442 - accuracy: 0.9868 - val_loss: 0.0362 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03136\n",
            "Epoch 62/200\n",
            "389/389 - 1s - loss: 0.0448 - accuracy: 0.9870 - val_loss: 0.0361 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03136\n",
            "Epoch 63/200\n",
            "389/389 - 1s - loss: 0.0435 - accuracy: 0.9869 - val_loss: 0.0353 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03136\n",
            "Epoch 64/200\n",
            "389/389 - 1s - loss: 0.0438 - accuracy: 0.9871 - val_loss: 0.0350 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03136\n",
            "Epoch 65/200\n",
            "389/389 - 1s - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.0356 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03136\n",
            "Epoch 66/200\n",
            "389/389 - 1s - loss: 0.0433 - accuracy: 0.9870 - val_loss: 0.0348 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03136\n",
            "Epoch 67/200\n",
            "389/389 - 1s - loss: 0.0422 - accuracy: 0.9877 - val_loss: 0.0343 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03136\n",
            "Epoch 68/200\n",
            "389/389 - 1s - loss: 0.0428 - accuracy: 0.9872 - val_loss: 0.0342 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03136\n",
            "Epoch 69/200\n",
            "389/389 - 1s - loss: 0.0421 - accuracy: 0.9876 - val_loss: 0.0337 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03136\n",
            "Epoch 70/200\n",
            "389/389 - 1s - loss: 0.0417 - accuracy: 0.9875 - val_loss: 0.0335 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03136\n",
            "Epoch 71/200\n",
            "389/389 - 1s - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0333 - val_accuracy: 0.9894\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03136\n",
            "Epoch 72/200\n",
            "389/389 - 1s - loss: 0.0419 - accuracy: 0.9873 - val_loss: 0.0338 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03136\n",
            "Epoch 73/200\n",
            "389/389 - 1s - loss: 0.0415 - accuracy: 0.9875 - val_loss: 0.0328 - val_accuracy: 0.9894\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03136\n",
            "Epoch 74/200\n",
            "389/389 - 1s - loss: 0.0409 - accuracy: 0.9873 - val_loss: 0.0328 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03136\n",
            "Epoch 75/200\n",
            "389/389 - 1s - loss: 0.0408 - accuracy: 0.9877 - val_loss: 0.0327 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03136\n",
            "Epoch 76/200\n",
            "389/389 - 1s - loss: 0.0403 - accuracy: 0.9877 - val_loss: 0.0329 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03136\n",
            "Epoch 00076: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.2375 - accuracy: 0.9521 - val_loss: 0.1102 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03136\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1082 - accuracy: 0.9724 - val_loss: 0.0923 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03136\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0945 - accuracy: 0.9763 - val_loss: 0.0820 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03136\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0854 - accuracy: 0.9782 - val_loss: 0.0753 - val_accuracy: 0.9802\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03136\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0796 - accuracy: 0.9789 - val_loss: 0.0702 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03136\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0757 - accuracy: 0.9794 - val_loss: 0.0668 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03136\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0726 - accuracy: 0.9802 - val_loss: 0.0638 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03136\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0704 - accuracy: 0.9806 - val_loss: 0.0619 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03136\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0686 - accuracy: 0.9812 - val_loss: 0.0596 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03136\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0667 - accuracy: 0.9813 - val_loss: 0.0579 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03136\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0655 - accuracy: 0.9815 - val_loss: 0.0567 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03136\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0645 - accuracy: 0.9819 - val_loss: 0.0557 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03136\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0633 - accuracy: 0.9820 - val_loss: 0.0544 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.03136\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0622 - accuracy: 0.9825 - val_loss: 0.0539 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03136\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0617 - accuracy: 0.9826 - val_loss: 0.0528 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03136\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0603 - accuracy: 0.9829 - val_loss: 0.0523 - val_accuracy: 0.9847\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03136\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0604 - accuracy: 0.9825 - val_loss: 0.0515 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03136\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0596 - accuracy: 0.9834 - val_loss: 0.0507 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03136\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0591 - accuracy: 0.9834 - val_loss: 0.0502 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03136\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0585 - accuracy: 0.9835 - val_loss: 0.0497 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03136\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0576 - accuracy: 0.9837 - val_loss: 0.0492 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03136\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0566 - accuracy: 0.9839 - val_loss: 0.0489 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03136\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0564 - accuracy: 0.9841 - val_loss: 0.0481 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03136\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0560 - accuracy: 0.9844 - val_loss: 0.0476 - val_accuracy: 0.9861\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03136\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0560 - accuracy: 0.9840 - val_loss: 0.0473 - val_accuracy: 0.9858\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03136\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0549 - accuracy: 0.9845 - val_loss: 0.0471 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03136\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.0464 - val_accuracy: 0.9867\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03136\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0547 - accuracy: 0.9843 - val_loss: 0.0461 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03136\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0538 - accuracy: 0.9849 - val_loss: 0.0456 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03136\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0533 - accuracy: 0.9847 - val_loss: 0.0452 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03136\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0536 - accuracy: 0.9845 - val_loss: 0.0449 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03136\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0528 - accuracy: 0.9850 - val_loss: 0.0445 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03136\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0524 - accuracy: 0.9850 - val_loss: 0.0441 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03136\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0526 - accuracy: 0.9849 - val_loss: 0.0438 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.03136\n",
            "Epoch 35/200\n",
            "389/389 - 1s - loss: 0.0517 - accuracy: 0.9852 - val_loss: 0.0434 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.03136\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0514 - accuracy: 0.9854 - val_loss: 0.0432 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03136\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0432 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03136\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0509 - accuracy: 0.9854 - val_loss: 0.0426 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03136\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0504 - accuracy: 0.9855 - val_loss: 0.0422 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03136\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0496 - accuracy: 0.9856 - val_loss: 0.0417 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.03136\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0496 - accuracy: 0.9859 - val_loss: 0.0414 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03136\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0494 - accuracy: 0.9859 - val_loss: 0.0418 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03136\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0495 - accuracy: 0.9858 - val_loss: 0.0408 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03136\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0489 - accuracy: 0.9858 - val_loss: 0.0410 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03136\n",
            "Epoch 45/200\n",
            "389/389 - 1s - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.0404 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03136\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0481 - accuracy: 0.9860 - val_loss: 0.0401 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03136\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0479 - accuracy: 0.9862 - val_loss: 0.0398 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03136\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0477 - accuracy: 0.9862 - val_loss: 0.0394 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.03136\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0472 - accuracy: 0.9861 - val_loss: 0.0392 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03136\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0468 - accuracy: 0.9863 - val_loss: 0.0387 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.03136\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0467 - accuracy: 0.9863 - val_loss: 0.0386 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03136\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0464 - accuracy: 0.9863 - val_loss: 0.0382 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03136\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0460 - accuracy: 0.9866 - val_loss: 0.0383 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03136\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0457 - accuracy: 0.9867 - val_loss: 0.0381 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03136\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0450 - accuracy: 0.9867 - val_loss: 0.0371 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03136\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0455 - accuracy: 0.9862 - val_loss: 0.0382 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03136\n",
            "Epoch 57/200\n",
            "389/389 - 1s - loss: 0.0450 - accuracy: 0.9866 - val_loss: 0.0378 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03136\n",
            "Epoch 58/200\n",
            "389/389 - 1s - loss: 0.0448 - accuracy: 0.9867 - val_loss: 0.0368 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03136\n",
            "Epoch 59/200\n",
            "389/389 - 1s - loss: 0.0443 - accuracy: 0.9870 - val_loss: 0.0359 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03136\n",
            "Epoch 60/200\n",
            "389/389 - 1s - loss: 0.0446 - accuracy: 0.9867 - val_loss: 0.0359 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03136\n",
            "Epoch 61/200\n",
            "389/389 - 1s - loss: 0.0443 - accuracy: 0.9869 - val_loss: 0.0366 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03136\n",
            "Epoch 62/200\n",
            "389/389 - 1s - loss: 0.0434 - accuracy: 0.9869 - val_loss: 0.0359 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03136\n",
            "Epoch 63/200\n",
            "389/389 - 1s - loss: 0.0437 - accuracy: 0.9868 - val_loss: 0.0358 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03136\n",
            "Epoch 64/200\n",
            "389/389 - 1s - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.0358 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03136\n",
            "Epoch 00064: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.2499 - accuracy: 0.9430 - val_loss: 0.1104 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03136\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.1089 - accuracy: 0.9715 - val_loss: 0.0911 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03136\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0941 - accuracy: 0.9763 - val_loss: 0.0803 - val_accuracy: 0.9799\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03136\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0851 - accuracy: 0.9785 - val_loss: 0.0739 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03136\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0795 - accuracy: 0.9794 - val_loss: 0.0691 - val_accuracy: 0.9826\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03136\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0752 - accuracy: 0.9801 - val_loss: 0.0655 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03136\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0720 - accuracy: 0.9805 - val_loss: 0.0625 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03136\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0700 - accuracy: 0.9808 - val_loss: 0.0603 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03136\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0682 - accuracy: 0.9813 - val_loss: 0.0587 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03136\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0667 - accuracy: 0.9816 - val_loss: 0.0573 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03136\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0659 - accuracy: 0.9815 - val_loss: 0.0560 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03136\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0639 - accuracy: 0.9821 - val_loss: 0.0550 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03136\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0632 - accuracy: 0.9822 - val_loss: 0.0537 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.03136\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0620 - accuracy: 0.9825 - val_loss: 0.0530 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03136\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0611 - accuracy: 0.9826 - val_loss: 0.0522 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03136\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0606 - accuracy: 0.9829 - val_loss: 0.0515 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03136\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0599 - accuracy: 0.9834 - val_loss: 0.0509 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03136\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0591 - accuracy: 0.9832 - val_loss: 0.0504 - val_accuracy: 0.9847\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03136\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0589 - accuracy: 0.9834 - val_loss: 0.0499 - val_accuracy: 0.9858\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03136\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0586 - accuracy: 0.9837 - val_loss: 0.0491 - val_accuracy: 0.9853\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03136\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0579 - accuracy: 0.9836 - val_loss: 0.0487 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03136\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0567 - accuracy: 0.9842 - val_loss: 0.0482 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03136\n",
            "Epoch 23/200\n",
            "389/389 - 1s - loss: 0.0570 - accuracy: 0.9839 - val_loss: 0.0477 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03136\n",
            "Epoch 24/200\n",
            "389/389 - 1s - loss: 0.0559 - accuracy: 0.9841 - val_loss: 0.0474 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03136\n",
            "Epoch 25/200\n",
            "389/389 - 1s - loss: 0.0556 - accuracy: 0.9842 - val_loss: 0.0469 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03136\n",
            "Epoch 26/200\n",
            "389/389 - 1s - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.0464 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03136\n",
            "Epoch 27/200\n",
            "389/389 - 1s - loss: 0.0546 - accuracy: 0.9846 - val_loss: 0.0460 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03136\n",
            "Epoch 28/200\n",
            "389/389 - 1s - loss: 0.0543 - accuracy: 0.9845 - val_loss: 0.0456 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03136\n",
            "Epoch 29/200\n",
            "389/389 - 1s - loss: 0.0540 - accuracy: 0.9848 - val_loss: 0.0452 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03136\n",
            "Epoch 30/200\n",
            "389/389 - 1s - loss: 0.0537 - accuracy: 0.9846 - val_loss: 0.0451 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03136\n",
            "Epoch 31/200\n",
            "389/389 - 1s - loss: 0.0531 - accuracy: 0.9848 - val_loss: 0.0445 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03136\n",
            "Epoch 32/200\n",
            "389/389 - 1s - loss: 0.0527 - accuracy: 0.9852 - val_loss: 0.0441 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03136\n",
            "Epoch 33/200\n",
            "389/389 - 1s - loss: 0.0523 - accuracy: 0.9850 - val_loss: 0.0437 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03136\n",
            "Epoch 34/200\n",
            "389/389 - 1s - loss: 0.0520 - accuracy: 0.9851 - val_loss: 0.0435 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.03136\n",
            "Epoch 35/200\n",
            "389/389 - 2s - loss: 0.0519 - accuracy: 0.9850 - val_loss: 0.0431 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.03136\n",
            "Epoch 36/200\n",
            "389/389 - 1s - loss: 0.0513 - accuracy: 0.9854 - val_loss: 0.0427 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03136\n",
            "Epoch 37/200\n",
            "389/389 - 1s - loss: 0.0507 - accuracy: 0.9854 - val_loss: 0.0424 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03136\n",
            "Epoch 38/200\n",
            "389/389 - 1s - loss: 0.0508 - accuracy: 0.9853 - val_loss: 0.0423 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03136\n",
            "Epoch 39/200\n",
            "389/389 - 1s - loss: 0.0507 - accuracy: 0.9854 - val_loss: 0.0417 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03136\n",
            "Epoch 40/200\n",
            "389/389 - 1s - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0416 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.03136\n",
            "Epoch 41/200\n",
            "389/389 - 1s - loss: 0.0493 - accuracy: 0.9858 - val_loss: 0.0417 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03136\n",
            "Epoch 42/200\n",
            "389/389 - 1s - loss: 0.0492 - accuracy: 0.9859 - val_loss: 0.0416 - val_accuracy: 0.9880\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03136\n",
            "Epoch 43/200\n",
            "389/389 - 1s - loss: 0.0488 - accuracy: 0.9861 - val_loss: 0.0404 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03136\n",
            "Epoch 44/200\n",
            "389/389 - 1s - loss: 0.0488 - accuracy: 0.9861 - val_loss: 0.0414 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03136\n",
            "Epoch 45/200\n",
            "389/389 - 2s - loss: 0.0483 - accuracy: 0.9860 - val_loss: 0.0402 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03136\n",
            "Epoch 46/200\n",
            "389/389 - 1s - loss: 0.0482 - accuracy: 0.9861 - val_loss: 0.0401 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03136\n",
            "Epoch 47/200\n",
            "389/389 - 1s - loss: 0.0479 - accuracy: 0.9860 - val_loss: 0.0398 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03136\n",
            "Epoch 48/200\n",
            "389/389 - 1s - loss: 0.0478 - accuracy: 0.9861 - val_loss: 0.0392 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.03136\n",
            "Epoch 49/200\n",
            "389/389 - 1s - loss: 0.0474 - accuracy: 0.9863 - val_loss: 0.0385 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03136\n",
            "Epoch 50/200\n",
            "389/389 - 1s - loss: 0.0467 - accuracy: 0.9863 - val_loss: 0.0389 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.03136\n",
            "Epoch 51/200\n",
            "389/389 - 1s - loss: 0.0470 - accuracy: 0.9862 - val_loss: 0.0383 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03136\n",
            "Epoch 52/200\n",
            "389/389 - 1s - loss: 0.0469 - accuracy: 0.9864 - val_loss: 0.0380 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03136\n",
            "Epoch 53/200\n",
            "389/389 - 1s - loss: 0.0460 - accuracy: 0.9866 - val_loss: 0.0387 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03136\n",
            "Epoch 54/200\n",
            "389/389 - 1s - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.0372 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03136\n",
            "Epoch 55/200\n",
            "389/389 - 1s - loss: 0.0454 - accuracy: 0.9868 - val_loss: 0.0370 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03136\n",
            "Epoch 56/200\n",
            "389/389 - 1s - loss: 0.0451 - accuracy: 0.9866 - val_loss: 0.0366 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03136\n",
            "Epoch 57/200\n",
            "389/389 - 1s - loss: 0.0446 - accuracy: 0.9867 - val_loss: 0.0364 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03136\n",
            "Epoch 58/200\n",
            "389/389 - 1s - loss: 0.0449 - accuracy: 0.9865 - val_loss: 0.0372 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03136\n",
            "Epoch 59/200\n",
            "389/389 - 1s - loss: 0.0445 - accuracy: 0.9867 - val_loss: 0.0361 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03136\n",
            "Epoch 60/200\n",
            "389/389 - 1s - loss: 0.0440 - accuracy: 0.9871 - val_loss: 0.0359 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03136\n",
            "Epoch 61/200\n",
            "389/389 - 1s - loss: 0.0444 - accuracy: 0.9870 - val_loss: 0.0353 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03136\n",
            "Epoch 62/200\n",
            "389/389 - 1s - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.0355 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03136\n",
            "Epoch 63/200\n",
            "389/389 - 1s - loss: 0.0438 - accuracy: 0.9869 - val_loss: 0.0355 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03136\n",
            "Epoch 64/200\n",
            "389/389 - 1s - loss: 0.0432 - accuracy: 0.9870 - val_loss: 0.0348 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03136\n",
            "Epoch 65/200\n",
            "389/389 - 1s - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.0343 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03136\n",
            "Epoch 66/200\n",
            "389/389 - 1s - loss: 0.0431 - accuracy: 0.9872 - val_loss: 0.0346 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03136\n",
            "Epoch 67/200\n",
            "389/389 - 1s - loss: 0.0423 - accuracy: 0.9873 - val_loss: 0.0350 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03136\n",
            "Epoch 68/200\n",
            "389/389 - 1s - loss: 0.0427 - accuracy: 0.9872 - val_loss: 0.0347 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03136\n",
            "Epoch 69/200\n",
            "389/389 - 1s - loss: 0.0424 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03136\n",
            "Epoch 70/200\n",
            "389/389 - 1s - loss: 0.0415 - accuracy: 0.9873 - val_loss: 0.0332 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03136\n",
            "Epoch 71/200\n",
            "389/389 - 1s - loss: 0.0417 - accuracy: 0.9874 - val_loss: 0.0350 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03136\n",
            "Epoch 72/200\n",
            "389/389 - 1s - loss: 0.0419 - accuracy: 0.9873 - val_loss: 0.0332 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03136\n",
            "Epoch 73/200\n",
            "389/389 - 1s - loss: 0.0417 - accuracy: 0.9873 - val_loss: 0.0338 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03136\n",
            "Epoch 74/200\n",
            "389/389 - 1s - loss: 0.0414 - accuracy: 0.9877 - val_loss: 0.0352 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03136\n",
            "Epoch 00074: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17469   120]\n",
            " [  194 11334]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9892159219699832\n",
            "Averaged F1: 0.9892098788985679\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     17589\n",
            "           1       0.99      0.98      0.99     11528\n",
            "\n",
            "    accuracy                           0.99     29117\n",
            "   macro avg       0.99      0.99      0.99     29117\n",
            "weighted avg       0.99      0.99      0.99     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnuNL7Y9DN0S"
      },
      "source": [
        "## Testing on differnet numbers of layer and neuron"
      ],
      "id": "xnuNL7Y9DN0S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBtxzyWI_NJl"
      },
      "source": [
        "## Activation = Relu, Optimizer = adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 10 neurons\n",
        "\n",
        "Accuracy: 0.9987292646907305\n",
        "\n",
        "Average F1 score: 0.998729140952868\n",
        "\n",
        "Time: 1m 36s\n"
      ],
      "id": "dBtxzyWI_NJl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idpOjEIhAS-w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16dae523-b05f-4651-979a-16d43467e82b"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D( 32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "idpOjEIhAS-w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0547 - accuracy: 0.9834 - val_loss: 0.0275 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02752, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0106 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02752 to 0.01061, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.0086 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01061 to 0.00864, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.0095 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00864\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0072 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00864 to 0.00717, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0072 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00717 to 0.00716, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00716 to 0.00606, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00606\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00606 to 0.00581, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0065 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00581\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00581\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00581\n",
            "Epoch 00012: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0545 - accuracy: 0.9838 - val_loss: 0.0276 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00581\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.0123 - val_accuracy: 0.9957\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00581\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0106 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00581\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0079 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00581\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00581\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00581\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00581\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00581\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00581 to 0.00544, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0055 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00544\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00544 to 0.00509, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0058 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00509\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0049 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00509 to 0.00491, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00491\n",
            "Epoch 00014: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0535 - accuracy: 0.9841 - val_loss: 0.0247 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00491\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0102 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00491\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.0067 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00491\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0064 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00491\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00491\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00491\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0053 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00491\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00491\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00491 to 0.00485, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0062 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00485\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00485\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00485\n",
            "Epoch 00012: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0553 - accuracy: 0.9831 - val_loss: 0.0244 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00485\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0221 - accuracy: 0.9926 - val_loss: 0.0092 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00485\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0086 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00485\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.0072 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00485\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00485\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00485\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00485\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00485\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00485\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0058 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00485\n",
            "Epoch 00010: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.0305 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00485\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.0106 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00485\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0084 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00485\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00485\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00485\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00485\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00485\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00485\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00485\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0087 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00485\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0070 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00485\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00485\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00485\n",
            "Epoch 00013: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17577    12]\n",
            " [   25 11503]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987292646907305\n",
            "Averaged F1: 0.998729140952868\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBUHOxaiDKwr"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 500 neurons\n",
        "\n",
        "Accuracy: 0.9987636088882783\n",
        "\n",
        "Average Score F1: 0.9987636273604831\n",
        "\n",
        "Time: 1m 38s"
      ],
      "id": "YBUHOxaiDKwr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkYfr7sIFVDn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06715ac5-e6d0-4c2a-e788-ab6fad3adf04"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "fkYfr7sIFVDn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0562 - accuracy: 0.9837 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02937, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.0143 - val_accuracy: 0.9953\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02937 to 0.01434, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0107 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01434 to 0.01068, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.0085 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01068 to 0.00853, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0073 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00853 to 0.00731, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0067 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00731 to 0.00666, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0060 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00666 to 0.00597, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0079 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00597\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00597 to 0.00560, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0067 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00560\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00560\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00560\n",
            "Epoch 00012: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0559 - accuracy: 0.9836 - val_loss: 0.0302 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00560\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.0099 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00560\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.0077 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00560\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0069 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00560\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00560\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00560\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00560 to 0.00534, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00534\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0057 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00534\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00534 to 0.00501, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00501\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0058 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00501\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0050 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00501\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0050 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00501\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00501\n",
            "Epoch 00015: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0549 - accuracy: 0.9837 - val_loss: 0.0298 - val_accuracy: 0.9901\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00501\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.0127 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00501\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.0083 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00501\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.0073 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00501\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9966 - val_loss: 0.0076 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00501\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00501\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00501\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0061 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00501\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0071 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00501\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00501\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00501\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00501 to 0.00482, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00482\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00482 to 0.00441, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00441\n",
            "Epoch 00015: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0608 - accuracy: 0.9822 - val_loss: 0.0377 - val_accuracy: 0.9882\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00441\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.0185 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00441\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0106 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00441\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0142 - accuracy: 0.9957 - val_loss: 0.0085 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00441\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0093 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00441\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.0071 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00441\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.0078 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00441\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0063 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00441\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00441\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00441\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0068 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00441\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0067 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00441\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0056 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00441\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00441\n",
            "Epoch 00014: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0587 - accuracy: 0.9836 - val_loss: 0.0301 - val_accuracy: 0.9901\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00441\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.0144 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00441\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.0090 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00441\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.0082 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00441\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0076 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00441\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0068 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00441\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00441\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00441\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0063 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00441\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00441\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0055 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00441\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00441\n",
            "Epoch 00012: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17570    19]\n",
            " [   17 11511]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987636088882783\n",
            "Averaged F1: 0.9987636273604831\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEGNr0LZDcsw"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 10 neurons\n",
        "## Layer 1 : 10 neurons\n",
        "\n",
        "Accuracy: 0.9984888553078958\n",
        "\n",
        "Averaged F1: 0.9984889905269724\n",
        "\n",
        "Time: 1m 56s"
      ],
      "id": "cEGNr0LZDcsw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZGHE_rfFscT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b352abef-6738-4f13-f5c3-5c8da19cde7c"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "XZGHE_rfFscT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0962 - accuracy: 0.9702 - val_loss: 0.0468 - val_accuracy: 0.9872\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04684, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0491 - accuracy: 0.9849 - val_loss: 0.0355 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04684 to 0.03550, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0386 - accuracy: 0.9873 - val_loss: 0.0255 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03550 to 0.02553, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0301 - accuracy: 0.9897 - val_loss: 0.0168 - val_accuracy: 0.9941\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02553 to 0.01677, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.0113 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01677 to 0.01131, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0110 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01131 to 0.01096, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.0093 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01096 to 0.00929, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.0094 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00929\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0084 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00929 to 0.00836, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.0078 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00836 to 0.00780, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0073 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00780 to 0.00734, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0072 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00734 to 0.00724, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0073 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00724\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0067 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00724 to 0.00674, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00674\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0069 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00674\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0065 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00674 to 0.00654, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00654\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0059 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00654 to 0.00592, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00019: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0822 - accuracy: 0.9796 - val_loss: 0.0417 - val_accuracy: 0.9885\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00592\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0408 - accuracy: 0.9877 - val_loss: 0.0273 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00592\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0288 - accuracy: 0.9910 - val_loss: 0.0173 - val_accuracy: 0.9949\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00592\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.0123 - val_accuracy: 0.9963\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00592\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0173 - accuracy: 0.9947 - val_loss: 0.0118 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00592\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0141 - accuracy: 0.9956 - val_loss: 0.0092 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00592\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0094 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00592\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0078 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00592\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0079 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00592\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0082 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00592\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0080 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00592\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0075 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00592\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0079 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00592\n",
            "Epoch 00013: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.1211 - accuracy: 0.9650 - val_loss: 0.0463 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00592\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0471 - accuracy: 0.9865 - val_loss: 0.0356 - val_accuracy: 0.9905\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00592\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0399 - accuracy: 0.9879 - val_loss: 0.0292 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00592\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0334 - accuracy: 0.9895 - val_loss: 0.0236 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00592\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.0196 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00592\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0237 - accuracy: 0.9924 - val_loss: 0.0129 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00592\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.0106 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00592\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.0081 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00592\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.0090 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00592\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0130 - accuracy: 0.9960 - val_loss: 0.0078 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00592\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0126 - accuracy: 0.9964 - val_loss: 0.0077 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00592\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0085 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00592\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0070 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00592\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00592\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0073 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00592\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0064 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00592\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0058 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00592 to 0.00579, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "389/389 - 1s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00579\n",
            "Epoch 19/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00579 to 0.00549, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "389/389 - 1s - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0067 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00549\n",
            "Epoch 21/200\n",
            "389/389 - 1s - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0055 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.00549 to 0.00547, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00547 to 0.00543, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00022: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0876 - accuracy: 0.9777 - val_loss: 0.0447 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00543\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0475 - accuracy: 0.9851 - val_loss: 0.0436 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00543\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0378 - accuracy: 0.9867 - val_loss: 0.0255 - val_accuracy: 0.9890\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00543\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.0154 - val_accuracy: 0.9946\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00543\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.0127 - val_accuracy: 0.9959\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00543\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.0114 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00543\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0162 - accuracy: 0.9947 - val_loss: 0.0121 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00543\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0096 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00543\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.0097 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00543\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.0084 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00543\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00543\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00543\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00543\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00543\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0069 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00543\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0066 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00543\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00543\n",
            "Epoch 00017: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0837 - accuracy: 0.9796 - val_loss: 0.0407 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00543\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0419 - accuracy: 0.9868 - val_loss: 0.0308 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00543\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.0226 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00543\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0257 - accuracy: 0.9913 - val_loss: 0.0175 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00543\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.0133 - val_accuracy: 0.9957\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00543\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0107 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00543\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.0111 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00543\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.0089 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00543\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0097 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00543\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.0089 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00543\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0079 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00543\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0074 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00543\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0070 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00543\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0066 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00543\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0074 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00543\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00543\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0074 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00543\n",
            "Epoch 00017: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17561    28]\n",
            " [   16 11512]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9984888553078958\n",
            "Averaged F1: 0.9984889905269724\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9HF_siCDxya"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 500 neurons\n",
        "## Layer 1 : 500 neurons\n",
        "\n",
        "Accuracy: 0.9985918879005392\n",
        "\n",
        "Averaged F1: 0.9985918984213269\n",
        "\n",
        "Time: 1m 26s\n"
      ],
      "id": "_9HF_siCDxya"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAfz6AjQF7tE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15d8fcff-7191-4b9d-9dc6-6336cfb9bd99"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "HAfz6AjQF7tE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0540 - accuracy: 0.9838 - val_loss: 0.0241 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02413, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0093 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02413 to 0.00927, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0078 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00927 to 0.00780, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.0073 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00780 to 0.00733, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0108 - accuracy: 0.9965 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00733 to 0.00694, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.0064 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00694 to 0.00644, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00644\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00644 to 0.00624, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0052 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00624 to 0.00518, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00518 to 0.00509, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0056 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00509\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00509 to 0.00489, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00489\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00489\n",
            "Epoch 00014: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0526 - accuracy: 0.9845 - val_loss: 0.0249 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00489\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0108 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00489\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00489\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0072 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00489\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00489\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00489\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00489\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0061 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00489\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0061 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00489\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0060 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00489\n",
            "Epoch 00010: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0544 - accuracy: 0.9840 - val_loss: 0.0364 - val_accuracy: 0.9865\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00489\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.0105 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00489\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.0093 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00489\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0069 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00489\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00489\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0058 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00489\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0074 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00489\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0054 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00489\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00489\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00489\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00489\n",
            "Epoch 00011: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.0305 - val_accuracy: 0.9901\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00489\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0113 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00489\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.0082 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00489\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00489\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.0086 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00489\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0071 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00489\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00489\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00489\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0058 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00489\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0082 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00489\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0074 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00489\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00489\n",
            "Epoch 00012: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.0242 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00489\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0089 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00489\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0088 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00489\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00489\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00489\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00489\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00489\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00489\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0051 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00489\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00489\n",
            "Epoch 00010: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17568    21]\n",
            " [   20 11508]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985918879005392\n",
            "Averaged F1: 0.9985918984213269\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXo4ezW5D3Oc"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.998626232098087 \n",
        "\n",
        "Averaged F1: 0.9986261910044079\n",
        "\n",
        "Time: 1m 42s"
      ],
      "id": "AXo4ezW5D3Oc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRfKPPcyMOJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15f85f78-a1ff-4385-d50d-7af02e2d9b3f"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "NRfKPPcyMOJS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0524 - accuracy: 0.9841 - val_loss: 0.0283 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02828, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0107 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02828 to 0.01072, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0085 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01072 to 0.00845, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0074 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00845 to 0.00745, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0061 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00745 to 0.00610, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00610 to 0.00569, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00569 to 0.00532, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00532\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0072 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00532\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0047 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00532 to 0.00473, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0060 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00473\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00473\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00473 to 0.00459, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00459 to 0.00454, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00454\n",
            "Epoch 00015: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0500 - accuracy: 0.9839 - val_loss: 0.0167 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00454\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.0109 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00454\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0073 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00454\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0066 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00454\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.0066 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00454\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00454\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00454\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0053 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00454\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00454\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00454\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00454\n",
            "Epoch 00011: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0505 - accuracy: 0.9841 - val_loss: 0.0246 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00454\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0089 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00454\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.0085 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00454\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00454\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00454\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00454\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0058 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00454\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00454\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0057 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00454\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00454\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00454\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00454\n",
            "Epoch 00012: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0525 - accuracy: 0.9840 - val_loss: 0.0225 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00454\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.0092 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00454\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0081 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00454\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0122 - accuracy: 0.9963 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00454\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0066 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00454\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.0064 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00454\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0070 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00454\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00454\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00454\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00454\n",
            "Epoch 00010: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0493 - accuracy: 0.9850 - val_loss: 0.0219 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00454\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.0113 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00454\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.0089 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00454\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00454\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0079 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00454\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00454\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00454\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00454\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00454\n",
            "Epoch 00009: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17571    18]\n",
            " [   22 11506]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.998626232098087\n",
            "Averaged F1: 0.9986261910044079\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9IQLFrID3tr"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 128  neurons, size = 3\n",
        "## Kernel 2: 64  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9989353298760174\n",
        "\n",
        "Averaged F1: 0.9989353059946118\n",
        "\n",
        "Time: 1m 54s"
      ],
      "id": "W9IQLFrID3tr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GOY-KNnMTnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc3c2b1d-3086-46c6-c9e5-1b1deac2e536"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (128, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "3GOY-KNnMTnj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0519 - accuracy: 0.9853 - val_loss: 0.0226 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02261, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.0109 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02261 to 0.01093, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01093 to 0.00621, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00621\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00621 to 0.00580, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0048 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00580 to 0.00482, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00482\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0050 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00482\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00482 to 0.00467, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00467 to 0.00441, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0040 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00441 to 0.00402, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00011: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0542 - accuracy: 0.9834 - val_loss: 0.0286 - val_accuracy: 0.9905\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00402\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0090 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00402\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0074 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00402\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0085 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00402\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0061 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00402\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0058 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00402\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00402\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00402\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00402\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00402\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00402\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00402 to 0.00394, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00394 to 0.00392, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00013: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0509 - accuracy: 0.9847 - val_loss: 0.0254 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00392\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0219 - accuracy: 0.9928 - val_loss: 0.0090 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00392\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00392\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00392\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00392\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0069 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00392\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0065 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00392\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00392\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00392\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0046 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00392\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00392\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00392\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0042 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00392\n",
            "Epoch 00013: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0488 - accuracy: 0.9850 - val_loss: 0.0237 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00392\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0208 - accuracy: 0.9933 - val_loss: 0.0088 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00392\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0070 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00392\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0073 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00392\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00392\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00392\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0056 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00392\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0062 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00392\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00392\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9981 - val_loss: 0.0048 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00392\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00392\n",
            "Epoch 00011: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0522 - accuracy: 0.9843 - val_loss: 0.0263 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00392\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0195 - accuracy: 0.9936 - val_loss: 0.0085 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00392\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.0075 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00392\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00392\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0064 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00392\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00392\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00392\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00392\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0057 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00392\n",
            "Epoch 00009: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17575    14]\n",
            " [   17 11511]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9989353298760174\n",
            "Averaged F1: 0.9989353059946118\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZB0mMCMrCz"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 500  neurons, size = 3\n",
        "## Kernel 2: 250  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9991413950613044\n",
        "\n",
        "Averaged F1: 0.9991413500908423\n",
        "\n",
        "Time: 7m 51s"
      ],
      "id": "YnZB0mMCMrCz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc9zcugbMhAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "991fc815-fabe-4c8a-aa4d-5f43aa18d4b6"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (500, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(250, (1, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "bc9zcugbMhAi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 7s - loss: 0.0414 - accuracy: 0.9872 - val_loss: 0.0230 - val_accuracy: 0.9948\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02304, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 7s - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02304 to 0.00693, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 7s - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0080 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00693\n",
            "Epoch 4/200\n",
            "389/389 - 7s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0072 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00693\n",
            "Epoch 5/200\n",
            "389/389 - 7s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0065 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00693 to 0.00653, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 7s - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0059 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00653 to 0.00594, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 7s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0046 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00594 to 0.00460, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 7s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0046 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00460 to 0.00458, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "389/389 - 7s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0037 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00458 to 0.00369, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 7s - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0052 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00369\n",
            "Epoch 11/200\n",
            "389/389 - 7s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0043 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00369\n",
            "Epoch 12/200\n",
            "389/389 - 7s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0051 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00369\n",
            "Epoch 00012: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 7s - loss: 0.0420 - accuracy: 0.9869 - val_loss: 0.0162 - val_accuracy: 0.9953\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00369\n",
            "Epoch 2/200\n",
            "389/389 - 7s - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.0108 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00369\n",
            "Epoch 3/200\n",
            "389/389 - 7s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00369\n",
            "Epoch 4/200\n",
            "389/389 - 7s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00369\n",
            "Epoch 5/200\n",
            "389/389 - 7s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00369\n",
            "Epoch 6/200\n",
            "389/389 - 7s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00369\n",
            "Epoch 7/200\n",
            "389/389 - 7s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00369\n",
            "Epoch 8/200\n",
            "389/389 - 7s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00369\n",
            "Epoch 9/200\n",
            "389/389 - 7s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00369\n",
            "Epoch 10/200\n",
            "389/389 - 7s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0071 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00369\n",
            "Epoch 00010: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 8s - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.0112 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00369\n",
            "Epoch 2/200\n",
            "389/389 - 7s - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.0080 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00369\n",
            "Epoch 3/200\n",
            "389/389 - 7s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00369\n",
            "Epoch 4/200\n",
            "389/389 - 7s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00369\n",
            "Epoch 5/200\n",
            "389/389 - 7s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0060 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00369\n",
            "Epoch 6/200\n",
            "389/389 - 7s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00369\n",
            "Epoch 7/200\n",
            "389/389 - 7s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00369\n",
            "Epoch 8/200\n",
            "389/389 - 7s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00369\n",
            "Epoch 9/200\n",
            "389/389 - 7s - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0040 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00369\n",
            "Epoch 10/200\n",
            "389/389 - 7s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0038 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00369\n",
            "Epoch 11/200\n",
            "389/389 - 7s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00369\n",
            "Epoch 12/200\n",
            "389/389 - 7s - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00369\n",
            "Epoch 13/200\n",
            "389/389 - 7s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00369\n",
            "Epoch 14/200\n",
            "389/389 - 7s - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0034 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00369 to 0.00341, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00014: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 7s - loss: 0.0387 - accuracy: 0.9875 - val_loss: 0.0111 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00341\n",
            "Epoch 2/200\n",
            "389/389 - 7s - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0070 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00341\n",
            "Epoch 3/200\n",
            "389/389 - 7s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0080 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00341\n",
            "Epoch 4/200\n",
            "389/389 - 7s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0063 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00341\n",
            "Epoch 5/200\n",
            "389/389 - 7s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0052 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00341\n",
            "Epoch 6/200\n",
            "389/389 - 7s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0046 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00341\n",
            "Epoch 7/200\n",
            "389/389 - 7s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0050 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00341\n",
            "Epoch 8/200\n",
            "389/389 - 7s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0046 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00341\n",
            "Epoch 9/200\n",
            "389/389 - 7s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00341\n",
            "Epoch 10/200\n",
            "389/389 - 7s - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00341\n",
            "Epoch 00010: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 7s - loss: 0.0436 - accuracy: 0.9863 - val_loss: 0.0155 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00341\n",
            "Epoch 2/200\n",
            "389/389 - 7s - loss: 0.0121 - accuracy: 0.9962 - val_loss: 0.0087 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00341\n",
            "Epoch 3/200\n",
            "389/389 - 7s - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0060 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00341\n",
            "Epoch 4/200\n",
            "389/389 - 7s - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0051 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00341\n",
            "Epoch 5/200\n",
            "389/389 - 7s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00341\n",
            "Epoch 6/200\n",
            "389/389 - 7s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00341\n",
            "Epoch 7/200\n",
            "389/389 - 7s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0048 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00341\n",
            "Epoch 8/200\n",
            "389/389 - 7s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0044 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00341\n",
            "Epoch 9/200\n",
            "389/389 - 7s - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00341\n",
            "Epoch 10/200\n",
            "389/389 - 7s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00341 to 0.00337, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 7s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0037 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00337\n",
            "Epoch 12/200\n",
            "389/389 - 7s - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00337\n",
            "Epoch 13/200\n",
            "389/389 - 7s - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0035 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00337\n",
            "Epoch 14/200\n",
            "389/389 - 7s - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0035 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00337\n",
            "Epoch 15/200\n",
            "389/389 - 7s - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00337\n",
            "Epoch 00015: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17578    11]\n",
            " [   19 11509]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9989696740735653\n",
            "Averaged F1: 0.9989696123887016\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODwJvXkhMypN"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32   neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9986949204931826\n",
        "\n",
        "Averaged F1: 0.9986949204931826\n",
        "\n",
        "Time: 1m 16s"
      ],
      "id": "ODwJvXkhMypN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9K2sOJhMj9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88cb002e-fdc2-4e87-b1c4-c723c503cc4f"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (32, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "T9K2sOJhMj9d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0468 - accuracy: 0.9862 - val_loss: 0.0160 - val_accuracy: 0.9954\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01602, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.0089 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01602 to 0.00891, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0070 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00891 to 0.00700, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0063 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00700 to 0.00629, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0065 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00629\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00629 to 0.00572, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0063 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00572\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0059 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00572\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0054 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00572 to 0.00540, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0057 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00540\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00540 to 0.00519, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00011: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0462 - accuracy: 0.9868 - val_loss: 0.0216 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00519\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.0101 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00519\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.0083 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00519\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0070 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00519\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9975 - val_loss: 0.0084 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00519\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00519\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0060 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00519\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00519\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00519\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00519\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00519 to 0.00466, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00466\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00466\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0049 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00466\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0051 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00466\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00466\n",
            "Epoch 00016: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0438 - accuracy: 0.9861 - val_loss: 0.0194 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00466\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.0095 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00466\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0064 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00466\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0067 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00466\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0072 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00466\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0078 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00466\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00466\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00466\n",
            "Epoch 00008: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0429 - accuracy: 0.9868 - val_loss: 0.0161 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00466\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.0084 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00466\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0083 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00466\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0065 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00466\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0057 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00466\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0075 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00466\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00466\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0059 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00466\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00466\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0064 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00466\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00466\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0079 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00466\n",
            "Epoch 00012: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.0170 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00466\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.0086 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00466\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00466\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0063 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00466\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00466\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0055 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00466\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0062 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00466\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0059 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00466\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0057 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00466\n",
            "Epoch 00009: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17570    19]\n",
            " [   19 11509]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9986949204931826\n",
            "Averaged F1: 0.9986949204931826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pb16_BNOrN"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 128  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9987979530858262\n",
        "\n",
        "Averaged F1: 0.9987979800196098\n",
        "\n",
        "Time: 1m 57s"
      ],
      "id": "T1pb16_BNOrN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mm6bC5iMjxq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8bc2893-db3e-4ee6-cd4d-ffbf983cc783"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (128, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "8mm6bC5iMjxq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0380 - accuracy: 0.9882 - val_loss: 0.0127 - val_accuracy: 0.9958\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01272, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01272 to 0.00738, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.0059 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00738 to 0.00587, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0060 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00587\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00587 to 0.00532, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0062 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00532\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00532\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00532 to 0.00513, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00008: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0382 - accuracy: 0.9878 - val_loss: 0.0144 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00513\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.0086 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00513\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.0065 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00513\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0063 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00513\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00513\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00513 to 0.00448, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00448\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00448\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00448\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0050 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00448\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0043 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00448 to 0.00429, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00011: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0376 - accuracy: 0.9884 - val_loss: 0.0137 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00429\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0121 - accuracy: 0.9966 - val_loss: 0.0084 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00429\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0064 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00429\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00429\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0061 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00429\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.0057 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00429\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00429\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00429\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00429\n",
            "Epoch 00009: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0185 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00429\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0137 - accuracy: 0.9959 - val_loss: 0.0084 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00429\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00429\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00429\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00429\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0052 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00429\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00429\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00429\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0072 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00429\n",
            "Epoch 00009: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0395 - accuracy: 0.9873 - val_loss: 0.0152 - val_accuracy: 0.9950\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00429\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0137 - accuracy: 0.9960 - val_loss: 0.0079 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00429\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0078 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00429\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0086 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00429\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00429\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0056 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00429\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00429\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00429\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00429\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00429\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0050 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00429\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0045 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00429\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00429\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00429\n",
            "Epoch 00014: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17570    19]\n",
            " [   16 11512]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987979530858262\n",
            "Averaged F1: 0.9987979800196098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHqKg2NNVhu"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 500  neurons, size = 3\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9989353298760174\n",
        "\n",
        "Averaged F1: 0.9989352900593563\n",
        "\n",
        "Time: 2m 11s\n"
      ],
      "id": "DzHqKg2NNVhu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFoBDiJLMjij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fce85d96-9e3c-4d30-f8e6-afb261a6ac57"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D (128, kernel_size=(1, 3), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "hFoBDiJLMjij",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.0152 - val_accuracy: 0.9955\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01516, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.0080 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01516 to 0.00803, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0076 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00803 to 0.00765, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00765 to 0.00576, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0058 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00576 to 0.00576, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0050 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00576 to 0.00496, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0066 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00496\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0063 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00496\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0058 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00496\n",
            "Epoch 00009: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0388 - accuracy: 0.9874 - val_loss: 0.0150 - val_accuracy: 0.9952\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00496\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.0091 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00496\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00496\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0063 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00496\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00496 to 0.00474, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00474\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00474 to 0.00462, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00462\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00462\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00462\n",
            "Epoch 00010: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0402 - accuracy: 0.9866 - val_loss: 0.0163 - val_accuracy: 0.9945\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00462\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.0100 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00462\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.0065 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00462\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0064 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00462\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00462\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0066 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00462\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0052 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00462\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0060 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00462\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0056 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00462\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00462\n",
            "Epoch 00010: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0404 - accuracy: 0.9869 - val_loss: 0.0192 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00462\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.0090 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00462\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.0093 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00462\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0086 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00462\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00462\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0065 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00462\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00462\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00462\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00462\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0063 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00462\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00462\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0071 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00462\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0052 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00462\n",
            "Epoch 00013: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 3s - loss: 0.0398 - accuracy: 0.9882 - val_loss: 0.0167 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00462\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.0080 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00462\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0074 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00462\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0094 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00462\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00462\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00462\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00462\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00462\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0046 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00462 to 0.00458, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0045 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00458 to 0.00448, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00448\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00448\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0054 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00448\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0040 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00448 to 0.00399, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 2s - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00399 to 0.00398, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00015: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17576    13]\n",
            " [   18 11510]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9989353298760174\n",
            "Averaged F1: 0.9989352900593563\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rswofQgvOEe-"
      },
      "source": [
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 2\n",
        "## Kernel 2: 64  neurons, size = 2\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9984888553078958\n",
        "\n",
        "Averaged F1: 0.9984888553078958\n",
        "\n",
        "Time: 1m 56s"
      ],
      "id": "rswofQgvOEe-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjawDBINuo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19cc7636-bf48-4f0a-95b2-3e4d3d72807b"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 2), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 2), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "6TjawDBINuo8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0598 - accuracy: 0.9821 - val_loss: 0.0330 - val_accuracy: 0.9895\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03305, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0271 - accuracy: 0.9913 - val_loss: 0.0144 - val_accuracy: 0.9950\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03305 to 0.01435, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0106 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01435 to 0.01062, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0084 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01062 to 0.00836, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0080 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00836 to 0.00802, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0073 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00802 to 0.00726, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00726 to 0.00682, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0072 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00682\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00682 to 0.00567, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0075 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00567\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.0051 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00567 to 0.00515, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0067 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00515\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0052 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00515\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00515\n",
            "Epoch 00014: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0527 - accuracy: 0.9851 - val_loss: 0.0238 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00515\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0230 - accuracy: 0.9925 - val_loss: 0.0111 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00515\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00515\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0081 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00515\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0083 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00515\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0085 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00515\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0066 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00515\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00515\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0071 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00515\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0059 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00515\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00515\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00515\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0053 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00515\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00515\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00515\n",
            "Epoch 16/200\n",
            "389/389 - 1s - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0070 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00515\n",
            "Epoch 17/200\n",
            "389/389 - 1s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0053 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00515\n",
            "Epoch 00017: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0542 - accuracy: 0.9835 - val_loss: 0.0221 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00515\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.0107 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00515\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.0090 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00515\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0082 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00515\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0090 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00515\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.0067 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00515\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0070 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00515\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0065 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00515\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0086 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00515\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00515\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0067 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00515\n",
            "Epoch 00011: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0553 - accuracy: 0.9836 - val_loss: 0.0262 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00515\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.0108 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00515\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.0086 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00515\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.0080 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00515\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0070 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00515\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.0070 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00515\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0073 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00515\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0103 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00515\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00515\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0052 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00515\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0062 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00515\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00515 to 0.00505, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00505 to 0.00490, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0051 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00490\n",
            "Epoch 15/200\n",
            "389/389 - 1s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0045 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00490 to 0.00448, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00015: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0545 - accuracy: 0.9830 - val_loss: 0.0253 - val_accuracy: 0.9910\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00448\n",
            "Epoch 2/200\n",
            "389/389 - 1s - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.0122 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00448\n",
            "Epoch 3/200\n",
            "389/389 - 1s - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0106 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00448\n",
            "Epoch 4/200\n",
            "389/389 - 1s - loss: 0.0141 - accuracy: 0.9956 - val_loss: 0.0095 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00448\n",
            "Epoch 5/200\n",
            "389/389 - 1s - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0071 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00448\n",
            "Epoch 6/200\n",
            "389/389 - 1s - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0073 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00448\n",
            "Epoch 7/200\n",
            "389/389 - 1s - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00448\n",
            "Epoch 8/200\n",
            "389/389 - 1s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0071 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00448\n",
            "Epoch 9/200\n",
            "389/389 - 1s - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.0059 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00448\n",
            "Epoch 10/200\n",
            "389/389 - 1s - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0059 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00448\n",
            "Epoch 11/200\n",
            "389/389 - 1s - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0063 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00448\n",
            "Epoch 12/200\n",
            "389/389 - 1s - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00448\n",
            "Epoch 13/200\n",
            "389/389 - 1s - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0058 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00448\n",
            "Epoch 14/200\n",
            "389/389 - 1s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00448\n",
            "Epoch 00014: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17567    22]\n",
            " [   22 11506]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9984888553078958\n",
            "Averaged F1: 0.9984888553078958\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER4gSGDqORsr"
      },
      "source": [
        "\n",
        "## Activation = Relu, Optimizer = Adam\n",
        "## Kernel 1: 32  neurons, size = 5\n",
        "## Kernel 2: 64  neurons, size = 5\n",
        "## Layer 1 : 1000 neurons\n",
        "\n",
        "Accuracy: 0.9989009856784696\n",
        "\n",
        "Averaged F1: 0.9989009528035263\n",
        "\n",
        "Time: 1m 37s"
      ],
      "id": "ER4gSGDqORsr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZzy7rF5Nv5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c44afcf7-b6c1-4f75-961c-3e32ab9bb1bd"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(1, 5), strides=(1, 1),\n",
        "                    activation='relu', padding='same',\n",
        "                    input_shape=(1, 116, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(1,2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (1, 5), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # define optimizer and objective, compile cnn\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=300, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "SZzy7rF5Nv5c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0520 - accuracy: 0.9832 - val_loss: 0.0284 - val_accuracy: 0.9897\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02844, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.0086 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02844 to 0.00858, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0071 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00858 to 0.00711, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00711 to 0.00644, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.0058 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00644 to 0.00584, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00584 to 0.00560, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0057 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00560\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0052 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00560 to 0.00520, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0050 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00520 to 0.00495, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00495 to 0.00495, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00010: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0517 - accuracy: 0.9838 - val_loss: 0.0231 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00495\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0102 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00495\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.0082 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00495\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.0074 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00495\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0061 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00495\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0057 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00495\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00495\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00495\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0049 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00495 to 0.00493, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0049 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00493 to 0.00485, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00485 to 0.00459, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00459 to 0.00454, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00454 to 0.00433, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0047 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00433\n",
            "Epoch 00014: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.0321 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00433\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0267 - accuracy: 0.9910 - val_loss: 0.0134 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00433\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.0076 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00433\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0063 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00433\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0068 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00433\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00433\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00433\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00433\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0077 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00433\n",
            "Epoch 00009: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0184 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00433\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0099 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00433\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.0068 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00433\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0071 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00433\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00433\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0069 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00433\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0062 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00433\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0053 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00433\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00433\n",
            "Epoch 10/200\n",
            "389/389 - 2s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0052 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00433\n",
            "Epoch 11/200\n",
            "389/389 - 2s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00433\n",
            "Epoch 12/200\n",
            "389/389 - 2s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0043 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00433 to 0.00426, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "389/389 - 2s - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00426\n",
            "Epoch 14/200\n",
            "389/389 - 2s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0041 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00426 to 0.00408, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "389/389 - 2s - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0042 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00408\n",
            "Epoch 16/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0038 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00408 to 0.00377, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "389/389 - 2s - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0040 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00377\n",
            "Epoch 00017: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "389/389 - 2s - loss: 0.0527 - accuracy: 0.9838 - val_loss: 0.0201 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00377\n",
            "Epoch 2/200\n",
            "389/389 - 2s - loss: 0.0205 - accuracy: 0.9930 - val_loss: 0.0086 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00377\n",
            "Epoch 3/200\n",
            "389/389 - 2s - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.0069 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00377\n",
            "Epoch 4/200\n",
            "389/389 - 2s - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.0058 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00377\n",
            "Epoch 5/200\n",
            "389/389 - 2s - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0056 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00377\n",
            "Epoch 6/200\n",
            "389/389 - 2s - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00377\n",
            "Epoch 7/200\n",
            "389/389 - 2s - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0059 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00377\n",
            "Epoch 8/200\n",
            "389/389 - 2s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00377\n",
            "Epoch 9/200\n",
            "389/389 - 2s - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00377\n",
            "Epoch 00009: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17575    14]\n",
            " [   18 11510]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9989009856784696\n",
            "Averaged F1: 0.9989009528035263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkwDdDUvb02-"
      },
      "source": [
        "## Fully Connected Models"
      ],
      "id": "TkwDdDUvb02-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slXOYfRVCvCJ"
      },
      "source": [
        "# Encode to a 2D matrix for training\n",
        "x = df_intrution.drop('outcome', axis='columns').to_numpy()\n",
        "y = df_intrution['outcome'].to_numpy()\n",
        "\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
      ],
      "id": "slXOYfRVCvCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1FPzHnEzpU"
      },
      "source": [
        "num_classes = 2\n",
        "\n",
        "# Converts a class vector (integers) to binary class matrix.   One-hot encoding!  Use with categorical_crossentropy.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "id": "jO1FPzHnEzpU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsMgsM_rcQzv"
      },
      "source": [
        "## Activation = Relu, Optimizer = adam\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "\n",
        "41s\n",
        "\n",
        "Accuracy: 0.9989009856784696\n",
        "\n",
        "Averaged F1: 0.9989009528035263\n"
      ],
      "id": "wsMgsM_rcQzv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4EvMg8gSBwAk",
        "outputId": "a9bbeba4-d879-4f46-833e-873f0f3850c9"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "4EvMg8gSBwAk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0228 - accuracy: 0.9801 - val_loss: 0.0072 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00723, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0061 - accuracy: 0.9922 - val_loss: 0.0047 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00723 to 0.00474, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9957 - val_loss: 0.0030 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00474 to 0.00298, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0027 - accuracy: 0.9979 - val_loss: 0.0021 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00298 to 0.00208, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00208 to 0.00192, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00192 to 0.00165, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00165\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00165\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00165\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00165 to 0.00159, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00010: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0230 - accuracy: 0.9803 - val_loss: 0.0074 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00159\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0063 - accuracy: 0.9918 - val_loss: 0.0049 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00159\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0041 - accuracy: 0.9953 - val_loss: 0.0033 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00159\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0027 - accuracy: 0.9979 - val_loss: 0.0021 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00159\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00159\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00159\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00159\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00159 to 0.00149, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0016 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00149\n",
            "Epoch 00009: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0226 - accuracy: 0.9796 - val_loss: 0.0073 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00149\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0062 - accuracy: 0.9920 - val_loss: 0.0049 - val_accuracy: 0.9941\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00149\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0042 - accuracy: 0.9951 - val_loss: 0.0032 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00149\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0028 - accuracy: 0.9978 - val_loss: 0.0022 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00149\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00149\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00149\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00149\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00149\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00149 to 0.00145, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00009: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0222 - accuracy: 0.9794 - val_loss: 0.0074 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00145\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0063 - accuracy: 0.9920 - val_loss: 0.0048 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00145\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9956 - val_loss: 0.0030 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00145\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0026 - accuracy: 0.9980 - val_loss: 0.0021 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00145\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00145\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00145\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0019 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00145\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00145\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00145 to 0.00144, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00144 to 0.00141, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00010: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0216 - accuracy: 0.9817 - val_loss: 0.0069 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00141\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0059 - accuracy: 0.9923 - val_loss: 0.0045 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00141\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0037 - accuracy: 0.9964 - val_loss: 0.0026 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00141\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0024 - accuracy: 0.9981 - val_loss: 0.0019 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00141\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00141\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00141\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00141\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00141\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00141\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0015 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00141\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0017 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00141\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0016 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00141\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9989 - val_loss: 0.0014 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00141 to 0.00140, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00013: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17575    14]\n",
            " [   18 11510]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9989009856784696\n",
            "Averaged F1: 0.9989009528035263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVyfEWHlcYFI"
      },
      "source": [
        "## Activation = Tanh, Optimizer = adam\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "\n",
        "47s\n",
        "\n",
        "Accuracy: 0.9985575437029914\n",
        "Averaged F1: 0.9985575221365692"
      ],
      "id": "zVyfEWHlcYFI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49whU40QceHi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "add15a33-37cd-408f-a9b4-fdb0f52af5b2"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='tanh'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "49whU40QceHi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0191 - accuracy: 0.9843 - val_loss: 0.0086 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00855, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0074 - accuracy: 0.9910 - val_loss: 0.0060 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00855 to 0.00605, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0054 - accuracy: 0.9927 - val_loss: 0.0055 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00605 to 0.00546, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9967 - val_loss: 0.0029 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00546 to 0.00295, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00295 to 0.00250, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9979 - val_loss: 0.0020 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00250 to 0.00202, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9980 - val_loss: 0.0021 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00202\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9981 - val_loss: 0.0020 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00202 to 0.00196, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00196 to 0.00168, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00168 to 0.00161, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00161 to 0.00161, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00161\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00161\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00161 to 0.00150, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00014: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0192 - accuracy: 0.9822 - val_loss: 0.0083 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00150\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0073 - accuracy: 0.9910 - val_loss: 0.0064 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00150\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0053 - accuracy: 0.9931 - val_loss: 0.0042 - val_accuracy: 0.9951\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00150\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9968 - val_loss: 0.0028 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00150\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0027 - accuracy: 0.9980 - val_loss: 0.0022 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00150\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0022 - accuracy: 0.9982 - val_loss: 0.0019 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00150\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9981 - val_loss: 0.0020 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00150\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9982 - val_loss: 0.0017 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00150\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9982 - val_loss: 0.0019 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00150\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00150\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00150\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00150\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0019 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00150\n",
            "Epoch 00013: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0237 - accuracy: 0.9805 - val_loss: 0.0088 - val_accuracy: 0.9895\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00150\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0077 - accuracy: 0.9909 - val_loss: 0.0065 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00150\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0058 - accuracy: 0.9921 - val_loss: 0.0048 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00150\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0042 - accuracy: 0.9959 - val_loss: 0.0034 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00150\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9979 - val_loss: 0.0026 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00150\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9981 - val_loss: 0.0021 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00150\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00150\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00150\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00150\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00150\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00150\n",
            "Epoch 00011: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0235 - accuracy: 0.9792 - val_loss: 0.0087 - val_accuracy: 0.9895\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00150\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0077 - accuracy: 0.9908 - val_loss: 0.0066 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00150\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0057 - accuracy: 0.9925 - val_loss: 0.0048 - val_accuracy: 0.9954\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00150\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0041 - accuracy: 0.9960 - val_loss: 0.0034 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00150\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0030 - accuracy: 0.9979 - val_loss: 0.0026 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00150\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0024 - accuracy: 0.9981 - val_loss: 0.0021 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00150\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00150\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00150\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00150\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00150\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00150 to 0.00149, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00011: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0249 - accuracy: 0.9792 - val_loss: 0.0086 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00149\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0077 - accuracy: 0.9908 - val_loss: 0.0062 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00149\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0056 - accuracy: 0.9925 - val_loss: 0.0045 - val_accuracy: 0.9947\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00149\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9961 - val_loss: 0.0033 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00149\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9979 - val_loss: 0.0023 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00149\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9982 - val_loss: 0.0027 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00149\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00149\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9983 - val_loss: 0.0023 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00149\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00149\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00149\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00149\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00149\n",
            "Epoch 00012: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17569    20]\n",
            " [   22 11506]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985575437029914\n",
            "Averaged F1: 0.9985575221365692\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHv5rrNgc5Eb"
      },
      "source": [
        "## Activation = Sigmoid, Optimizer = adam\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "\n",
        "1 m\n",
        "\n",
        "Accuracy: 0.9974928735790088\n",
        "Averaged F1: 0.9974924408781408"
      ],
      "id": "cHv5rrNgc5Eb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5DiOFMdA4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b1e4093-6482-43e0-e8fb-28f8760eb40c"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='sigmoid'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "1Q5DiOFMdA4T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0496 - accuracy: 0.9583 - val_loss: 0.0204 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02036, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0185 - accuracy: 0.9835 - val_loss: 0.0186 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.02036 to 0.01865, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0160 - accuracy: 0.9837 - val_loss: 0.0160 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01865 to 0.01603, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0144 - accuracy: 0.9836 - val_loss: 0.0138 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01603 to 0.01375, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0136 - accuracy: 0.9837 - val_loss: 0.0129 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01375 to 0.01285, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0130 - accuracy: 0.9838 - val_loss: 0.0134 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01285\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0125 - accuracy: 0.9841 - val_loss: 0.0115 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01285 to 0.01153, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0118 - accuracy: 0.9844 - val_loss: 0.0110 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01153 to 0.01097, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0111 - accuracy: 0.9846 - val_loss: 0.0107 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01097 to 0.01074, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0106 - accuracy: 0.9849 - val_loss: 0.0098 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01074 to 0.00983, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0101 - accuracy: 0.9853 - val_loss: 0.0121 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00983\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0095 - accuracy: 0.9862 - val_loss: 0.0089 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00983 to 0.00891, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0088 - accuracy: 0.9879 - val_loss: 0.0079 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00891 to 0.00793, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0081 - accuracy: 0.9896 - val_loss: 0.0073 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00793 to 0.00726, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0073 - accuracy: 0.9921 - val_loss: 0.0087 - val_accuracy: 0.9941\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00726\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0068 - accuracy: 0.9938 - val_loss: 0.0059 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00726 to 0.00590, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0060 - accuracy: 0.9953 - val_loss: 0.0058 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00590 to 0.00578, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0059 - accuracy: 0.9962 - val_loss: 0.0062 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00578\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0050 - accuracy: 0.9964 - val_loss: 0.0048 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00578 to 0.00477, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0048 - accuracy: 0.9964 - val_loss: 0.0053 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00477\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0045 - accuracy: 0.9964 - val_loss: 0.0039 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.00477 to 0.00388, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0041 - accuracy: 0.9965 - val_loss: 0.0039 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00388\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0043 - accuracy: 0.9966 - val_loss: 0.0039 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00388\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9966 - val_loss: 0.0043 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00388\n",
            "Epoch 00024: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0473 - accuracy: 0.9551 - val_loss: 0.0194 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00388\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0174 - accuracy: 0.9833 - val_loss: 0.0152 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00388\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0148 - accuracy: 0.9836 - val_loss: 0.0139 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00388\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0132 - accuracy: 0.9837 - val_loss: 0.0123 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00388\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0123 - accuracy: 0.9843 - val_loss: 0.0115 - val_accuracy: 0.9853\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00388\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0117 - accuracy: 0.9848 - val_loss: 0.0114 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00388\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0110 - accuracy: 0.9851 - val_loss: 0.0106 - val_accuracy: 0.9860\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00388\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0105 - accuracy: 0.9857 - val_loss: 0.0109 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00388\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0101 - accuracy: 0.9865 - val_loss: 0.0122 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00388\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0096 - accuracy: 0.9877 - val_loss: 0.0091 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00388\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0089 - accuracy: 0.9888 - val_loss: 0.0086 - val_accuracy: 0.9899\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00388\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0084 - accuracy: 0.9901 - val_loss: 0.0077 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00388\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0080 - accuracy: 0.9907 - val_loss: 0.0077 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00388\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0074 - accuracy: 0.9918 - val_loss: 0.0079 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00388\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0069 - accuracy: 0.9932 - val_loss: 0.0083 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00388\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0065 - accuracy: 0.9945 - val_loss: 0.0062 - val_accuracy: 0.9957\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00388\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0059 - accuracy: 0.9954 - val_loss: 0.0055 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00388\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.0057 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00388\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0053 - accuracy: 0.9966 - val_loss: 0.0060 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00388\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0049 - accuracy: 0.9969 - val_loss: 0.0045 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00388\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0046 - accuracy: 0.9969 - val_loss: 0.0043 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00388\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0045 - accuracy: 0.9969 - val_loss: 0.0043 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00388\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0043 - accuracy: 0.9970 - val_loss: 0.0040 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00388\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0043 - accuracy: 0.9969 - val_loss: 0.0041 - val_accuracy: 0.9972\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00388\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9970 - val_loss: 0.0044 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00388\n",
            "Epoch 00025: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0485 - accuracy: 0.9536 - val_loss: 0.0196 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00388\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0178 - accuracy: 0.9835 - val_loss: 0.0158 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00388\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9836 - val_loss: 0.0139 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00388\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9837 - val_loss: 0.0129 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00388\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0129 - accuracy: 0.9839 - val_loss: 0.0124 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00388\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0126 - accuracy: 0.9842 - val_loss: 0.0118 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00388\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0121 - accuracy: 0.9843 - val_loss: 0.0110 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00388\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0114 - accuracy: 0.9847 - val_loss: 0.0116 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00388\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0108 - accuracy: 0.9850 - val_loss: 0.0112 - val_accuracy: 0.9872\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00388\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0103 - accuracy: 0.9860 - val_loss: 0.0098 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00388\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0099 - accuracy: 0.9870 - val_loss: 0.0107 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00388\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0094 - accuracy: 0.9885 - val_loss: 0.0086 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00388\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0087 - accuracy: 0.9896 - val_loss: 0.0081 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00388\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0082 - accuracy: 0.9904 - val_loss: 0.0076 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00388\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0075 - accuracy: 0.9910 - val_loss: 0.0072 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00388\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0069 - accuracy: 0.9918 - val_loss: 0.0077 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00388\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0065 - accuracy: 0.9930 - val_loss: 0.0058 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00388\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0062 - accuracy: 0.9941 - val_loss: 0.0089 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00388\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0057 - accuracy: 0.9952 - val_loss: 0.0050 - val_accuracy: 0.9958\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00388\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0051 - accuracy: 0.9962 - val_loss: 0.0047 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00388\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.0050 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00388\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0046 - accuracy: 0.9970 - val_loss: 0.0042 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00388\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0042 - accuracy: 0.9970 - val_loss: 0.0037 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00388 to 0.00371, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9971 - val_loss: 0.0048 - val_accuracy: 0.9970\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00371\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9972 - val_loss: 0.0035 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00371 to 0.00345, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0037 - accuracy: 0.9973 - val_loss: 0.0034 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00345 to 0.00338, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0035 - accuracy: 0.9973 - val_loss: 0.0032 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00338 to 0.00318, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0034 - accuracy: 0.9973 - val_loss: 0.0037 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00318\n",
            "Epoch 00028: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0488 - accuracy: 0.9645 - val_loss: 0.0200 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00318\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0180 - accuracy: 0.9833 - val_loss: 0.0165 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00318\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9834 - val_loss: 0.0147 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00318\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9833 - val_loss: 0.0138 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00318\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0138 - accuracy: 0.9833 - val_loss: 0.0129 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00318\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0130 - accuracy: 0.9838 - val_loss: 0.0124 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00318\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0125 - accuracy: 0.9841 - val_loss: 0.0120 - val_accuracy: 0.9849\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00318\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0117 - accuracy: 0.9844 - val_loss: 0.0118 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00318\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0112 - accuracy: 0.9845 - val_loss: 0.0104 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00318\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0106 - accuracy: 0.9849 - val_loss: 0.0099 - val_accuracy: 0.9853\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00318\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0101 - accuracy: 0.9855 - val_loss: 0.0099 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00318\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0094 - accuracy: 0.9865 - val_loss: 0.0086 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00318\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0087 - accuracy: 0.9881 - val_loss: 0.0085 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00318\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0081 - accuracy: 0.9901 - val_loss: 0.0077 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00318\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0075 - accuracy: 0.9916 - val_loss: 0.0069 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00318\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0069 - accuracy: 0.9938 - val_loss: 0.0061 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00318\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0063 - accuracy: 0.9951 - val_loss: 0.0056 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00318\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0057 - accuracy: 0.9959 - val_loss: 0.0053 - val_accuracy: 0.9955\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00318\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0054 - accuracy: 0.9959 - val_loss: 0.0074 - val_accuracy: 0.9955\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00318\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0050 - accuracy: 0.9959 - val_loss: 0.0060 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00318\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0046 - accuracy: 0.9961 - val_loss: 0.0042 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00318\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0045 - accuracy: 0.9962 - val_loss: 0.0038 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00318\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0044 - accuracy: 0.9963 - val_loss: 0.0040 - val_accuracy: 0.9964\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00318\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9964 - val_loss: 0.0037 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00318\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9964 - val_loss: 0.0042 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00318\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9966 - val_loss: 0.0036 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00318\n",
            "Epoch 00026: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0470 - accuracy: 0.9599 - val_loss: 0.0202 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00318\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0179 - accuracy: 0.9835 - val_loss: 0.0156 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00318\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9835 - val_loss: 0.0142 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00318\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0140 - accuracy: 0.9834 - val_loss: 0.0130 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00318\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0129 - accuracy: 0.9838 - val_loss: 0.0129 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00318\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0126 - accuracy: 0.9842 - val_loss: 0.0125 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00318\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0117 - accuracy: 0.9845 - val_loss: 0.0114 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00318\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0111 - accuracy: 0.9848 - val_loss: 0.0117 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00318\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0108 - accuracy: 0.9851 - val_loss: 0.0100 - val_accuracy: 0.9858\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00318\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0104 - accuracy: 0.9857 - val_loss: 0.0101 - val_accuracy: 0.9862\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00318\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0099 - accuracy: 0.9866 - val_loss: 0.0090 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00318\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0092 - accuracy: 0.9877 - val_loss: 0.0089 - val_accuracy: 0.9892\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00318\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0089 - accuracy: 0.9891 - val_loss: 0.0082 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00318\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0085 - accuracy: 0.9901 - val_loss: 0.0075 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00318\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0078 - accuracy: 0.9911 - val_loss: 0.0093 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00318\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0074 - accuracy: 0.9920 - val_loss: 0.0093 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00318\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0066 - accuracy: 0.9935 - val_loss: 0.0067 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00318\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0062 - accuracy: 0.9950 - val_loss: 0.0089 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00318\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0060 - accuracy: 0.9957 - val_loss: 0.0050 - val_accuracy: 0.9962\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00318\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0053 - accuracy: 0.9963 - val_loss: 0.0059 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00318\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0050 - accuracy: 0.9969 - val_loss: 0.0054 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00318\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0046 - accuracy: 0.9969 - val_loss: 0.0051 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00318\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0045 - accuracy: 0.9970 - val_loss: 0.0037 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00318\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0041 - accuracy: 0.9970 - val_loss: 0.0035 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00318\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9970 - val_loss: 0.0035 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00318\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9971 - val_loss: 0.0038 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00318\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0036 - accuracy: 0.9971 - val_loss: 0.0031 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00318 to 0.00314, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9971 - val_loss: 0.0031 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00314 to 0.00306, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00028: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17564    25]\n",
            " [   48 11480]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9974928735790088\n",
            "Averaged F1: 0.9974924408781408\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMzoQYv6d8lI"
      },
      "source": [
        "## Activation = Relu, Optimizer = sgd\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "\n",
        "1 m\n",
        "\n",
        "Accuracy: 0.9837208503623313\n",
        "Averaged F1: 0.9836845243872174\n",
        "\n"
      ],
      "id": "tMzoQYv6d8lI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cVC72seSP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d68281df-994a-4c67-e7b5-e0b7b36a82a6"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "29cVC72seSP_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0798 - accuracy: 0.9477 - val_loss: 0.0373 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03730, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0341 - accuracy: 0.9648 - val_loss: 0.0318 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03730 to 0.03179, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0301 - accuracy: 0.9652 - val_loss: 0.0288 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03179 to 0.02875, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0276 - accuracy: 0.9655 - val_loss: 0.0266 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02875 to 0.02661, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9659 - val_loss: 0.0250 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02661 to 0.02499, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0243 - accuracy: 0.9672 - val_loss: 0.0237 - val_accuracy: 0.9666\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.02499 to 0.02368, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0231 - accuracy: 0.9678 - val_loss: 0.0226 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02368 to 0.02261, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0222 - accuracy: 0.9688 - val_loss: 0.0217 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02261 to 0.02170, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0214 - accuracy: 0.9697 - val_loss: 0.0209 - val_accuracy: 0.9699\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.02170 to 0.02092, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0207 - accuracy: 0.9705 - val_loss: 0.0202 - val_accuracy: 0.9705\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.02092 to 0.02024, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0201 - accuracy: 0.9712 - val_loss: 0.0196 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.02024 to 0.01964, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0195 - accuracy: 0.9716 - val_loss: 0.0191 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01964 to 0.01911, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0190 - accuracy: 0.9724 - val_loss: 0.0186 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01911 to 0.01863, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0186 - accuracy: 0.9742 - val_loss: 0.0182 - val_accuracy: 0.9754\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01863 to 0.01819, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0182 - accuracy: 0.9758 - val_loss: 0.0178 - val_accuracy: 0.9777\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01819 to 0.01779, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0178 - accuracy: 0.9772 - val_loss: 0.0174 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01779 to 0.01742, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0175 - accuracy: 0.9779 - val_loss: 0.0171 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01742 to 0.01708, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0172 - accuracy: 0.9784 - val_loss: 0.0168 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01708 to 0.01676, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9795 - val_loss: 0.0165 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01676 to 0.01647, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0166 - accuracy: 0.9805 - val_loss: 0.0162 - val_accuracy: 0.9816\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01647 to 0.01620, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0163 - accuracy: 0.9808 - val_loss: 0.0159 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.01620 to 0.01594, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9815 - val_loss: 0.0157 - val_accuracy: 0.9829\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01594 to 0.01570, saving model to dnn/best_weights.hdf5\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0159 - accuracy: 0.9818 - val_loss: 0.0155 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01570 to 0.01548, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9819 - val_loss: 0.0153 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01548 to 0.01526, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9820 - val_loss: 0.0151 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01526 to 0.01506, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9821 - val_loss: 0.0149 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01506 to 0.01487, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9822 - val_loss: 0.0147 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01487 to 0.01469, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9824 - val_loss: 0.0145 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01469 to 0.01453, saving model to dnn/best_weights.hdf5\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0148 - accuracy: 0.9828 - val_loss: 0.0144 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01453 to 0.01436, saving model to dnn/best_weights.hdf5\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9831 - val_loss: 0.0142 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.01436 to 0.01420, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00030: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1043 - accuracy: 0.9058 - val_loss: 0.0433 - val_accuracy: 0.9652\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01420\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0380 - accuracy: 0.9672 - val_loss: 0.0341 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01420\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0320 - accuracy: 0.9679 - val_loss: 0.0301 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01420\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0289 - accuracy: 0.9680 - val_loss: 0.0276 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01420\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0267 - accuracy: 0.9681 - val_loss: 0.0256 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01420\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0250 - accuracy: 0.9681 - val_loss: 0.0241 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01420\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0236 - accuracy: 0.9682 - val_loss: 0.0227 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01420\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0224 - accuracy: 0.9686 - val_loss: 0.0216 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01420\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0214 - accuracy: 0.9697 - val_loss: 0.0206 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01420\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0205 - accuracy: 0.9727 - val_loss: 0.0197 - val_accuracy: 0.9754\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01420\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0197 - accuracy: 0.9763 - val_loss: 0.0190 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01420\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0190 - accuracy: 0.9789 - val_loss: 0.0183 - val_accuracy: 0.9799\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01420\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0184 - accuracy: 0.9803 - val_loss: 0.0176 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01420\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0178 - accuracy: 0.9814 - val_loss: 0.0171 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01420\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0173 - accuracy: 0.9814 - val_loss: 0.0166 - val_accuracy: 0.9826\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01420\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9820 - val_loss: 0.0162 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01420\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0165 - accuracy: 0.9820 - val_loss: 0.0158 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01420\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9820 - val_loss: 0.0154 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01420\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9820 - val_loss: 0.0151 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01420\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0154 - accuracy: 0.9820 - val_loss: 0.0148 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01420\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9821 - val_loss: 0.0145 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01420\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9821 - val_loss: 0.0142 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01420\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9821 - val_loss: 0.0140 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01420 to 0.01400, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0144 - accuracy: 0.9822 - val_loss: 0.0138 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01400 to 0.01378, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0142 - accuracy: 0.9824 - val_loss: 0.0136 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01378 to 0.01358, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0140 - accuracy: 0.9825 - val_loss: 0.0134 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01358 to 0.01339, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0138 - accuracy: 0.9825 - val_loss: 0.0132 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01339 to 0.01321, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0137 - accuracy: 0.9825 - val_loss: 0.0131 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01321 to 0.01305, saving model to dnn/best_weights.hdf5\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0135 - accuracy: 0.9826 - val_loss: 0.0129 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01305 to 0.01290, saving model to dnn/best_weights.hdf5\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0133 - accuracy: 0.9826 - val_loss: 0.0128 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.01290 to 0.01275, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00030: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1024 - accuracy: 0.9357 - val_loss: 0.0429 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01275\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0391 - accuracy: 0.9674 - val_loss: 0.0359 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01275\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0337 - accuracy: 0.9677 - val_loss: 0.0316 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01275\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0301 - accuracy: 0.9677 - val_loss: 0.0285 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01275\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0275 - accuracy: 0.9677 - val_loss: 0.0263 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01275\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0256 - accuracy: 0.9678 - val_loss: 0.0246 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01275\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0241 - accuracy: 0.9678 - val_loss: 0.0232 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01275\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0229 - accuracy: 0.9678 - val_loss: 0.0221 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01275\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0218 - accuracy: 0.9681 - val_loss: 0.0210 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01275\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0209 - accuracy: 0.9701 - val_loss: 0.0202 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01275\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0201 - accuracy: 0.9724 - val_loss: 0.0194 - val_accuracy: 0.9741\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01275\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0193 - accuracy: 0.9756 - val_loss: 0.0187 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01275\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0187 - accuracy: 0.9783 - val_loss: 0.0181 - val_accuracy: 0.9807\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01275\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0182 - accuracy: 0.9798 - val_loss: 0.0176 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01275\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0177 - accuracy: 0.9799 - val_loss: 0.0171 - val_accuracy: 0.9817\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01275\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0172 - accuracy: 0.9805 - val_loss: 0.0167 - val_accuracy: 0.9826\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01275\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0168 - accuracy: 0.9812 - val_loss: 0.0163 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01275\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0164 - accuracy: 0.9816 - val_loss: 0.0159 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01275\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9818 - val_loss: 0.0156 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01275\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0158 - accuracy: 0.9819 - val_loss: 0.0153 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01275\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0156 - accuracy: 0.9819 - val_loss: 0.0151 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01275\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9819 - val_loss: 0.0148 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01275\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9820 - val_loss: 0.0146 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01275\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9820 - val_loss: 0.0144 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01275\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0147 - accuracy: 0.9820 - val_loss: 0.0142 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01275\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0145 - accuracy: 0.9820 - val_loss: 0.0140 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01275\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0143 - accuracy: 0.9821 - val_loss: 0.0139 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01275\n",
            "Epoch 00027: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1015 - accuracy: 0.9488 - val_loss: 0.0419 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01275\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0381 - accuracy: 0.9665 - val_loss: 0.0348 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01275\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0329 - accuracy: 0.9675 - val_loss: 0.0307 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01275\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0296 - accuracy: 0.9676 - val_loss: 0.0281 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01275\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0274 - accuracy: 0.9676 - val_loss: 0.0261 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01275\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9676 - val_loss: 0.0246 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01275\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0244 - accuracy: 0.9677 - val_loss: 0.0234 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01275\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0233 - accuracy: 0.9678 - val_loss: 0.0224 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01275\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0224 - accuracy: 0.9680 - val_loss: 0.0215 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01275\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0216 - accuracy: 0.9684 - val_loss: 0.0208 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01275\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0209 - accuracy: 0.9693 - val_loss: 0.0201 - val_accuracy: 0.9710\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01275\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0203 - accuracy: 0.9707 - val_loss: 0.0195 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01275\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0197 - accuracy: 0.9727 - val_loss: 0.0189 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01275\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0192 - accuracy: 0.9754 - val_loss: 0.0184 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01275\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0187 - accuracy: 0.9788 - val_loss: 0.0180 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01275\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0183 - accuracy: 0.9805 - val_loss: 0.0175 - val_accuracy: 0.9826\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01275\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0179 - accuracy: 0.9811 - val_loss: 0.0172 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01275\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0176 - accuracy: 0.9817 - val_loss: 0.0168 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01275\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0172 - accuracy: 0.9821 - val_loss: 0.0165 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01275\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9823 - val_loss: 0.0162 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01275\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0166 - accuracy: 0.9823 - val_loss: 0.0159 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01275\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0164 - accuracy: 0.9823 - val_loss: 0.0157 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01275\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9823 - val_loss: 0.0155 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01275\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0159 - accuracy: 0.9824 - val_loss: 0.0152 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01275\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9824 - val_loss: 0.0150 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01275\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9824 - val_loss: 0.0148 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01275\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9824 - val_loss: 0.0146 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01275\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9824 - val_loss: 0.0145 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01275\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9825 - val_loss: 0.0143 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01275\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0148 - accuracy: 0.9825 - val_loss: 0.0141 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01275\n",
            "Epoch 31/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9825 - val_loss: 0.0140 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01275\n",
            "Epoch 00031: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0849 - accuracy: 0.9394 - val_loss: 0.0387 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01275\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0350 - accuracy: 0.9682 - val_loss: 0.0318 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01275\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0302 - accuracy: 0.9681 - val_loss: 0.0283 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01275\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0273 - accuracy: 0.9682 - val_loss: 0.0260 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01275\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0253 - accuracy: 0.9686 - val_loss: 0.0243 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01275\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0238 - accuracy: 0.9691 - val_loss: 0.0229 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01275\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0225 - accuracy: 0.9703 - val_loss: 0.0217 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01275\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0215 - accuracy: 0.9710 - val_loss: 0.0208 - val_accuracy: 0.9713\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01275\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0206 - accuracy: 0.9723 - val_loss: 0.0199 - val_accuracy: 0.9743\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01275\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0199 - accuracy: 0.9747 - val_loss: 0.0192 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01275\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0192 - accuracy: 0.9752 - val_loss: 0.0186 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01275\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0186 - accuracy: 0.9769 - val_loss: 0.0180 - val_accuracy: 0.9796\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01275\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0181 - accuracy: 0.9789 - val_loss: 0.0175 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01275\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0177 - accuracy: 0.9799 - val_loss: 0.0171 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01275\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0173 - accuracy: 0.9805 - val_loss: 0.0167 - val_accuracy: 0.9823\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01275\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9811 - val_loss: 0.0163 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01275\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0166 - accuracy: 0.9817 - val_loss: 0.0160 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01275\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0163 - accuracy: 0.9820 - val_loss: 0.0157 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01275\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0160 - accuracy: 0.9820 - val_loss: 0.0155 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01275\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9820 - val_loss: 0.0152 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01275\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9821 - val_loss: 0.0150 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01275\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9821 - val_loss: 0.0148 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01275\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9821 - val_loss: 0.0146 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01275\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9821 - val_loss: 0.0144 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01275\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0147 - accuracy: 0.9822 - val_loss: 0.0142 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01275\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0145 - accuracy: 0.9824 - val_loss: 0.0140 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01275\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0144 - accuracy: 0.9825 - val_loss: 0.0138 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01275\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0142 - accuracy: 0.9826 - val_loss: 0.0137 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01275\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0141 - accuracy: 0.9826 - val_loss: 0.0135 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01275\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9826 - val_loss: 0.0134 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01275\n",
            "Epoch 00030: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17494    95]\n",
            " [  379 11149]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9837208503623313\n",
            "Averaged F1: 0.9836845243872174\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     17589\n",
            "           1       0.99      0.97      0.98     11528\n",
            "\n",
            "    accuracy                           0.98     29117\n",
            "   macro avg       0.99      0.98      0.98     29117\n",
            "weighted avg       0.98      0.98      0.98     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14a34GOdRk3"
      },
      "source": [
        "## Activation = Tanh, Optimizer = sgd\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "1 m\n",
        "\n",
        "Accuracy: 0.985128962461792\n",
        "Averaged F1: 0.9851017369243829\n",
        "\n"
      ],
      "id": "a14a34GOdRk3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlfixMw7I_hp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb9454dc-388f-4529-fe9b-66236f4cdbf5"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='tanh'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "ZlfixMw7I_hp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0780 - accuracy: 0.9332 - val_loss: 0.0358 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03583, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0324 - accuracy: 0.9683 - val_loss: 0.0297 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03583 to 0.02966, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0280 - accuracy: 0.9686 - val_loss: 0.0264 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02966 to 0.02640, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0254 - accuracy: 0.9691 - val_loss: 0.0242 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02640 to 0.02422, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0236 - accuracy: 0.9700 - val_loss: 0.0226 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02422 to 0.02261, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0222 - accuracy: 0.9712 - val_loss: 0.0214 - val_accuracy: 0.9720\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.02261 to 0.02136, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0211 - accuracy: 0.9722 - val_loss: 0.0204 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02136 to 0.02035, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0202 - accuracy: 0.9748 - val_loss: 0.0195 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02035 to 0.01952, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0195 - accuracy: 0.9784 - val_loss: 0.0188 - val_accuracy: 0.9816\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01952 to 0.01882, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0188 - accuracy: 0.9806 - val_loss: 0.0182 - val_accuracy: 0.9820\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01882 to 0.01822, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0183 - accuracy: 0.9813 - val_loss: 0.0177 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01822 to 0.01769, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0178 - accuracy: 0.9819 - val_loss: 0.0172 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01769 to 0.01723, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0174 - accuracy: 0.9821 - val_loss: 0.0168 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01723 to 0.01681, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0170 - accuracy: 0.9822 - val_loss: 0.0164 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01681 to 0.01643, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0166 - accuracy: 0.9821 - val_loss: 0.0161 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01643 to 0.01610, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0163 - accuracy: 0.9821 - val_loss: 0.0158 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01610 to 0.01579, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0160 - accuracy: 0.9822 - val_loss: 0.0155 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01579 to 0.01550, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0158 - accuracy: 0.9822 - val_loss: 0.0152 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01550 to 0.01524, saving model to dnn/best_weights.hdf5\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9823 - val_loss: 0.0150 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01524 to 0.01500, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0153 - accuracy: 0.9823 - val_loss: 0.0148 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01500 to 0.01477, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0151 - accuracy: 0.9824 - val_loss: 0.0146 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.01477 to 0.01456, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9826 - val_loss: 0.0144 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01456 to 0.01436, saving model to dnn/best_weights.hdf5\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0147 - accuracy: 0.9826 - val_loss: 0.0142 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01436 to 0.01418, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0145 - accuracy: 0.9826 - val_loss: 0.0140 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01418 to 0.01399, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0143 - accuracy: 0.9826 - val_loss: 0.0138 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01399 to 0.01383, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0142 - accuracy: 0.9826 - val_loss: 0.0137 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01383 to 0.01367, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0140 - accuracy: 0.9827 - val_loss: 0.0135 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01367 to 0.01352, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9826 - val_loss: 0.0134 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01352 to 0.01339, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00028: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0837 - accuracy: 0.9292 - val_loss: 0.0352 - val_accuracy: 0.9674\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01339\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0319 - accuracy: 0.9677 - val_loss: 0.0291 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01339\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0278 - accuracy: 0.9679 - val_loss: 0.0262 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01339\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0254 - accuracy: 0.9681 - val_loss: 0.0242 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01339\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0237 - accuracy: 0.9685 - val_loss: 0.0227 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01339\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0224 - accuracy: 0.9701 - val_loss: 0.0214 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01339\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0213 - accuracy: 0.9726 - val_loss: 0.0204 - val_accuracy: 0.9748\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01339\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0204 - accuracy: 0.9761 - val_loss: 0.0196 - val_accuracy: 0.9790\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01339\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0196 - accuracy: 0.9779 - val_loss: 0.0189 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01339\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0190 - accuracy: 0.9790 - val_loss: 0.0183 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01339\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0184 - accuracy: 0.9803 - val_loss: 0.0177 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01339\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0179 - accuracy: 0.9810 - val_loss: 0.0172 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01339\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0174 - accuracy: 0.9815 - val_loss: 0.0168 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01339\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0170 - accuracy: 0.9817 - val_loss: 0.0164 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01339\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0167 - accuracy: 0.9818 - val_loss: 0.0161 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01339\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0163 - accuracy: 0.9820 - val_loss: 0.0157 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01339\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0160 - accuracy: 0.9822 - val_loss: 0.0154 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01339\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9822 - val_loss: 0.0152 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01339\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9823 - val_loss: 0.0149 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01339\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0152 - accuracy: 0.9823 - val_loss: 0.0147 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01339\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0150 - accuracy: 0.9823 - val_loss: 0.0144 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01339\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0148 - accuracy: 0.9824 - val_loss: 0.0142 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01339\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9824 - val_loss: 0.0140 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01339\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0144 - accuracy: 0.9824 - val_loss: 0.0138 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01339\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0142 - accuracy: 0.9824 - val_loss: 0.0137 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01339\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0140 - accuracy: 0.9824 - val_loss: 0.0135 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01339\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9824 - val_loss: 0.0134 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01339 to 0.01335, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0137 - accuracy: 0.9824 - val_loss: 0.0132 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01335 to 0.01321, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00028: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0644 - accuracy: 0.9553 - val_loss: 0.0366 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01321\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0339 - accuracy: 0.9676 - val_loss: 0.0314 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01321\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0299 - accuracy: 0.9677 - val_loss: 0.0282 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01321\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0272 - accuracy: 0.9677 - val_loss: 0.0259 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01321\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9678 - val_loss: 0.0241 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01321\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0237 - accuracy: 0.9681 - val_loss: 0.0227 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01321\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0224 - accuracy: 0.9687 - val_loss: 0.0215 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01321\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0213 - accuracy: 0.9712 - val_loss: 0.0205 - val_accuracy: 0.9726\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01321\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0204 - accuracy: 0.9732 - val_loss: 0.0196 - val_accuracy: 0.9741\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01321\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0196 - accuracy: 0.9744 - val_loss: 0.0189 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01321\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0189 - accuracy: 0.9762 - val_loss: 0.0182 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01321\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0183 - accuracy: 0.9787 - val_loss: 0.0177 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01321\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0177 - accuracy: 0.9805 - val_loss: 0.0171 - val_accuracy: 0.9826\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01321\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0173 - accuracy: 0.9813 - val_loss: 0.0167 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01321\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0168 - accuracy: 0.9815 - val_loss: 0.0163 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01321\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0164 - accuracy: 0.9815 - val_loss: 0.0159 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01321\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9815 - val_loss: 0.0156 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01321\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9816 - val_loss: 0.0152 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01321\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0154 - accuracy: 0.9816 - val_loss: 0.0150 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01321\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0152 - accuracy: 0.9816 - val_loss: 0.0147 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01321\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0149 - accuracy: 0.9817 - val_loss: 0.0144 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01321\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0147 - accuracy: 0.9818 - val_loss: 0.0142 - val_accuracy: 0.9832\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01321\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0145 - accuracy: 0.9821 - val_loss: 0.0140 - val_accuracy: 0.9835\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01321\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0143 - accuracy: 0.9826 - val_loss: 0.0138 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01321\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0141 - accuracy: 0.9831 - val_loss: 0.0137 - val_accuracy: 0.9841\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01321\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9834 - val_loss: 0.0135 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01321\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0138 - accuracy: 0.9837 - val_loss: 0.0133 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01321\n",
            "Epoch 00027: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0807 - accuracy: 0.9412 - val_loss: 0.0366 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01321\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0333 - accuracy: 0.9680 - val_loss: 0.0310 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01321\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0291 - accuracy: 0.9681 - val_loss: 0.0278 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01321\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0265 - accuracy: 0.9682 - val_loss: 0.0255 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01321\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0245 - accuracy: 0.9683 - val_loss: 0.0237 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01321\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0229 - accuracy: 0.9682 - val_loss: 0.0222 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01321\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0217 - accuracy: 0.9683 - val_loss: 0.0211 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01321\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0207 - accuracy: 0.9695 - val_loss: 0.0201 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01321\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0198 - accuracy: 0.9726 - val_loss: 0.0193 - val_accuracy: 0.9738\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01321\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0190 - accuracy: 0.9746 - val_loss: 0.0186 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01321\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0184 - accuracy: 0.9754 - val_loss: 0.0180 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01321\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0179 - accuracy: 0.9772 - val_loss: 0.0174 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01321\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0174 - accuracy: 0.9783 - val_loss: 0.0170 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01321\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0170 - accuracy: 0.9804 - val_loss: 0.0165 - val_accuracy: 0.9819\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01321\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0166 - accuracy: 0.9812 - val_loss: 0.0162 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01321\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0162 - accuracy: 0.9816 - val_loss: 0.0159 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01321\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0159 - accuracy: 0.9817 - val_loss: 0.0155 - val_accuracy: 0.9829\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01321\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0157 - accuracy: 0.9818 - val_loss: 0.0153 - val_accuracy: 0.9829\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01321\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0154 - accuracy: 0.9818 - val_loss: 0.0150 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01321\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0152 - accuracy: 0.9820 - val_loss: 0.0148 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01321\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0150 - accuracy: 0.9823 - val_loss: 0.0146 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01321\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0148 - accuracy: 0.9830 - val_loss: 0.0144 - val_accuracy: 0.9838\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01321\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0146 - accuracy: 0.9835 - val_loss: 0.0142 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01321\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0144 - accuracy: 0.9837 - val_loss: 0.0141 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01321\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0143 - accuracy: 0.9838 - val_loss: 0.0139 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01321\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0141 - accuracy: 0.9838 - val_loss: 0.0138 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01321\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0140 - accuracy: 0.9838 - val_loss: 0.0136 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01321\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9838 - val_loss: 0.0135 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01321\n",
            "Epoch 00028: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0740 - accuracy: 0.9346 - val_loss: 0.0343 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01321\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0312 - accuracy: 0.9693 - val_loss: 0.0292 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01321\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0275 - accuracy: 0.9693 - val_loss: 0.0263 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01321\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9695 - val_loss: 0.0242 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01321\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0234 - accuracy: 0.9702 - val_loss: 0.0226 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01321\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0220 - accuracy: 0.9715 - val_loss: 0.0213 - val_accuracy: 0.9731\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01321\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0209 - accuracy: 0.9736 - val_loss: 0.0203 - val_accuracy: 0.9749\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01321\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0200 - accuracy: 0.9761 - val_loss: 0.0194 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01321\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0192 - accuracy: 0.9791 - val_loss: 0.0186 - val_accuracy: 0.9807\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01321\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0185 - accuracy: 0.9803 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01321\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0179 - accuracy: 0.9811 - val_loss: 0.0174 - val_accuracy: 0.9827\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01321\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0174 - accuracy: 0.9815 - val_loss: 0.0168 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01321\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9821 - val_loss: 0.0164 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01321\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0165 - accuracy: 0.9826 - val_loss: 0.0160 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01321\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0161 - accuracy: 0.9827 - val_loss: 0.0156 - val_accuracy: 0.9842\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01321\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0158 - accuracy: 0.9829 - val_loss: 0.0153 - val_accuracy: 0.9843\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01321\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9832 - val_loss: 0.0150 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01321\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0152 - accuracy: 0.9834 - val_loss: 0.0147 - val_accuracy: 0.9846\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01321\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0150 - accuracy: 0.9836 - val_loss: 0.0145 - val_accuracy: 0.9848\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01321\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0147 - accuracy: 0.9839 - val_loss: 0.0142 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01321\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0145 - accuracy: 0.9840 - val_loss: 0.0140 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01321\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0143 - accuracy: 0.9842 - val_loss: 0.0138 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01321\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0141 - accuracy: 0.9843 - val_loss: 0.0136 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01321\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0139 - accuracy: 0.9845 - val_loss: 0.0135 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01321\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0138 - accuracy: 0.9845 - val_loss: 0.0133 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01321\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0136 - accuracy: 0.9846 - val_loss: 0.0131 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01321 to 0.01313, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0135 - accuracy: 0.9846 - val_loss: 0.0130 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01313 to 0.01299, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0133 - accuracy: 0.9846 - val_loss: 0.0128 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01299 to 0.01285, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00028: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17490    99]\n",
            " [  334 11194]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.985128962461792\n",
            "Averaged F1: 0.9851017369243829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     17589\n",
            "           1       0.99      0.97      0.98     11528\n",
            "\n",
            "    accuracy                           0.99     29117\n",
            "   macro avg       0.99      0.98      0.98     29117\n",
            "weighted avg       0.99      0.99      0.99     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnrpQVHVry-v"
      },
      "source": [
        "## Activation = Sigmoid, Optimizer = sgd\n",
        "## Layer 1 : 100 neurons\n",
        "## Layer 2:  500 neurons\n",
        "2 mins\n",
        "\n",
        "Accuracy: 0.9715630044304014\n",
        "\n",
        "Averaged F1: 0.9713849954645342\n",
        "\n",
        "\n"
      ],
      "id": "bnrpQVHVry-v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G1IXUW0Xry-1",
        "outputId": "5e70d21d-a073-496d-b00a-82bfb7a0e6a8"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='sigmoid'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "G1IXUW0Xry-1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1716 - accuracy: 0.7973 - val_loss: 0.1119 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.11191, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0765 - accuracy: 0.9655 - val_loss: 0.0519 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.11191 to 0.05186, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0440 - accuracy: 0.9680 - val_loss: 0.0397 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.05186 to 0.03969, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0381 - accuracy: 0.9680 - val_loss: 0.0369 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03969 to 0.03689, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0360 - accuracy: 0.9680 - val_loss: 0.0353 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.03689 to 0.03529, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0346 - accuracy: 0.9680 - val_loss: 0.0341 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.03529 to 0.03414, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0337 - accuracy: 0.9680 - val_loss: 0.0335 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.03414 to 0.03350, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0330 - accuracy: 0.9681 - val_loss: 0.0328 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.03350 to 0.03284, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0324 - accuracy: 0.9681 - val_loss: 0.0323 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.03284 to 0.03226, saving model to dnn/best_weights.hdf5\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0319 - accuracy: 0.9682 - val_loss: 0.0318 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.03226 to 0.03176, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0315 - accuracy: 0.9683 - val_loss: 0.0313 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.03176 to 0.03133, saving model to dnn/best_weights.hdf5\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0311 - accuracy: 0.9682 - val_loss: 0.0310 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.03133 to 0.03096, saving model to dnn/best_weights.hdf5\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0307 - accuracy: 0.9682 - val_loss: 0.0306 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.03096 to 0.03058, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0303 - accuracy: 0.9683 - val_loss: 0.0303 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.03058 to 0.03025, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0300 - accuracy: 0.9683 - val_loss: 0.0299 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.03025 to 0.02994, saving model to dnn/best_weights.hdf5\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0297 - accuracy: 0.9683 - val_loss: 0.0296 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.02994 to 0.02958, saving model to dnn/best_weights.hdf5\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0293 - accuracy: 0.9683 - val_loss: 0.0292 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.02958 to 0.02924, saving model to dnn/best_weights.hdf5\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0290 - accuracy: 0.9684 - val_loss: 0.0292 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02924\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0287 - accuracy: 0.9684 - val_loss: 0.0287 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.02924 to 0.02868, saving model to dnn/best_weights.hdf5\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0284 - accuracy: 0.9684 - val_loss: 0.0285 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.02868 to 0.02849, saving model to dnn/best_weights.hdf5\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0282 - accuracy: 0.9685 - val_loss: 0.0281 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.02849 to 0.02805, saving model to dnn/best_weights.hdf5\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0279 - accuracy: 0.9685 - val_loss: 0.0278 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.02805 to 0.02780, saving model to dnn/best_weights.hdf5\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0276 - accuracy: 0.9685 - val_loss: 0.0275 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.02780 to 0.02750, saving model to dnn/best_weights.hdf5\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0274 - accuracy: 0.9686 - val_loss: 0.0274 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.02750 to 0.02736, saving model to dnn/best_weights.hdf5\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0271 - accuracy: 0.9688 - val_loss: 0.0270 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.02736 to 0.02699, saving model to dnn/best_weights.hdf5\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0269 - accuracy: 0.9689 - val_loss: 0.0267 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.02699 to 0.02673, saving model to dnn/best_weights.hdf5\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0266 - accuracy: 0.9690 - val_loss: 0.0265 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.02673 to 0.02650, saving model to dnn/best_weights.hdf5\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0264 - accuracy: 0.9691 - val_loss: 0.0263 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.02650 to 0.02626, saving model to dnn/best_weights.hdf5\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0261 - accuracy: 0.9692 - val_loss: 0.0261 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.02626 to 0.02610, saving model to dnn/best_weights.hdf5\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0259 - accuracy: 0.9692 - val_loss: 0.0259 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.02610 to 0.02586, saving model to dnn/best_weights.hdf5\n",
            "Epoch 31/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9693 - val_loss: 0.0257 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.02586 to 0.02565, saving model to dnn/best_weights.hdf5\n",
            "Epoch 32/200\n",
            "167/167 - 1s - loss: 0.0255 - accuracy: 0.9694 - val_loss: 0.0253 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.02565 to 0.02531, saving model to dnn/best_weights.hdf5\n",
            "Epoch 33/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9695 - val_loss: 0.0252 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.02531 to 0.02520, saving model to dnn/best_weights.hdf5\n",
            "Epoch 34/200\n",
            "167/167 - 1s - loss: 0.0250 - accuracy: 0.9698 - val_loss: 0.0249 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.02520 to 0.02488, saving model to dnn/best_weights.hdf5\n",
            "Epoch 35/200\n",
            "167/167 - 1s - loss: 0.0248 - accuracy: 0.9699 - val_loss: 0.0247 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.02488 to 0.02471, saving model to dnn/best_weights.hdf5\n",
            "Epoch 36/200\n",
            "167/167 - 1s - loss: 0.0246 - accuracy: 0.9701 - val_loss: 0.0245 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.02471 to 0.02453, saving model to dnn/best_weights.hdf5\n",
            "Epoch 37/200\n",
            "167/167 - 1s - loss: 0.0244 - accuracy: 0.9705 - val_loss: 0.0243 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.02453 to 0.02430, saving model to dnn/best_weights.hdf5\n",
            "Epoch 38/200\n",
            "167/167 - 1s - loss: 0.0242 - accuracy: 0.9707 - val_loss: 0.0241 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.02430 to 0.02406, saving model to dnn/best_weights.hdf5\n",
            "Epoch 39/200\n",
            "167/167 - 1s - loss: 0.0240 - accuracy: 0.9708 - val_loss: 0.0239 - val_accuracy: 0.9705\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.02406 to 0.02389, saving model to dnn/best_weights.hdf5\n",
            "Epoch 40/200\n",
            "167/167 - 1s - loss: 0.0238 - accuracy: 0.9710 - val_loss: 0.0237 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.02389 to 0.02368, saving model to dnn/best_weights.hdf5\n",
            "Epoch 41/200\n",
            "167/167 - 1s - loss: 0.0237 - accuracy: 0.9713 - val_loss: 0.0236 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.02368 to 0.02363, saving model to dnn/best_weights.hdf5\n",
            "Epoch 42/200\n",
            "167/167 - 1s - loss: 0.0235 - accuracy: 0.9714 - val_loss: 0.0234 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.02363 to 0.02338, saving model to dnn/best_weights.hdf5\n",
            "Epoch 43/200\n",
            "167/167 - 1s - loss: 0.0233 - accuracy: 0.9715 - val_loss: 0.0232 - val_accuracy: 0.9710\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.02338 to 0.02315, saving model to dnn/best_weights.hdf5\n",
            "Epoch 44/200\n",
            "167/167 - 1s - loss: 0.0232 - accuracy: 0.9716 - val_loss: 0.0230 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.02315 to 0.02297, saving model to dnn/best_weights.hdf5\n",
            "Epoch 45/200\n",
            "167/167 - 1s - loss: 0.0230 - accuracy: 0.9718 - val_loss: 0.0228 - val_accuracy: 0.9716\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.02297 to 0.02281, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00045: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 14s - loss: 0.1697 - accuracy: 0.7835 - val_loss: 0.1093 - val_accuracy: 0.9552\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02281\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0744 - accuracy: 0.9631 - val_loss: 0.0507 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02281\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0437 - accuracy: 0.9674 - val_loss: 0.0397 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02281\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0386 - accuracy: 0.9679 - val_loss: 0.0375 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02281\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0369 - accuracy: 0.9681 - val_loss: 0.0363 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02281\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0358 - accuracy: 0.9681 - val_loss: 0.0351 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02281\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0348 - accuracy: 0.9681 - val_loss: 0.0342 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02281\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0341 - accuracy: 0.9681 - val_loss: 0.0337 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.02281\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0336 - accuracy: 0.9682 - val_loss: 0.0333 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.02281\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0331 - accuracy: 0.9681 - val_loss: 0.0327 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02281\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0326 - accuracy: 0.9682 - val_loss: 0.0324 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02281\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0322 - accuracy: 0.9682 - val_loss: 0.0319 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02281\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0318 - accuracy: 0.9682 - val_loss: 0.0315 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02281\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0314 - accuracy: 0.9682 - val_loss: 0.0312 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02281\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0311 - accuracy: 0.9682 - val_loss: 0.0308 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02281\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0307 - accuracy: 0.9682 - val_loss: 0.0304 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02281\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0304 - accuracy: 0.9681 - val_loss: 0.0301 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02281\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0301 - accuracy: 0.9681 - val_loss: 0.0298 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02281\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0297 - accuracy: 0.9681 - val_loss: 0.0295 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02281\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0294 - accuracy: 0.9681 - val_loss: 0.0292 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.02281\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0291 - accuracy: 0.9681 - val_loss: 0.0289 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02281\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0288 - accuracy: 0.9681 - val_loss: 0.0287 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02281\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0286 - accuracy: 0.9681 - val_loss: 0.0286 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02281\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0283 - accuracy: 0.9682 - val_loss: 0.0280 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02281\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0280 - accuracy: 0.9682 - val_loss: 0.0279 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02281\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0277 - accuracy: 0.9682 - val_loss: 0.0275 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02281\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0275 - accuracy: 0.9682 - val_loss: 0.0272 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02281\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0272 - accuracy: 0.9682 - val_loss: 0.0270 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02281\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0270 - accuracy: 0.9682 - val_loss: 0.0267 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.02281\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0267 - accuracy: 0.9683 - val_loss: 0.0265 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02281\n",
            "Epoch 31/200\n",
            "167/167 - 1s - loss: 0.0265 - accuracy: 0.9683 - val_loss: 0.0263 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02281\n",
            "Epoch 32/200\n",
            "167/167 - 1s - loss: 0.0263 - accuracy: 0.9683 - val_loss: 0.0260 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02281\n",
            "Epoch 33/200\n",
            "167/167 - 1s - loss: 0.0260 - accuracy: 0.9683 - val_loss: 0.0260 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.02281\n",
            "Epoch 34/200\n",
            "167/167 - 1s - loss: 0.0258 - accuracy: 0.9683 - val_loss: 0.0256 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.02281\n",
            "Epoch 35/200\n",
            "167/167 - 1s - loss: 0.0256 - accuracy: 0.9684 - val_loss: 0.0253 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.02281\n",
            "Epoch 36/200\n",
            "167/167 - 1s - loss: 0.0254 - accuracy: 0.9684 - val_loss: 0.0251 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.02281\n",
            "Epoch 37/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9685 - val_loss: 0.0251 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.02281\n",
            "Epoch 38/200\n",
            "167/167 - 1s - loss: 0.0250 - accuracy: 0.9684 - val_loss: 0.0248 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.02281\n",
            "Epoch 39/200\n",
            "167/167 - 1s - loss: 0.0248 - accuracy: 0.9686 - val_loss: 0.0245 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.02281\n",
            "Epoch 40/200\n",
            "167/167 - 1s - loss: 0.0246 - accuracy: 0.9688 - val_loss: 0.0243 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.02281\n",
            "Epoch 41/200\n",
            "167/167 - 1s - loss: 0.0244 - accuracy: 0.9693 - val_loss: 0.0241 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.02281\n",
            "Epoch 42/200\n",
            "167/167 - 1s - loss: 0.0242 - accuracy: 0.9696 - val_loss: 0.0239 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.02281\n",
            "Epoch 43/200\n",
            "167/167 - 1s - loss: 0.0240 - accuracy: 0.9698 - val_loss: 0.0239 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.02281\n",
            "Epoch 44/200\n",
            "167/167 - 1s - loss: 0.0238 - accuracy: 0.9700 - val_loss: 0.0235 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.02281\n",
            "Epoch 45/200\n",
            "167/167 - 1s - loss: 0.0236 - accuracy: 0.9701 - val_loss: 0.0234 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.02281\n",
            "Epoch 46/200\n",
            "167/167 - 1s - loss: 0.0235 - accuracy: 0.9701 - val_loss: 0.0232 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.02281\n",
            "Epoch 00046: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1815 - accuracy: 0.7614 - val_loss: 0.1303 - val_accuracy: 0.8927\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02281\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0918 - accuracy: 0.9277 - val_loss: 0.0613 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02281\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0489 - accuracy: 0.9667 - val_loss: 0.0412 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02281\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0390 - accuracy: 0.9684 - val_loss: 0.0369 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02281\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0359 - accuracy: 0.9685 - val_loss: 0.0349 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02281\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0341 - accuracy: 0.9685 - val_loss: 0.0334 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02281\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0329 - accuracy: 0.9687 - val_loss: 0.0324 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02281\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0321 - accuracy: 0.9687 - val_loss: 0.0316 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.02281\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0314 - accuracy: 0.9688 - val_loss: 0.0312 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.02281\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0309 - accuracy: 0.9690 - val_loss: 0.0306 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02281\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0304 - accuracy: 0.9690 - val_loss: 0.0302 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02281\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0300 - accuracy: 0.9690 - val_loss: 0.0298 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02281\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0296 - accuracy: 0.9690 - val_loss: 0.0294 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02281\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0292 - accuracy: 0.9690 - val_loss: 0.0290 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02281\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0289 - accuracy: 0.9690 - val_loss: 0.0288 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02281\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0285 - accuracy: 0.9690 - val_loss: 0.0283 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02281\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0282 - accuracy: 0.9690 - val_loss: 0.0280 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02281\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0279 - accuracy: 0.9690 - val_loss: 0.0277 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02281\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0276 - accuracy: 0.9690 - val_loss: 0.0274 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02281\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0273 - accuracy: 0.9690 - val_loss: 0.0273 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.02281\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0270 - accuracy: 0.9690 - val_loss: 0.0268 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02281\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0267 - accuracy: 0.9690 - val_loss: 0.0266 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02281\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0265 - accuracy: 0.9692 - val_loss: 0.0263 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02281\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0262 - accuracy: 0.9693 - val_loss: 0.0261 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02281\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0259 - accuracy: 0.9693 - val_loss: 0.0258 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02281\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9694 - val_loss: 0.0255 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02281\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0254 - accuracy: 0.9696 - val_loss: 0.0253 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02281\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9697 - val_loss: 0.0251 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02281\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0250 - accuracy: 0.9700 - val_loss: 0.0248 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.02281\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0247 - accuracy: 0.9704 - val_loss: 0.0246 - val_accuracy: 0.9710\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02281\n",
            "Epoch 31/200\n",
            "167/167 - 1s - loss: 0.0245 - accuracy: 0.9710 - val_loss: 0.0243 - val_accuracy: 0.9711\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02281\n",
            "Epoch 32/200\n",
            "167/167 - 1s - loss: 0.0243 - accuracy: 0.9713 - val_loss: 0.0241 - val_accuracy: 0.9711\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02281\n",
            "Epoch 33/200\n",
            "167/167 - 1s - loss: 0.0241 - accuracy: 0.9715 - val_loss: 0.0239 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.02281\n",
            "Epoch 34/200\n",
            "167/167 - 1s - loss: 0.0239 - accuracy: 0.9717 - val_loss: 0.0237 - val_accuracy: 0.9716\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.02281\n",
            "Epoch 35/200\n",
            "167/167 - 1s - loss: 0.0237 - accuracy: 0.9719 - val_loss: 0.0236 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.02281\n",
            "Epoch 00035: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1878 - accuracy: 0.7552 - val_loss: 0.1296 - val_accuracy: 0.9605\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02281\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0884 - accuracy: 0.9656 - val_loss: 0.0565 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02281\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0442 - accuracy: 0.9675 - val_loss: 0.0374 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02281\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0360 - accuracy: 0.9678 - val_loss: 0.0348 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02281\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0345 - accuracy: 0.9679 - val_loss: 0.0339 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02281\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0336 - accuracy: 0.9681 - val_loss: 0.0331 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02281\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0329 - accuracy: 0.9681 - val_loss: 0.0325 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02281\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0324 - accuracy: 0.9682 - val_loss: 0.0321 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.02281\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0320 - accuracy: 0.9683 - val_loss: 0.0317 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.02281\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0316 - accuracy: 0.9684 - val_loss: 0.0313 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02281\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0312 - accuracy: 0.9685 - val_loss: 0.0309 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02281\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0309 - accuracy: 0.9686 - val_loss: 0.0307 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02281\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0305 - accuracy: 0.9687 - val_loss: 0.0302 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02281\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0302 - accuracy: 0.9688 - val_loss: 0.0300 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02281\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0299 - accuracy: 0.9689 - val_loss: 0.0297 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02281\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0296 - accuracy: 0.9689 - val_loss: 0.0294 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02281\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0293 - accuracy: 0.9691 - val_loss: 0.0291 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02281\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0291 - accuracy: 0.9692 - val_loss: 0.0288 - val_accuracy: 0.9699\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02281\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0288 - accuracy: 0.9696 - val_loss: 0.0285 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02281\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0285 - accuracy: 0.9698 - val_loss: 0.0283 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.02281\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0283 - accuracy: 0.9702 - val_loss: 0.0281 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02281\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0280 - accuracy: 0.9705 - val_loss: 0.0278 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02281\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0278 - accuracy: 0.9707 - val_loss: 0.0275 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02281\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0276 - accuracy: 0.9708 - val_loss: 0.0273 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02281\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0273 - accuracy: 0.9709 - val_loss: 0.0271 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02281\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0271 - accuracy: 0.9711 - val_loss: 0.0268 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02281\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0269 - accuracy: 0.9711 - val_loss: 0.0267 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02281\n",
            "Epoch 28/200\n",
            "167/167 - 1s - loss: 0.0267 - accuracy: 0.9712 - val_loss: 0.0264 - val_accuracy: 0.9710\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02281\n",
            "Epoch 29/200\n",
            "167/167 - 1s - loss: 0.0265 - accuracy: 0.9714 - val_loss: 0.0264 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.02281\n",
            "Epoch 30/200\n",
            "167/167 - 1s - loss: 0.0263 - accuracy: 0.9715 - val_loss: 0.0261 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02281\n",
            "Epoch 31/200\n",
            "167/167 - 1s - loss: 0.0261 - accuracy: 0.9716 - val_loss: 0.0258 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02281\n",
            "Epoch 32/200\n",
            "167/167 - 1s - loss: 0.0259 - accuracy: 0.9717 - val_loss: 0.0256 - val_accuracy: 0.9719\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02281\n",
            "Epoch 33/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9718 - val_loss: 0.0255 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.02281\n",
            "Epoch 34/200\n",
            "167/167 - 1s - loss: 0.0255 - accuracy: 0.9719 - val_loss: 0.0253 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.02281\n",
            "Epoch 35/200\n",
            "167/167 - 1s - loss: 0.0253 - accuracy: 0.9720 - val_loss: 0.0251 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.02281\n",
            "Epoch 36/200\n",
            "167/167 - 1s - loss: 0.0252 - accuracy: 0.9721 - val_loss: 0.0249 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.02281\n",
            "Epoch 37/200\n",
            "167/167 - 1s - loss: 0.0250 - accuracy: 0.9722 - val_loss: 0.0248 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.02281\n",
            "Epoch 38/200\n",
            "167/167 - 1s - loss: 0.0248 - accuracy: 0.9723 - val_loss: 0.0245 - val_accuracy: 0.9724\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.02281\n",
            "Epoch 39/200\n",
            "167/167 - 1s - loss: 0.0246 - accuracy: 0.9724 - val_loss: 0.0244 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.02281\n",
            "Epoch 40/200\n",
            "167/167 - 1s - loss: 0.0245 - accuracy: 0.9725 - val_loss: 0.0242 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.02281\n",
            "Epoch 00040: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.1642 - accuracy: 0.8103 - val_loss: 0.1019 - val_accuracy: 0.9565\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02281\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0683 - accuracy: 0.9661 - val_loss: 0.0469 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02281\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0410 - accuracy: 0.9680 - val_loss: 0.0378 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02281\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0369 - accuracy: 0.9684 - val_loss: 0.0359 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02281\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0354 - accuracy: 0.9684 - val_loss: 0.0347 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02281\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0344 - accuracy: 0.9685 - val_loss: 0.0339 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02281\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0336 - accuracy: 0.9685 - val_loss: 0.0334 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02281\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0331 - accuracy: 0.9684 - val_loss: 0.0328 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.02281\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0326 - accuracy: 0.9684 - val_loss: 0.0323 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.02281\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0322 - accuracy: 0.9684 - val_loss: 0.0320 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02281\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0319 - accuracy: 0.9685 - val_loss: 0.0316 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02281\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0316 - accuracy: 0.9685 - val_loss: 0.0313 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02281\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0313 - accuracy: 0.9685 - val_loss: 0.0310 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02281\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0310 - accuracy: 0.9685 - val_loss: 0.0308 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02281\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0307 - accuracy: 0.9685 - val_loss: 0.0305 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02281\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0305 - accuracy: 0.9685 - val_loss: 0.0305 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02281\n",
            "Epoch 17/200\n",
            "167/167 - 1s - loss: 0.0302 - accuracy: 0.9685 - val_loss: 0.0300 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02281\n",
            "Epoch 18/200\n",
            "167/167 - 1s - loss: 0.0300 - accuracy: 0.9685 - val_loss: 0.0299 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02281\n",
            "Epoch 19/200\n",
            "167/167 - 1s - loss: 0.0297 - accuracy: 0.9685 - val_loss: 0.0295 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02281\n",
            "Epoch 20/200\n",
            "167/167 - 1s - loss: 0.0295 - accuracy: 0.9685 - val_loss: 0.0293 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.02281\n",
            "Epoch 21/200\n",
            "167/167 - 1s - loss: 0.0292 - accuracy: 0.9685 - val_loss: 0.0291 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02281\n",
            "Epoch 22/200\n",
            "167/167 - 1s - loss: 0.0290 - accuracy: 0.9686 - val_loss: 0.0288 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02281\n",
            "Epoch 23/200\n",
            "167/167 - 1s - loss: 0.0288 - accuracy: 0.9685 - val_loss: 0.0285 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02281\n",
            "Epoch 24/200\n",
            "167/167 - 1s - loss: 0.0286 - accuracy: 0.9686 - val_loss: 0.0284 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02281\n",
            "Epoch 25/200\n",
            "167/167 - 1s - loss: 0.0283 - accuracy: 0.9685 - val_loss: 0.0281 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02281\n",
            "Epoch 26/200\n",
            "167/167 - 1s - loss: 0.0281 - accuracy: 0.9685 - val_loss: 0.0279 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02281\n",
            "Epoch 27/200\n",
            "167/167 - 1s - loss: 0.0279 - accuracy: 0.9686 - val_loss: 0.0278 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02281\n",
            "Epoch 00027: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17544    45]\n",
            " [  783 10745]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9715630044304014\n",
            "Averaged F1: 0.9713849954645342\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     17589\n",
            "           1       1.00      0.93      0.96     11528\n",
            "\n",
            "    accuracy                           0.97     29117\n",
            "   macro avg       0.98      0.96      0.97     29117\n",
            "weighted avg       0.97      0.97      0.97     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5QtM0Kt0V4O"
      },
      "source": [
        "# Testing on differnet numbers of layer and neuron\n",
        "activation = relu, optimizer = adam\n",
        "\n",
        "100 neuroins on first layer, 500 neuroins on second layer"
      ],
      "id": "P5QtM0Kt0V4O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJzwzfeP09WF"
      },
      "source": [
        "### 10 neurons per layer, add 1 more hidden layers\n",
        "41s\n",
        "\n",
        "Accuracy: 0.9986605762956349\n",
        "Averaged F1: 0.998660606307565"
      ],
      "id": "NJzwzfeP09WF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8BcHbQGU0v9_",
        "outputId": "4cbe52cc-2ab3-4ed7-a6da-508c1201b202"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "8BcHbQGU0v9_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0249 - accuracy: 0.9752 - val_loss: 0.0079 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00786, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0062 - accuracy: 0.9919 - val_loss: 0.0044 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00786 to 0.00444, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0036 - accuracy: 0.9963 - val_loss: 0.0026 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00444 to 0.00264, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0022 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00264 to 0.00183, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00183 to 0.00148, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00148 to 0.00144, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00144 to 0.00133, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00133 to 0.00133, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00133\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00133\n",
            "Epoch 00010: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0291 - accuracy: 0.9577 - val_loss: 0.0074 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00133\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0060 - accuracy: 0.9923 - val_loss: 0.0041 - val_accuracy: 0.9947\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00133\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0031 - accuracy: 0.9971 - val_loss: 0.0021 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00133\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9981 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00133\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00133\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00133\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00133\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00133 to 0.00130, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00008: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0256 - accuracy: 0.9788 - val_loss: 0.0074 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00130\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0058 - accuracy: 0.9927 - val_loss: 0.0042 - val_accuracy: 0.9946\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00130\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0031 - accuracy: 0.9975 - val_loss: 0.0021 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00130\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00130\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00130\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0016 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00130\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00130\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00130\n",
            "Epoch 00008: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0269 - accuracy: 0.9776 - val_loss: 0.0071 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00130\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0060 - accuracy: 0.9922 - val_loss: 0.0045 - val_accuracy: 0.9940\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00130\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9956 - val_loss: 0.0030 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00130\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0024 - accuracy: 0.9981 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00130\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00130\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00130\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00130\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00130\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00130 to 0.00130, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00009: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0274 - accuracy: 0.9809 - val_loss: 0.0074 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00130\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0063 - accuracy: 0.9920 - val_loss: 0.0050 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00130\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0042 - accuracy: 0.9953 - val_loss: 0.0030 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00130\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0025 - accuracy: 0.9981 - val_loss: 0.0020 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00130\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00130\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00130\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00130\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00130\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00130\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00130\n",
            "Epoch 00010: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17568    21]\n",
            " [   18 11510]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9986605762956349\n",
            "Averaged F1: 0.998660606307565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5d3Mexz1VIc"
      },
      "source": [
        "### 100 neurons per layer, add 1 more hidden layers\n",
        "43s\n",
        "\n",
        "Accuracy: 0.9987979530858262\n",
        "Averaged F1: 0.9987979081315314"
      ],
      "id": "d5d3Mexz1VIc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sESGeOj92MBn",
        "outputId": "be31c8b9-5b2a-4d7b-85bd-2c188aaa7524"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "model.summary()\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "sESGeOj92MBn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0201 - accuracy: 0.9847 - val_loss: 0.0063 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00632, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0051 - accuracy: 0.9939 - val_loss: 0.0037 - val_accuracy: 0.9963\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00632 to 0.00372, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0027 - accuracy: 0.9978 - val_loss: 0.0018 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00372 to 0.00184, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00184 to 0.00159, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00159 to 0.00146, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00146 to 0.00145, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00145 to 0.00132, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00132 to 0.00129, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00008: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0233 - accuracy: 0.9782 - val_loss: 0.0069 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00129\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0055 - accuracy: 0.9931 - val_loss: 0.0039 - val_accuracy: 0.9959\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00129\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9974 - val_loss: 0.0028 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00129\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00129\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00129\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0019 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00129\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00129 to 0.00128, saving model to dnn/best_weights.hdf5\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00128\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00128\n",
            "Epoch 00009: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0201 - accuracy: 0.9828 - val_loss: 0.0065 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00128\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0055 - accuracy: 0.9930 - val_loss: 0.0037 - val_accuracy: 0.9965\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00128\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9976 - val_loss: 0.0025 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00128\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00128\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00128\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00128\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00128\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00128\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00128\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00128 to 0.00121, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9989 - val_loss: 0.0012 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00121\n",
            "Epoch 00011: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0202 - accuracy: 0.9848 - val_loss: 0.0065 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00121\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0050 - accuracy: 0.9939 - val_loss: 0.0032 - val_accuracy: 0.9966\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00121\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0025 - accuracy: 0.9979 - val_loss: 0.0019 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00121\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00121\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00121\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00121\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00121\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00121\n",
            "Epoch 00008: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 2s - loss: 0.0201 - accuracy: 0.9821 - val_loss: 0.0060 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00121\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0048 - accuracy: 0.9941 - val_loss: 0.0033 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00121\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0025 - accuracy: 0.9978 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00121\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00121\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0018 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00121\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00121\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00121\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00121\n",
            "Epoch 00008: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_225 (Dense)            (None, 100)               11700     \n",
            "_________________________________________________________________\n",
            "dense_226 (Dense)            (None, 500)               50500     \n",
            "_________________________________________________________________\n",
            "dense_227 (Dense)            (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "dense_228 (Dense)            (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 112,502\n",
            "Trainable params: 112,502\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[17574    15]\n",
            " [   20 11508]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987979530858262\n",
            "Averaged F1: 0.9987979081315314\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIlQ9GJq1Veb"
      },
      "source": [
        "### 500 neurons per layer, add 1 more hidden layers\n",
        "41s\n",
        "\n",
        "Accuracy: 0.9987292646907305\n",
        "Averaged F1: 0.9987293310795642"
      ],
      "id": "FIlQ9GJq1Veb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cPoX9nGG2LMw",
        "outputId": "08d1aae3-95e7-4258-d9e7-b85c97c2a0aa"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "cPoX9nGG2LMw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0168 - accuracy: 0.9836 - val_loss: 0.0054 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00537, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9959 - val_loss: 0.0022 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00537 to 0.00222, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00222 to 0.00177, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00177 to 0.00155, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00155 to 0.00139, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00139\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00139 to 0.00133, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00007: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0186 - accuracy: 0.9821 - val_loss: 0.0060 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00133\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9959 - val_loss: 0.0023 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00133\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00133\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00133\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00133\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00133 to 0.00133, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0019 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00133\n",
            "Epoch 00007: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0169 - accuracy: 0.9836 - val_loss: 0.0056 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00133\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0043 - accuracy: 0.9950 - val_loss: 0.0026 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00133\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9980 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00133\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00133\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00133\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0019 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00133\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00133\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00133 to 0.00115, saving model to dnn/best_weights.hdf5\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00115\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9989 - val_loss: 0.0011 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00115 to 0.00114, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00010: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0180 - accuracy: 0.9851 - val_loss: 0.0058 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00114\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0046 - accuracy: 0.9947 - val_loss: 0.0028 - val_accuracy: 0.9974\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00114\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9979 - val_loss: 0.0021 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00114\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00114\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00114\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00114\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00114\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00114\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9989 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00114\n",
            "Epoch 00009: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0190 - accuracy: 0.9837 - val_loss: 0.0060 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00114\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0044 - accuracy: 0.9953 - val_loss: 0.0025 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00114\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9982 - val_loss: 0.0020 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00114\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00114\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00114\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00114\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00114\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00114\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00114\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9989 - val_loss: 0.0012 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00114\n",
            "Epoch 00010: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17567    22]\n",
            " [   15 11513]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987292646907305\n",
            "Averaged F1: 0.9987293310795642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0iwN2pj1Vo0"
      },
      "source": [
        "### 10 neurons per layer, add 2 more hidden layers\n",
        "57s\n",
        "\n",
        "Accuracy: 0.9987292646907305\n",
        "Averaged F1: 0.998729217167619\n"
      ],
      "id": "E0iwN2pj1Vo0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yzHAyGWY2KB_",
        "outputId": "94dced85-696b-46e0-e9f8-b2df5912bdd4"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "yzHAyGWY2KB_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0239 - accuracy: 0.9808 - val_loss: 0.0076 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00757, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0058 - accuracy: 0.9927 - val_loss: 0.0034 - val_accuracy: 0.9961\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00757 to 0.00340, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0028 - accuracy: 0.9975 - val_loss: 0.0019 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00340 to 0.00192, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9979 - val_loss: 0.0017 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00192 to 0.00169, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.0019 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00169\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00169 to 0.00148, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0016 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00148\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00148 to 0.00128, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00008: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0324 - accuracy: 0.9574 - val_loss: 0.0069 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00128\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0057 - accuracy: 0.9929 - val_loss: 0.0040 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00128\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0034 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00128\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9981 - val_loss: 0.0017 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00128\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00128\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00128\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0013 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00128\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00128\n",
            "Epoch 00008: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0327 - accuracy: 0.9799 - val_loss: 0.0075 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00128\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0062 - accuracy: 0.9921 - val_loss: 0.0050 - val_accuracy: 0.9957\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00128\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0024 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00128\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0023 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00128\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9984 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00128\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00128\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00128\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00128\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00128\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00128\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9989 - val_loss: 0.0014 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00128\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00128 to 0.00126, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00012: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.2432 - accuracy: 0.9473 - val_loss: 0.1515 - val_accuracy: 0.9777\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00126\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.1325 - accuracy: 0.9853 - val_loss: 0.1127 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00126\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0962 - accuracy: 0.9941 - val_loss: 0.0798 - val_accuracy: 0.9973\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00126\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0670 - accuracy: 0.9978 - val_loss: 0.0547 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00126\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0452 - accuracy: 0.9981 - val_loss: 0.0362 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00126\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0294 - accuracy: 0.9984 - val_loss: 0.0230 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00126\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0184 - accuracy: 0.9985 - val_loss: 0.0142 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00126\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0086 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00126\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00126\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0034 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00126\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0026 - accuracy: 0.9987 - val_loss: 0.0020 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00126\n",
            "Epoch 12/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9987 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00126\n",
            "Epoch 13/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00126 to 0.00124, saving model to dnn/best_weights.hdf5\n",
            "Epoch 14/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9989 - val_loss: 0.0012 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00124 to 0.00117, saving model to dnn/best_weights.hdf5\n",
            "Epoch 15/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00117\n",
            "Epoch 16/200\n",
            "167/167 - 1s - loss: 0.0011 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00117\n",
            "Epoch 00016: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0396 - accuracy: 0.9708 - val_loss: 0.0083 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00117\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0060 - accuracy: 0.9923 - val_loss: 0.0036 - val_accuracy: 0.9957\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00117\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0029 - accuracy: 0.9971 - val_loss: 0.0020 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00117\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00117\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9981 - val_loss: 0.0018 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00117\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00117\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9984 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00117\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00117\n",
            "Epoch 00008: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17573    16]\n",
            " [   21 11507]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9987292646907305\n",
            "Averaged F1: 0.998729217167619\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-A5ISoj2RM1"
      },
      "source": [
        "### 100 neurons per layer, add 2 more hidden layers\n",
        "31s\n",
        "\n",
        "Accuracy: 0.9985575437029914\n",
        "Averaged F1: 0.9985575437029914"
      ],
      "id": "Y-A5ISoj2RM1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R-iyHu0m2RM1",
        "outputId": "e66361ac-9f96-4edf-c1f5-04e5d355d76c"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "R-iyHu0m2RM1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0203 - accuracy: 0.9819 - val_loss: 0.0059 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00588, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0043 - accuracy: 0.9951 - val_loss: 0.0024 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00588 to 0.00237, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9981 - val_loss: 0.0017 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00237 to 0.00165, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00165 to 0.00163, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00163 to 0.00149, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00149\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00149\n",
            "Epoch 00007: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0253 - accuracy: 0.9774 - val_loss: 0.0059 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00149\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0041 - accuracy: 0.9956 - val_loss: 0.0022 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00149\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9980 - val_loss: 0.0019 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00149\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00149\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00149\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00149\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00149 to 0.00130, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00007: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0217 - accuracy: 0.9780 - val_loss: 0.0054 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00130\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0038 - accuracy: 0.9961 - val_loss: 0.0021 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00130\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9979 - val_loss: 0.0020 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00130\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00130\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00130 to 0.00128, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00128\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00128\n",
            "Epoch 00007: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0213 - accuracy: 0.9749 - val_loss: 0.0049 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00128\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0034 - accuracy: 0.9967 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00128\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9981 - val_loss: 0.0016 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00128\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00128\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00128\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00128\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0012 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00128 to 0.00120, saving model to dnn/best_weights.hdf5\n",
            "Epoch 00007: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0257 - accuracy: 0.9799 - val_loss: 0.0063 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00120\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0044 - accuracy: 0.9951 - val_loss: 0.0029 - val_accuracy: 0.9971\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00120\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9981 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00120\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00120\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0012 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00120\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00120\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00120\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00120\n",
            "Epoch 00008: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17568    21]\n",
            " [   21 11507]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985575437029914\n",
            "Averaged F1: 0.9985575437029914\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4pMVRoW1VwU"
      },
      "source": [
        "### 500 neurons per layer, add 2 more hidden layers\n",
        "54s\n",
        "\n",
        "Accuracy: 0.9985231995054435\n",
        "Averaged F1: 0.9985232105394404"
      ],
      "id": "V4pMVRoW1VwU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0NWXRfd21kBr",
        "outputId": "2e2b06e1-34b3-4d7e-caf9-c36a7e0f6c41"
      },
      "source": [
        "# Define ModelCheckpoint outside the loop\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=700, epochs=200, verbose=2, callbacks=[monitor, checkpointer], validation_data=(x_test, y_test))\n",
        "    # model.summary()\n",
        "\n",
        "print('Training finished...Loading the best model')  \n",
        "print()\n",
        "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
        "\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "print(cm)\n",
        "\n",
        "print('Plotting confusion matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "score = metrics.accuracy_score(y_true, pred)\n",
        "print('Accuracy: {}'.format(score))\n",
        "\n",
        "\n",
        "f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "print('Averaged F1: {}'.format(f1))\n",
        "\n",
        "           \n",
        "print(metrics.classification_report(y_true, pred))"
      ],
      "id": "0NWXRfd21kBr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "167/167 - 2s - loss: 0.0160 - accuracy: 0.9845 - val_loss: 0.0057 - val_accuracy: 0.9945\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00565, saving model to dnn/best_weights.hdf5\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0033 - accuracy: 0.9967 - val_loss: 0.0022 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00565 to 0.00224, saving model to dnn/best_weights.hdf5\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00224 to 0.00148, saving model to dnn/best_weights.hdf5\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0014 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00148 to 0.00144, saving model to dnn/best_weights.hdf5\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0013 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00144 to 0.00126, saving model to dnn/best_weights.hdf5\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00126 to 0.00121, saving model to dnn/best_weights.hdf5\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00121\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0022 - val_accuracy: 0.9968\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00121\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00121\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00121 to 0.00116, saving model to dnn/best_weights.hdf5\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0033 - accuracy: 0.9962 - val_loss: 0.0047 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00116\n",
            "Epoch 00011: early stopping\n",
            "1\n",
            "Epoch 1/200\n",
            "167/167 - 2s - loss: 0.0195 - accuracy: 0.9840 - val_loss: 0.0059 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00116\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9960 - val_loss: 0.0024 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00116\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0019 - accuracy: 0.9981 - val_loss: 0.0015 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00116\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00116\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00116\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00116\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00116\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9987 - val_loss: 0.0017 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00116\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0033 - accuracy: 0.9966 - val_loss: 0.0019 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00116\n",
            "Epoch 10/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00116\n",
            "Epoch 11/200\n",
            "167/167 - 1s - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00116\n",
            "Epoch 00011: early stopping\n",
            "2\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0155 - accuracy: 0.9855 - val_loss: 0.0043 - val_accuracy: 0.9947\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00116\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0030 - accuracy: 0.9971 - val_loss: 0.0037 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00116\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0021 - accuracy: 0.9980 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00116\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00116\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00116\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00116\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00116\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.0017 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00116\n",
            "Epoch 00008: early stopping\n",
            "3\n",
            "Epoch 1/200\n",
            "167/167 - 2s - loss: 0.0163 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00116\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0039 - accuracy: 0.9958 - val_loss: 0.0024 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00116\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00116\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0017 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00116\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0015 - accuracy: 0.9985 - val_loss: 0.0017 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00116\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00116\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0020 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00116\n",
            "Epoch 8/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0016 - val_accuracy: 0.9984\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00116\n",
            "Epoch 9/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00116\n",
            "Epoch 00009: early stopping\n",
            "4\n",
            "Epoch 1/200\n",
            "167/167 - 1s - loss: 0.0172 - accuracy: 0.9841 - val_loss: 0.0047 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00116\n",
            "Epoch 2/200\n",
            "167/167 - 1s - loss: 0.0034 - accuracy: 0.9968 - val_loss: 0.0019 - val_accuracy: 0.9982\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00116\n",
            "Epoch 3/200\n",
            "167/167 - 1s - loss: 0.0018 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00116\n",
            "Epoch 4/200\n",
            "167/167 - 1s - loss: 0.0016 - accuracy: 0.9984 - val_loss: 0.0014 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00116\n",
            "Epoch 5/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00116\n",
            "Epoch 6/200\n",
            "167/167 - 1s - loss: 0.0013 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00116\n",
            "Epoch 7/200\n",
            "167/167 - 1s - loss: 0.0014 - accuracy: 0.9986 - val_loss: 0.0018 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00116\n",
            "Epoch 00007: early stopping\n",
            "Training finished...Loading the best model\n",
            "\n",
            "[[17567    22]\n",
            " [   21 11507]]\n",
            "Plotting confusion matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEmCAYAAAAnRIjxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf6ElEQVR4nO3debgdVZ3u8e+bRCaZCSIGkKgRjdxGMQLKlUZoIaC3gz4OIGoa0x0HRNvhqtjejo3i1dYWoRVtlAiIMogDURHIRbmIjwwRESGA5IJIwhgS5jHw3j9qHdkezrD3zq7UOfu8H556TtWqVVVrn5Bf1l61BtkmIiJ6b1LTBYiI6FcJsBERNUmAjYioSQJsRERNEmAjImqSABsRUZME2AlE0oaSfiLpXknfX4v7HCrp/F6WrSmSXi3p+qbLEf1J6Qc79kh6G/Bh4EXA/cCVwNG2L17L+74DOAJ4le01a13QMU6SgRm2lzVdlpiYUoMdYyR9GPgK8DlgG2AH4HhgTg9u/1zgjxMhuLZD0pSmyxB9zna2MbIBmwEPAG8eIc/6VAH41rJ9BVi/nNsbWA58BLgTuA04rJz7N+Ax4PHyjHnAp4FTW+69I2BgSjn+B+BGqlr0TcChLekXt1z3KuBy4N7y81Ut5y4EPgP8utznfGDqMJ9toPwfayn/QcCBwB+BVcAnW/LvBvwGuKfk/SqwXjl3UfksD5bP+9aW+38cuB34zkBaueb55Rm7luPnAHcBezf9/0a28bmlBju2vBLYAPjRCHn+BdgDeCmwC1WQ+VTL+WdTBeppVEH0a5K2sL2AqlZ8hu2NbZ84UkEkPRM4DjjA9iZUQfTKIfJtCfys5N0K+DLwM0lbtWR7G3AY8CxgPeCjIzz62VS/g2nAvwLfBN4OvBx4NfC/JE0veZ8APgRMpfrd7Qu8D8D2XiXPLuXzntFy/y2pavPzWx9s+/9RBd9TJW0EfBs42faFI5Q3YlgJsGPLVsBKj/wV/lDgKNt32r6Lqmb6jpbzj5fzj9s+h6r2tlOX5XkS2FnShrZvs33NEHleB9xg+zu219g+DbgO+B8teb5t+4+2HwbOpPrHYTiPU7U3Pw6cThU8j7V9f3n+Uqp/WLD9W9uXlOf+Cfgv4G/b+EwLbD9ayvNXbH8TWAZcCmxL9Q9aRFcSYMeWu4Gpo7QNPge4ueX45pL2l3sMCtAPARt3WhDbD1J9rX4PcJukn0l6URvlGSjTtJbj2zsoz922nyj7AwHwjpbzDw9cL+mFkn4q6XZJ91HV0KeOcG+Au2w/MkqebwI7A/9p+9FR8kYMKwF2bPkN8ChVu+NwbqX6ejtgh5LWjQeBjVqOn9160vZ5tl9LVZO7jirwjFaegTKt6LJMnfg6Vblm2N4U+CSgUa4ZsduMpI2p2rVPBD5dmkAiupIAO4bYvpeq3fFrkg6StJGkZ0g6QNK/l2ynAZ+StLWkqSX/qV0+8kpgL0k7SNoMOHLghKRtJM0pbbGPUjU1PDnEPc4BXijpbZKmSHorMBP4aZdl6sQmwH3AA6V2/d5B5+8AntfhPY8Fltj+R6q25W+sdSljwkqAHWNs/wdVH9hPUb3BvgV4P/DjkuWzwBLgKuAPwBUlrZtnLQbOKPf6LX8dFCeVctxK9Wb9b3l6AMP23cDrqXou3E3VA+D1tld2U6YOfZTqBdr9VLXrMwad/zRwsqR7JL1ltJtJmgPM5qnP+WFgV0mH9qzEMaFkoEFERE1Sg42IqEkCbERETRJgIyJqkgAbEVGTMTXZhaZsaK23SdPFiB552Yt3aLoI0SM33/wnVq5cOVof445M3vS59pqnDaYblh++6zzbs3tZhrqNrQC73iasv9OovWlinPj1pV9tugjRI3vuPqvn9/Sahzv6+/7IlV8bbZTemDOmAmxETCQC9XcrZQJsRDRDgHra6jDmJMBGRHNSg42IqINg0uSmC1GrBNiIaE6aCCIiaiDSRBARUQ+lBhsRUZvUYCMiapIabEREHTLQICKiHhloEBFRo9RgIyLqIJicgQYREb2XfrARETVKG2xERB36vxdBf3+6iBjbpPa3UW+lhZLulHT1oPQjJF0n6RpJ/96SfqSkZZKul7R/S/rskrZM0ida0qdLurSknyFpvdHKlAAbEc3RpPa30Z0E/NWSMpJeA8wBdrH9EuBLJX0mcDDwknLN8ZImS5oMfA04AJgJHFLyAnwBOMb2C4DVwLzRCpQAGxHN6KT22kYN1vZFwKpBye8FPm/70ZLnzpI+Bzjd9qO2bwKWAbuVbZntG20/BpwOzJEkYB/grHL9ycBBo5UpATYimtNZDXaqpCUt2/w2nvBC4NXlq/3/lfSKkj4NuKUl3/KSNlz6VsA9ttcMSh9RXnJFRHM660Ww0nanqy9OAbYE9gBeAZwp6Xkd3qNrCbAR0ZB10otgOfBD2wYuk/QkMBVYAWzfkm+7ksYw6XcDm0uaUmqxrfmHlSaCiGiGqJaMaXfrzo+B1wBIeiGwHrASWAQcLGl9SdOBGcBlwOXAjNJjYD2qF2GLSoD+JfCmct+5wNmjPTw12IhoSG9rsJJOA/amaqtdDiwAFgILS9etx4C5JVheI+lMYCmwBjjc9hPlPu8HzgMmAwttX1Me8XHgdEmfBX4HnDhamRJgI6I5PRzJZfuQYU69fZj8RwNHD5F+DnDOEOk3UvUyaFsCbEQ0p89HciXARkRzMhdBREQN1P9zESTARkRzUoONiKiHEmAjInqvWpIrATYiovckNCkBNiKiFqnBRkTUJAE2IqImCbAREXVQ2fpYAmxENEIoNdiIiLokwEZE1CQBNiKiJgmwERF1yEuuiIh6CDFpUn/PptXfny4ixjRJbW9t3GuhpDvL8jCDz31EkiVNLceSdJykZZKukrRrS965km4o29yW9JdL+kO55ji1UagE2IhojjrYRncSMPtpj5C2B/YD/tySfADVQoczgPnA10veLanW8tqdanmYBZK2KNd8Hfinluue9qzBEmAjohnqbQ3W9kXAqiFOHQN8DHBL2hzgFFcuoVqSe1tgf2Cx7VW2VwOLgdnl3Ka2LymLJp4CHDRamdIGGxGNqbsXgaQ5wArbvx/0rGnALS3Hy0vaSOnLh0gfUQJsRDSmwwA7VdKSluMTbJ8wwr03Aj5J1TzQiATYiGhEF0NlV9qe1UH+5wPTgYHa63bAFZJ2A1YA27fk3a6krQD2HpR+YUnfboj8I0obbEQ0p7cvuf6K7T/YfpbtHW3vSPW1flfbtwOLgHeW3gR7APfavg04D9hP0hbl5dZ+wHnl3H2S9ii9B94JnD1aGVKDjYhmqLdtsJJOo6p9TpW0HFhg+8Rhsp8DHAgsAx4CDgOwvUrSZ4DLS76jbA+8OHsfVU+FDYGfl21ECbAR0ZheBljbh4xyfseWfQOHD5NvIbBwiPQlwM6dlCkBNiIakzW5IiJq0u+TvdT6kkvSbEnXl6Fln6jzWRExvnQyyGC8BuLaarCSJgNfA15L9fbuckmLbC+t65kRMb6M18DZrjprsLsBy2zfaPsx4HSq4WkREUBvh8qORXUG2OGGnP0VSfMlLZG0xGserrE4ETHm1NgPdixo/CVXGep2AsCkjZ7lUbJHRB8ZrzXTdtUZYIcbihYR0fOBBmNRnU0ElwMzJE2XtB5wMNXwtIiI6pu/2t/Go9pqsLbXSHo/1djeycBC29fU9byIGG/EpAw06J7tc6jG/EZEPE2/NxE0/pIrIiaocfzVv10JsBHRCEGaCCIi6pIabERETdIGGxFRh7TBRkTUo+oH298RNgE2IhoyfidxaVcWPYyIxvRyJJekhZLulHR1S9oXJV0n6SpJP5K0ecu5I8tc1ddL2r8lfch5rMuo1EtL+hllhOqIEmAjohmqumm1u7XhJGD2oLTFwM62/wb4I3AkgKSZVMP3X1KuOV7S5JZ5rA8AZgKHlLwAXwCOsf0CYDUwb7QCJcBGRCMG2mB7NR+s7YuAVYPSzre9phxeQjXpFFRzU59u+1HbN1GtLrsbw8xjXZbq3gc4q1x/MnDQaGVKgI2IxnTYRDB1YO7oss3v8HHv4qmltoebr3q49K2Ae1qC9ZDzWw+Wl1wR0ZgOX3KttD2ry+f8C7AG+G4313crATYiGrMuOhFI+gfg9cC+tgcm9R9pvuqh0u8GNpc0pdRi25rfOk0EEdEM1b8ml6TZwMeAv7f9UMupRcDBktaXNB2YAVzGMPNYl8D8S+BN5fq5wNmjPT8BNiIa0esJtyWdBvwG2EnScknzgK8CmwCLJV0p6RsAZW7qM4GlwLnA4bafKLXTgXmsrwXObJnH+uPAhyUto2qTPXG0MqWJICIa0tuBBrYPGSJ52CBo+2jg6CHSh5zH2vaNVL0M2pYAGxGN6fOBXAmwEdEQZT7YiIhaZLKXiIgaJcBGRNSkz+NrAmxENCc12IiIOmRFg4iIemgCTLidABsRjenz+JoAGxHNmdTnETYBNiIa0+fxNQE2IpohweSM5IqIqEdeckVE1KTP4+vwAVbSfwIe7rztD9RSooiYEETVVaufjVSDXbLOShERE1KfN8EOH2Btn9x6LGmjQUsuRER0by2WghkvRl0yRtIrJS0FrivHu0g6vvaSRUTf6/GSMQsl3Snp6pa0LSUtlnRD+blFSZek4yQtk3SVpF1brplb8t8gaW5L+ssl/aFcc5za+NehnTW5vgLsT7WqIrZ/D+zVxnUREcMS1UCDdrc2nATMHpT2CeAC2zOAC8oxwAFUCx3OAOYDX4cqIAMLgN2plodZMBCUS55/arlu8LOepq1FD23fMijpiXaui4gYSS9rsLYvAlYNSp4DDDR3ngwc1JJ+iiuXUC3JvS1VZXKx7VW2VwOLgdnl3Ka2LykrzJ7Scq9htdNN6xZJrwIs6RnAB6lWW4yIWCsdtsFOldT68v0E2yeMcs02tm8r+7cD25T9aUBrxXF5SRspffkQ6SNqJ8C+Bzi23OxWquVsD2/juoiIYXUxkmul7VndPs+2JQ3b9bQOowZY2yuBQ9dBWSJiglkHfQjukLSt7dvK1/w7S/oKYPuWfNuVtBXA3oPSLyzp2w2Rf0Tt9CJ4nqSfSLqrvKE7W9LzRrsuImI0Kl212tm6tAgY6AkwFzi7Jf2dpTfBHsC9pSnhPGA/SVuUl1v7AeeVc/dJ2qP0Hnhny72G1U4TwfeArwFvKMcHA6dRvWWLiOhK1Yugh/eTTqOqfU6VtJyqN8DngTMlzQNuBt5Ssp8DHAgsAx4CDgOwvUrSZ4DLS76jbA+8OHsfVU+FDYGfl21E7QTYjWx/p+X4VEn/s43rIiKG1+OBBrYPGebUvkPkNcO8S7K9EFg4RPoSYOdOyjTSXARblt2fS/oEcDrV3ARvpYr+ERFrpc8Hco1Yg/0tVUAd+BW8u+WcgSPrKlRETAz9PlR2pLkIpq/LgkTExNLrNtixqK35YCXtDMwENhhIs31KXYWKiIlhwtZgB0haQPVmbiZV2+sBwMVUQ8UiIroiweQ+D7DtzEXwJqq3cLfbPgzYBdis1lJFxITQy7kIxqJ2mggetv2kpDWSNqUaCbH9aBdFRIxmwjcRAEskbQ58k6pnwQPAb2otVURMCH0eX9uai+B9Zfcbks6lmrLrqnqLFRH9TrQ9z+u4NdJAg11HOmf7inqKFBETwjhuW23XSDXY/xjhnIF9elwWXvbiHfj1pV/t9W2jIfsec1HTRYgeuf6OB2q574Rtg7X9mnVZkIiYeNpaUmUca2ugQUREr4kJXIONiKhbhspGRNSgiyVjxp12VjSQpLdL+tdyvIOk3eovWkT0u0lqfxuP2mljPh54JTAwme39VCscRESslQyVhd1t7yrpdwC2V0tar+ZyRUSfq6YrHKeRs03t1GAflzSZqu8rkrYGnqy1VBExIUzqYBuNpA9JukbS1ZJOk7SBpOmSLpW0TNIZA5VDSeuX42Xl/I4t9zmypF8vaf+1/XyjOQ74EfAsSUdTTVX4ubV5aEQE9K6JQNI04APALNs7A5OpFmj9AnCM7RcAq4F55ZJ5wOqSfkzJh6SZ5bqXALOB40sFsyujBljb3wU+Bvxv4DbgINvf7/aBERFQ9YGd1MHWhinAhpKmABtRxat9gLPK+ZOBg8r+nHJMOb9vWY57DnC67Udt30S16mzXL/XbmXB7B6plbX/Smmb7z90+NCICOn55NVXSkpbjE2yfAGB7haQvAX8GHgbOp5r97x7ba0r+5cC0sj8NuKVcu0bSvcBWJf2Slme0XtOxdl5y/YynFj/cAJgOXE9VhY6I6FqH3a9W2p411AlJW1DVPqcD9wDfp/qK36h2piv8b63HZZat9w2TPSKiLaKnAw3+DrjJ9l0Akn4I7AlsLmlKqcVuB6wo+VdQLRywvDQpbAbc3ZI+oPWajnU810KZpnD3bh8YEQFAB4MM2ojDfwb2kLRRaUvdF1gK/JJq2SuAucDZZX9ROaac/4Vtl/SDSy+D6cAM4LJuP2I7bbAfbjmcBOwK3NrtAyMiBoje1GBtXyrpLOAKYA3wO+AEqibO0yV9tqSdWC45EfiOpGXAKqqeA9i+RtKZVMF5DXC47Se6LVc7bbCbtOyvKQX+QbcPjIiAgYEGvbuf7QXAgkHJNzJELwDbjwBvHuY+RwNH96JMIwbY0v9rE9sf7cXDIiJajdc5Bto10pIxU0r3hT3XZYEiYuKYyPPBXkbV3nqlpEVU3R4eHDhp+4c1ly0i+livmwjGonbaYDeg6r6wD0/1hzWQABsR3RvHs2S1a6QA+6zSg+BqngqsA1xrqSJiQuj32bRGCrCTgY1hyH4UCbARsVYmehPBbbaPWmcliYgJRkyewDXY/v7kEdGoalXZpktRr5EC7L7rrBQRMfGM47W22jVsgLW9al0WJCImnon8kisiojYTvYkgIqJWqcFGRNSkz+NrAmxENEN0MSH1OJMAGxHN0MSe7CUiolb9HV4TYCOiIYK+H8nV700gETGGSe1vo99Lm0s6S9J1kq6V9EpJW0paLOmG8nOLkleSjpO0TNJVZTHXgfvMLflvkDR3+CeOLgE2IhoipPa3NhwLnGv7RcAuwLXAJ4ALbM8ALijHAAdQLWg4A5gPfB1A0pZUy87sTrXUzIKBoNyNBNiIaMRAL4J2txHvJW0G7EVZ1ND2Y7bvAeYAJ5dsJwMHlf05wCmuXEK1vPe2wP7AYturbK8GFgOzu/2MCbAR0ZgOa7BTJS1p2ea33Go6cBfwbUm/k/QtSc8EtrF9W8lzO7BN2Z8G3NJy/fKSNlx6V/KSKyIa0+ErrpW2Zw1zbgrVEldHlCW8j+Wp5gAAbFvSOp3LOjXYiGiGOq7BjmQ5sNz2peX4LKqAe0f56k/5eWc5vwLYvuX67UracOldSYCNiEb0sg3W9u3ALZJ2Kkn7AkuBRcBAT4C5wNllfxHwztKbYA/g3tKUcB6wn6Qtysut/UpaV9JEEBGN6fFIriOA70paD7gROIwqNp8paR5wM/CWkvcc4EBgGfBQyYvtVZI+A1xe8h21NlO3JsBGRGN6OeG27SuBodpon7Z4gG0Dhw9zn4XAwl6UKQE2IhpRNRH090iuBNiIaEyfj5RNgI2IpgilBhsRUY/UYCMiapA22IiIurQ5S9Z4lgAbEY1JgI2IqEleckVE1ED0dqDBWJQAGxGNmdTnbQQJsBHRmDQRRETUYCI0EdQ2XaGkhZLulHR1Xc+IiPFMHf03HtU5H+xJrMVaNhHR5zpYUXa8NtXWFmBtXwR0PY9iRPQ/dbCNR423wZaFy+YDbL/DDg2XJiLWlaoNdryGzvY0vmSM7RNsz7I9a+upWzddnIhYh/q9Btt4gI2ICazHEVbS5LJs90/L8XRJl0paJumMspwMktYvx8vK+R1b7nFkSb9e0v5r8/ESYCOiMZOktrc2fRC4tuX4C8Axtl8ArAbmlfR5wOqSfkzJh6SZwMHAS6he0h8vaXLXn6/bC0cj6TTgN8BOkpaXRcciIv6ilxVYSdsBrwO+VY4F7EO1hDfAycBBZX9OOaac37fknwOcbvtR2zdRLYq4W7efr7aXXLYPqeveEdEnetu4+hXgY8Am5Xgr4B7ba8rxcmBa2Z8G3AJge42ke0v+acAlLfdsvaZjaSKIiEZUNdOOBhpMlbSkZZv/l3tJrwfutP3bpj7PUBrvphURE1TnAwhW2h5qWW6APYG/l3QgsAGwKXAssLmkKaUWux2wouRfAWwPLJc0BdgMuLslfUDrNR1LDTYiGtOrNljbR9rezvaOVC+pfmH7UOCXwJtKtrnA2WV/UTmmnP+FbZf0g0svg+nADOCybj9farAR0Zz6O7h+HDhd0meB3wEnlvQTge9IWkY14vRgANvXSDoTWAqsAQ63/US3D0+AjYiG1DOJi+0LgQvL/o0M0QvA9iPAm4e5/mjg6F6UJQE2IhrT5yNlE2AjohnjeQhsuxJgI6Ix6vMqbAJsRDSmz+NrAmxENKfP42sCbEQ0ZAI0wibARkRjxutaW+1KgI2IRoi0wUZE1KbP42sCbEQ0qM8jbAJsRDQmbbARETWZ1N/xNQE2IhqUABsR0XsDKxr0swTYiGhG5ysajDsJsBHRmD6PrwmwEdGgPo+wCbAR0ZB6VjQYS7LoYUQ0Rmp/G/k+2l7SLyUtlXSNpA+W9C0lLZZ0Q/m5RUmXpOMkLZN0laRdW+41t+S/QdLc4Z7ZjgTYiGhEJyvKtlHPXQN8xPZMYA/gcEkzgU8AF9ieAVxQjgEOoFoxdgYwH/g6VAEZWADsTrWW14KBoNyNBNiIaE6PIqzt22xfUfbvB64FpgFzgJNLtpOBg8r+HOAUVy4BNpe0LbA/sNj2KturgcXA7G4/XtpgI6IxkzrrpzVV0pKW4xNsnzA4k6QdgZcBlwLb2L6tnLod2KbsTwNuablseUkbLr0rCbAR0ZgOX3GttD1rxPtJGwM/AP7Z9n2ta37ZtiR3UcyupYkgIprRwQuudiq6kp5BFVy/a/uHJfmO8tWf8vPOkr4C2L7l8u1K2nDpXUmAjYgG9aYRVlVV9UTgWttfbjm1CBjoCTAXOLsl/Z2lN8EewL2lKeE8YD9JW5SXW/uVtK6kiSAiGtHjFQ32BN4B/EHSlSXtk8DngTMlzQNuBt5Szp0DHAgsAx4CDgOwvUrSZ4DLS76jbK/qtlAJsBHRmF7FV9sXj3C7fYfIb+DwYe61EFjYi3IlwEZEYzLZS0RETfp9qGwCbEQ0p7/jawJsRDSnz+NrAmxENEPqeCTXuJMAGxHN6e/4mgAbEc3p8/iaABsRzenzFoIE2IhoSv+vaJAAGxGN6PFQ2TEpk71ERNQkNdiIaEy/12ATYCOiMWmDjYioQTXQoOlS1CsBNiKakwAbEVGPNBFERNQkL7kiImrS5/E1ATYiGtTnETYBNiIa0+9tsKrW/hobJN1FtfJjv5sKrGy6ENETE+XP8rm2t+7lDSWdS/X7a9dK27N7WYa6jakAO1FIWmJ7VtPliLWXP8sYSeYiiIioSQJsRERNEmCbcULTBYieyZ9lDCttsBERNUkNNiKiJgmwERE1SYCNiKhJAuw6IGknSa+U9AxJk5suT6y9/DlGO/KSq2aS3gh8DlhRtiXASbbva7Rg0RVJL7T9x7I/2fYTTZcpxq7UYGsk6RnAW4F5tvcFzga2Bz4uadNGCxcdk/R64EpJ3wOw/URqsjGSBNj6bQrMKPs/An4KPAN4m9Tvs2H2D0nPBN4P/DPwmKRTIUE2RpYAWyPbjwNfBt4o6dW2nwQuBq4E/nujhYuO2H4QeBfwPeCjwAatQbbJssXYlQBbv18B5wPvkLSX7Sdsfw94DrBLs0WLTti+1fYDtlcC7wY2HAiyknaV9KJmSxhjTeaDrZntRyR9FzBwZPlL+CiwDXBbo4WLrtm+W9K7gS9Kug6YDLym4WLFGJMAuw7YXi3pm8BSqprPI8Dbbd/RbMlibdheKekq4ADgtbaXN12mGFvSTWsdKy9EXNpjYxyTtAVwJvAR21c1XZ4YexJgI9aCpA1sP9J0OWJsSoCNiKhJehFERNQkATYioiYJsBERNUmAjYioSQJsn5D0hKQrJV0t6fuSNlqLe50k6U1l/1uSZo6Qd29Jr+riGX+SNLXd9EF5HujwWZ+W9NFOyxixthJg+8fDtl9qe2fgMeA9rScldTWoxPY/2l46Qpa9gY4DbMREkADbn34FvKDULn8laRGwVNJkSV+UdLmkq8pQT1T5qqTrJf0f4FkDN5J0oaRZZX+2pCsk/V7SBZJ2pArkHyq151dL2lrSD8ozLpe0Z7l2K0nnS7pG0reAUWcSk/RjSb8t18wfdO6Ykn6BpK1L2vMlnVuu+VXmBoimZahsnyk11QOAc0vSrsDOtm8qQepe26+QtD7wa0nnAy8DdgJmUs2RsBRYOOi+WwPfBPYq99rS9ipJ3wAesP2lku97wDG2L5a0A3Ae8GJgAXCx7aMkvQ6Y18bHeVd5xobA5ZJ+YPtu4JnAEtsfkvSv5d7vp1pC+z22b5C0O3A8sE8Xv8aInkiA7R8bSrqy7P8KOJHqq/tltm8q6fsBfzPQvgpsRjVX7V7AaWXavVsl/WKI++8BXDRwL9urhinH3wEzW6a63VTSxuUZbyzX/kzS6jY+0wckvaHsb1/KejfwJHBGST8V+GF5xquA77c8e/02nhFRmwTY/vGw7Ze2JpRA82BrEnCE7fMG5Tuwh+WYBOwxePhop3OLS9qbKli/0vZDki4ENhgmu8tz7xn8O4hoUtpgJ5bzgPeWpWyQ9MIyU/9FwFtLG+22DD3t3iXAXpKml2u3LOn3A5u05DsfOGLgQNJAwLsIeFtJOwDYYpSybgasLsH1RVQ16AGTgIFa+Nuomh7uA26S9ObyDEnKfLvRqATYieVbVO2rV0i6Gvgvqm8xPwJuKOdOAX4z+ELbdwHzqb6O/56nvqL/BHjDwEsu4APArPISbSlP9Wb4N6oAfQ1VU8GfRynrucAUSdcCn6cK8AMeBHYrn2Ef4KiSfigwr5TvGmBOG7+TiNpkspeIiJqkBhsRUZME2IiImiTARkTUJAE2IqImCbARETVJgI2IqEkCbERETf4/IBPim7Bkp8YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9985231995054435\n",
            "Averaged F1: 0.9985232105394404\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     17589\n",
            "           1       1.00      1.00      1.00     11528\n",
            "\n",
            "    accuracy                           1.00     29117\n",
            "   macro avg       1.00      1.00      1.00     29117\n",
            "weighted avg       1.00      1.00      1.00     29117\n",
            "\n"
          ]
        }
      ]
    }
  ]
}